{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ">[Load Colab](#scrollTo=0oVgMzR_ywr_)\n",
        "\n",
        ">[initiate](#scrollTo=2y1omcz8ywsF)\n",
        "\n",
        ">[Gleason Score](#scrollTo=diKStuuskvkS)\n",
        "\n",
        ">[Augment](#scrollTo=aqf2izgHRetB)\n",
        "\n",
        ">[Transform](#scrollTo=L3A1UwZUywsS)\n",
        "\n",
        ">[Train](#scrollTo=QA0eNCpVywsW)\n",
        "\n",
        ">[ResNet](#scrollTo=AOE9Lac4k6cy)\n",
        "\n",
        ">[Trash](#scrollTo=txse4NidVs-S)\n",
        "\n",
        ">>[Check](#scrollTo=QAA2fmxbywsV)\n",
        "\n",
        ">>[3 attempt](#scrollTo=oEDLevmu6TRU)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "NkjTDA-cXZLV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93MDd95az4Te",
        "outputId": "b3c08a0c-72eb-4cdd-f088-82b76b11ccb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'PROSTATEx_masks'...\n",
            "remote: Enumerating objects: 5026, done.\u001b[K\n",
            "remote: Counting objects: 100% (376/376), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 5026 (delta 353), reused 195 (delta 179), pack-reused 4650\u001b[K\n",
            "Receiving objects: 100% (5026/5026), 902.72 MiB | 14.32 MiB/s, done.\n",
            "Resolving deltas: 100% (3701/3701), done.\n",
            "Updating files: 100% (2565/2565), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rcuocolo/PROSTATEx_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oVgMzR_ywr_"
      },
      "source": [
        "# Load Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4mUqLGhywsA",
        "outputId": "c4a3b229-04d9-4958-9c38-7dbe0fb4b906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P_ajbq-ywsC",
        "outputId": "be086be5-a7d7-414a-e137-d4e5423910ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1wCK5vBkYx3cQUsc06EFZG4zaqE0idTPk/Prostate_Lesion\n"
          ]
        }
      ],
      "source": [
        "#for colab\n",
        "%cd \"/content/drive/MyDrive/Prostate_Lesion/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0lVBl5IywsE",
        "outputId": "1c406872-813f-4651-f811-d52caaa17afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting monai\n",
            "  Downloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dicom2nifti\n",
            "  Downloading dicom2nifti-2.4.8-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 4)) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 6)) (7.7.1)\n",
            "Collecting elasticdeform\n",
            "  Downloading elasticdeform-0.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.3/91.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 8)) (0.19.3)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.9/dist-packages (from monai->-r requirement.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from monai->-r requirement.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from dicom2nifti->-r requirement.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.9/dist-packages (from dicom2nifti->-r requirement.txt (line 2)) (3.0.2)\n",
            "Collecting python-gdcm\n",
            "  Downloading python_gdcm-3.0.21-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydicom>=2.2.0\n",
            "  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirement.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirement.txt (line 3)) (2022.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (5.5.6)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (7.34.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (3.0.7)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (3.6.4)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (5.7.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirement.txt (line 8)) (2023.4.12)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirement.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirement.txt (line 8)) (2.25.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirement.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r requirement.txt (line 5)) (3.15.0)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirement.txt (line 6)) (6.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirement.txt (line 6)) (6.1.12)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (67.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (3.0.38)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.1.6)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirement.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8->monai->-r requirement.txt (line 1)) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8->monai->-r requirement.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (6.4.8)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.8.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (5.3.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (6.5.4)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (21.3.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.17.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (5.8.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (23.2.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.5.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8->monai->-r requirement.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8->monai->-r requirement.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (21.2.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.2.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (6.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (4.9.2)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.2.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (4.11.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.7.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (2.21)\n",
            "Installing collected packages: python-gdcm, pydicom, jedi, elasticdeform, dicom2nifti, torchmetrics, monai\n",
            "Successfully installed dicom2nifti-2.4.8 elasticdeform-0.5.0 jedi-0.18.2 monai-1.1.0 pydicom-2.3.1 python-gdcm-3.0.21 torchmetrics-0.11.4\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirement.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y1omcz8ywsF"
      },
      "source": [
        "# initiate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbJoro_hywsG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import os\n",
        "from glob import glob\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import dicom2nifti\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from ipywidgets import interact, interactive, IntSlider, ToggleButtons\n",
        "import os\n",
        "\n",
        "from monai.utils import first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVBHgyXmywsG"
      },
      "outputs": [],
      "source": [
        "HOME_DIR =       \"./\"\n",
        "DATA_DIR =       \"./PROSTATEx_masks/Files/lesions/\"\n",
        "OUT_DIR =        \"./results/lesion/\"\n",
        "SLICED_OUT_DIR = \"./data/sliced/\"\n",
        "AUG_OUT_DIR = \"./data/augmented/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp9_EMN1trtr"
      },
      "outputs": [],
      "source": [
        "from monai.utils import first\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils.train import train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Pasts0BAz-F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Prostate_Lesion/PROSTATEx_masks/Files/lesions/Image_list.csv')"
      ],
      "metadata": {
        "id": "snhTABh_0DHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"images/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"images/\")\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"prostates/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"prostates/\")\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"pz_masks/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"pz_masks/\")\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"tz_masks/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"tz_masks/\")\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"labels/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"labels/\")"
      ],
      "metadata": {
        "id": "JeEybDb40LeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for index, row in df.iterrows():\n",
        "    value = row['T2'].split(\"-\")\n",
        "    numb= value[1].split(\"_\")[0]\n",
        "    if int(numb) == 128:\n",
        "        t2 = DATA_DIR+'Images/T2/'+row['T2']+'.nii'\n",
        "    else:\n",
        "        t2 = DATA_DIR+'Images/T2/'+row['T2']+'.nii.gz'\n",
        "    if int(numb) == 25:\n",
        "        adc = DATA_DIR+'Images/ADC/'+row['ADC']+'a.nii.gz'\n",
        "    elif int(numb) == 113:\n",
        "        value = row['ADC'][:-2]+'9'\n",
        "        adc = DATA_DIR+'Images/ADC/'+value+'.nii.gz'\n",
        "    elif int(numb) == 203:\n",
        "        adc = DATA_DIR+'Images/ADC/'+'ProstateX-0203_diffusie-3ProstateX-0203_diffusie-3Scan-4bval_fs_7.niiScan-4bval_fs_7.nii.gz'\n",
        "    else:\n",
        "        adc = DATA_DIR+'Images/ADC/'+row['ADC']+'.nii.gz'\n",
        "   \n",
        "    data.append({ 'T2': t2, 'ADC': adc , 'name': value[0] + '_' +str(numb)})"
      ],
      "metadata": {
        "id": "4KggD8-P0NjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def load_nifti(image_nifty_file, label_nifty_file):\n",
        "    # load the image and label file, get the image content and return a numpy array for each\n",
        "    image = nib.load(image_nifty_file)\n",
        "    label = nib.load(label_nifty_file)\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "def save_to_json(data, path):\n",
        "  with open(path, 'w') as fp:\n",
        "    json.dump(data, fp)\n",
        "\n",
        "\n",
        "def remove_slices(img,start, end):\n",
        "  imgvol = np.array( img.dataobj )\n",
        "  imgvol = imgvol[ :, :, start:end ]\n",
        "  newimg = nib.Nifti1Image ( imgvol, img.affine )\n",
        "  return newimg\n",
        "\n"
      ],
      "metadata": {
        "id": "aTNBHbJE0SCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(DATA_DIR +'image_list.csv')"
      ],
      "metadata": {
        "id": "T60QdJfp0XuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "\n",
        "# specify the directory path\n",
        "image_path = './PROSTATEx_masks/Files/prostate/Images/'\n",
        "prostate_path = './PROSTATEx_masks/Files/prostate/mask_prostate/'\n",
        "pz_path = './PROSTATEx_masks/Files/prostate/mask_pz/'\n",
        "tz_path = './PROSTATEx_masks/Files/prostate/mask_tz/'\n",
        "lesion_path = './PROSTATEx_masks/Files/lesions/Masks/T2/'\n",
        "#t2_path = './PROSTATEx_masks/Files/lesions/Images/T2/'\n",
        "#adc_path = './PROSTATEx_masks/Files/lesions/Masks/ADC/'\n",
        "\n",
        "# get all the file names in the directory\n",
        "t2_images = os.listdir(image_path)\n",
        "prostates = os.listdir(prostate_path)\n",
        "pz_images = os.listdir(pz_path)\n",
        "tz_images = os.listdir(tz_path)\n",
        "t2_lesions = os.listdir(lesion_path)\n",
        "#t2_2 = os.listdir(t2_path)\n",
        "#adc_images = os.listdir(adc_path)\n",
        "\n",
        "data = []\n",
        "for i in range(len(df['T2'])):\n",
        "    t2 = nib.load(image_path + t2_images[i])\n",
        "    prostate = nib.load(prostate_path + prostates[i])\n",
        "    pz = nib.load(pz_path + pz_images[i])\n",
        "    tz = nib.load(tz_path + tz_images[i])\n",
        "    lesion = nib.load(lesion_path + t2_lesions[i])\n",
        "    name = t2_images[i].split('_')[0]\n",
        "\n",
        "    \n",
        "    if t2.shape[2] == lesion.shape[2]:\n",
        "        path = {\n",
        "            'T2': image_path + t2_images[i],\n",
        "            'prostate': prostate_path + prostates[i],\n",
        "            'PZ' : pz_path + pz_images[i],\n",
        "            'TZ': tz_path + tz_images[i],\n",
        "            'label': lesion_path + t2_lesions[i],\n",
        "            'Name':name\n",
        "        }\n",
        "        data.append(path)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XJ14L86u0bWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def load_nifti(image_path_1,image_path_2,image_path_3,image_path_4, label_path):\n",
        "    # load the image and label file, get the image content and return a numpy array for each\n",
        "    image1 = nib.load(image_path_1)\n",
        "    image2 = nib.load(image_path_2)\n",
        "    image3 = nib.load(image_path_3)\n",
        "    image4 = nib.load(image_path_4)\n",
        "    label = nib.load(label_path)\n",
        "    \n",
        "    return image1,image2,image3,image4, label\n",
        "\n",
        "def save_to_json(data, path):\n",
        "  with open(path, 'w') as fp:\n",
        "    json.dump(data, fp)\n",
        "\n",
        "\n",
        "def remove_slice(img,  slice_size):\n",
        "    total_slize_size = img.shape[2]\n",
        "    extra_slices = total_slize_size - slice_size\n",
        "    end  = total_slize_size - (extra_slices // 2 )\n",
        "    start = end - slice_size \n",
        "    imgvol = np.array( img.dataobj )\n",
        "    imgvol = imgvol[ :, :, start:end ]\n",
        "    newimg = nib.Nifti1Image ( imgvol, img.affine )\n",
        "    return newimg\n",
        "\n",
        "def create_same_slice_nifti(data, slice_size ,dir):\n",
        "  paths = []\n",
        "  total = len(data)\n",
        "  count = 1\n",
        "  for entry in data:\n",
        "    image, prostate, pz , tz, label = load_nifti(entry[\"T2\"], entry[\"prostate\"], entry[\"PZ\"], entry[\"TZ\"], entry[\"label\"])\n",
        "\n",
        "    total_slize_size = min( image.shape[2], prostate.shape[2], pz.shape[2] , tz.shape[2], label.shape[2])\n",
        "    if(total_slize_size < slice_size): \n",
        "      print(\"ERROR: slice upper limit exceeds\")\n",
        "      continue\n",
        "    \n",
        "\n",
        "    new_image = remove_slice(image,slice_size)\n",
        "    new_prostate = remove_slice(prostate,slice_size)\n",
        "    new_pz = remove_slice(pz,slice_size)\n",
        "    new_tz = remove_slice(tz,slice_size)\n",
        "    new_lbl = remove_slice(label,slice_size)\n",
        "\n",
        "    image_path = dir + 'images/'+ entry['Name']+'.nii.gz'\n",
        "    label_path = dir + 'labels/' + entry['Name']+'.nii.gz'\n",
        "    prostate_path = dir + 'prostates/' + entry['Name']+'.nii.gz'\n",
        "    pz_path = dir + 'pz_masks/' + entry['Name']+'.nii.gz'\n",
        "    tz_path = dir + 'tz_masks/' + entry['Name']+'.nii.gz'\n",
        "    \n",
        "    paths.append({\n",
        "      \"image\":image_path, \n",
        "      'prostate':prostate_path,\n",
        "      'PZ': pz_path,\n",
        "      'TZ' : tz_path,\n",
        "      \"label\":label_path, \n",
        "    })\n",
        "    new_image.to_filename(image_path)\n",
        "    new_prostate.to_filename(prostate_path)\n",
        "    new_pz.to_filename(pz_path)\n",
        "    new_tz.to_filename(tz_path)\n",
        "    new_lbl.to_filename(label_path)\n",
        "    \n",
        "    print(f\"{count}/{total}\")\n",
        "    count += 1\n",
        "\n",
        "  save_to_json({\"path\": paths}, dir + 'config.json')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lBzW3e9M0hfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slice_size = 16\n",
        "create_same_slice_nifti(data, slice_size ,SLICED_OUT_DIR)"
      ],
      "metadata": {
        "id": "rDlti3Nr0pRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diKStuuskvkS"
      },
      "source": [
        "# Gleason Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRfmbmVwSN7v"
      },
      "outputs": [],
      "source": [
        "def get_data_path(path):\n",
        "  f = open( path + 'config.json')\n",
        "  jdata = json.load(f)\n",
        "  f.close()\n",
        "  return jdata[\"path\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbzkY12W3C42"
      },
      "outputs": [],
      "source": [
        "data = get_data_path(SLICED_OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY1lS8ItlF15"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Prostate_Lesion/PROSTATEx_masks/Files/lesions/PROSTATEx_Classes.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88w9rvMClKCe"
      },
      "outputs": [],
      "source": [
        "df['Gleason Grade Group'] = df['Gleason Grade Group'].replace('No biopsy information', '0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM-dJSpulIAQ",
        "outputId": "e339dd57-e6f4-4aa1-c9ac-84461fae5f05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['3', '1', '2', '0', '4', '5'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df['Gleason Grade Group'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJvSGhCgud1L",
        "outputId": "daa1f356-d135-4198-c666-eced60c02973"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    187\n",
              "2     41\n",
              "1     36\n",
              "3     20\n",
              "4      8\n",
              "5      7\n",
              "Name: Gleason Grade Group, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df['Gleason Grade Group'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPebMmiz7qe0",
        "outputId": "dfbec151-fdb5-44c2-8346-4469a77b970d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File name: ProstateX-0170-Finding2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the file path\n",
        "file_path = './data/sliced/images/ProstateX-0170-Finding2.nii.gz'\n",
        "\n",
        "def getFileName(file_path):\n",
        "\n",
        "  # Get the base filename without the extension\n",
        "  file_name = os.path.basename(file_path)\n",
        "  file_name = os.path.splitext(file_name)[0]\n",
        "\n",
        "  # Remove the '.nii' extension\n",
        "  file_name = file_name.replace('.nii', '')\n",
        "\n",
        "  return file_name\n",
        "\n",
        "# Display the filename\n",
        "print('File name:', getFileName(file_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEQehTyy9fz8",
        "outputId": "a623d20a-6594-4cf4-f3bf-4eb487f9cec4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID                        299\n",
              "Clinically Significant      2\n",
              "Gleason Grade Group         6\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt3uI7tLQrXk"
      },
      "outputs": [],
      "source": [
        "def save_to_json(data, path):\n",
        "  with open(path, 'w') as fp:\n",
        "    json.dump(data, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YOege-e3ycH"
      },
      "outputs": [],
      "source": [
        "updated_data = []\n",
        "df['ID'] = df['ID'].str.replace('_','-')\n",
        "\n",
        "\n",
        "for row in data:\n",
        "  name = getFileName(row['image'])\n",
        " \n",
        "  gleason = df[ df['ID'] == name]\n",
        "  row['name'] = name\n",
        "  row['Clinically Significant'] = str(gleason['Clinically Significant'].values[0])\n",
        "  row['Gleason Grade Group'] = gleason['Gleason Grade Group'].values[0]\n",
        "  updated_data.append(row)\n",
        " \n",
        "\n",
        "save_to_json({\"path\": updated_data}, SLICED_OUT_DIR + 'combined_config.json')   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqf2izgHRetB"
      },
      "source": [
        "# Augment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwWvG8QUSQED"
      },
      "outputs": [],
      "source": [
        "def get_data_path(path):\n",
        "  f = open( path + 'combined_config.json')\n",
        "  jdata = json.load(f)\n",
        "  f.close()\n",
        "  return jdata[\"path\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0-EQ4EGSBGN"
      },
      "outputs": [],
      "source": [
        "combine_data = data = get_data_path(SLICED_OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g23I4q6Rhkx"
      },
      "outputs": [],
      "source": [
        "indexes = [[],[],[],[],[],[]]\n",
        "\n",
        "for i, data in enumerate(combine_data):  \n",
        "  indexes[int(data['Gleason Grade Group'])].append(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1htOVKmW-4C"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import random\n",
        "import elasticdeform \n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import time\n",
        "\n",
        "from scipy.ndimage import affine_transform, rotate, zoom\n",
        "from skimage.exposure import adjust_gamma, rescale_intensity\n",
        "from skimage.util import random_noise\n",
        "from scipy.ndimage import map_coordinates\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "\n",
        "def rotated(voxels, mask_1, mask_2, mask_3, lbl, theta = None):\n",
        "    # Rotate volume by a minor angle (+/- 10 degrees: determined by investigation of dataset variability)\n",
        "    if theta is None:\n",
        "        theta = random.randint(-10, 10)\n",
        "    vox_new = rotate(voxels, theta, reshape = False)\n",
        "    mask_1_new = rotate(mask_1, theta, reshape = False)\n",
        "    mask_2_new = rotate(mask_2, theta, reshape = False)\n",
        "    mask_3_new = rotate(mask_3, theta, reshape = False)\n",
        "    lbl_new =  rotate(lbl, theta, reshape = False)\n",
        "\n",
        "    return vox_new, mask_1_new, mask_2_new, mask_3_new, lbl\n",
        "\n",
        "def scale_and_crop(voxels, mask_1, mask_2, mask_3, lbl):\n",
        "    # Scale the volume by a minor size and crop around centre (can also modify for random crop)\n",
        "    o_s = voxels.shape\n",
        "    r_s = [0]*len(o_s)\n",
        "    scale_factor = random.uniform(1, 1.2)\n",
        "    vox_zoom = zoom(voxels, scale_factor, order=1)\n",
        "    mask_1_zoom = zoom(mask_1, scale_factor, order=0)\n",
        "    mask_2_zoom = zoom(mask_2, scale_factor, order=0)\n",
        "    mask_3_zoom = zoom(mask_3, scale_factor, order=0)\n",
        "    lbl_zoom = zoom(lbl, scale_factor, order=0)\n",
        "    new_shape = vox_zoom.shape\n",
        "    # Start with offset\n",
        "    for i in range(len(o_s)):\n",
        "        if new_shape[i] == 1: \n",
        "            r_s[i] = 0\n",
        "            continue\n",
        "        r_c = int(((new_shape[i] - o_s[i]) - 1)/2)\n",
        "        r_s[i] = r_c\n",
        "    r_e = [r_s[i] + o_s[i] for i in list(range(len(o_s)))]\n",
        "    vox_zoom = vox_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    mask_1_zoom = mask_1_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    mask_2_zoom = mask_2_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    mask_3_zoom = mask_3_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    lbl_zoom = lbl_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    return vox_zoom, mask_1_zoom, mask_2_zoom, mask_3_zoom, lbl_zoom\n",
        "\n",
        "def grayscale_variation(voxels,  mask_1, mask_2, mask_3, lbl):\n",
        "    # Introduce a random global increment in gray-level value of volume. \n",
        "    im_min = np.min(voxels)\n",
        "    im_max = np.max(voxels)\n",
        "    mean = np.random.normal(0, 0.1)\n",
        "    smp = np.random.normal(mean, 0.01, size = np.shape(voxels))\n",
        "    voxels = voxels + im_max*smp\n",
        "    voxels[voxels <= im_min] = im_min # Clamp to min value\n",
        "    voxels[voxels > im_max] = im_max  # Clamp to max value\n",
        "    return voxels, mask_1, mask_2, mask_3, lbl\n",
        "\n",
        "\n",
        "def elastic_deformation(voxels, mask_1, mask_2, mask_3, lbl, alpha=None, sigma=None, mode=\"constant\", cval=0, is_random=False): \n",
        "    # Apply elastic deformation/distortion to the wolume\n",
        "    # Adapted from: https://tensorlayer.readthedocs.io/en/stable/_modules/tensorlayer/prepro.html#elastic_transform\n",
        "    if alpha == None:\n",
        "        alpha=voxels.shape[1]*3.\n",
        "    if sigma == None:\n",
        "        sigma=voxels.shape[1]*0.07\n",
        "    if is_random is False:\n",
        "        random_state = np.random.RandomState(None)\n",
        "    else:\n",
        "        random_state = np.random.RandomState(int(time.time()))\n",
        "        \n",
        "    if len(voxels.shape) == 3:\n",
        "        voxels = np.reshape(voxels, (voxels.shape[0], voxels.shape[1], voxels.shape[2], 1) )\n",
        "        mask_1 = np.reshape(mask_1, (mask_1.shape[0], mask_1.shape[1],mask_1.shape[2], 1) )\n",
        "        mask_2 = np.reshape(mask_2, (mask_2.shape[0], mask_2.shape[1],mask_2.shape[2], 1) )\n",
        "        mask_3 = np.reshape(mask_3, (mask_3.shape[0], mask_3.shape[1],mask_3.shape[2], 1) )\n",
        "        lbl = np.reshape( lbl, (lbl.shape[0], lbl.shape[1], lbl.shape[2], 1 ) )\n",
        "        \n",
        "    shape = (voxels.shape[0], voxels.shape[1])\n",
        "    label_shape = (lbl.shape[0], lbl.shape[1])\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n",
        "    x_, y_ = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
        "    indices = np.reshape(x_ + dx, (-1, 1)), np.reshape(y_ + dy, (-1, 1))\n",
        "    \n",
        "    new_voxels = np.zeros(voxels.shape)\n",
        "    new_mask_1 = np.zeros(mask_1.shape)\n",
        "    new_mask_2 = np.zeros(mask_2.shape)\n",
        "    new_mask_3 = np.zeros(mask_3.shape)\n",
        "    new_lbl = np.zeros( lbl.shape)\n",
        "    for i in range(voxels.shape[2]): # apply the same distortion to each slice within the volume\n",
        "        new_voxels[:,:,i,0] = map_coordinates(voxels[:,:,i,0], indices, order=1).reshape(shape)\n",
        "        new_mask_1[:,:,i,0] = map_coordinates(mask_1[:,:,i,0], indices, order=1).reshape(shape)\n",
        "        new_mask_2[:,:,i,0] = map_coordinates(mask_2[:,:,i,0], indices, order=1).reshape(shape)\n",
        "        new_mask_3[:,:,i,0] = map_coordinates(mask_3[:,:,i,0], indices, order=1).reshape(shape)\n",
        "        new_lbl[:,:,i,0] = map_coordinates(lbl[:,:,i,0], indices, order=1).reshape( label_shape)\n",
        "        \n",
        "    return new_voxels.squeeze(), new_mask_1.squeeze(), new_mask_2.squeeze(), new_mask_3.squeeze(), new_lbl.squeeze()\n",
        "\n",
        "\n",
        "    \n",
        "def translate(image, mask_1, mask_2, mask_3, lbl):\n",
        "\n",
        "    translate = (random.uniform(-10, 10), random.uniform(-10, 10))\n",
        "    matrix = np.array([[1, 0, 0, translate[0]], [0, 1, 0, translate[1]], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "\n",
        "    # Translate\n",
        "    img = affine_transform(image, matrix, order=1)\n",
        "    mask_1_new = affine_transform(mask_1, matrix, order=0)\n",
        "    mask_2_new = affine_transform(mask_2, matrix, order=0)\n",
        "    mask_3_new = affine_transform(mask_3, matrix, order=0)\n",
        "    lbl_new = affine_transform( lbl, matrix, order=0)\n",
        "\n",
        "    return img, mask_1_new, mask_2_new, mask_3_new, lbl_new \n",
        "\n",
        "def randomFlip(image, mask_1, mask_2, mask_3, lbl):\n",
        "    flip_axes = [ i for i in range(2) if i!=2 and np.random.choice([0, 1]) == 1]\n",
        "\n",
        "    # Randomly flip the image and label along one or more axes, except for the z-axis\n",
        "    img = np.flip(image, axis=flip_axes)\n",
        "    mask_1_new = np.flip(mask_1, axis=flip_axes)\n",
        "    mask_2_new = np.flip(mask_2, axis=flip_axes)\n",
        "    mask_3_new = np.flip(mask_3, axis=flip_axes)\n",
        "    lbl_new = np.flip( lbl, axis=flip_axes)\n",
        "\n",
        "    return img, mask_1_new, mask_2_new, mask_3_new, lbl_new\n",
        "\n",
        "\n",
        "\n",
        "def shear(image, mask_1, mask_2, mask_3, lbl):\n",
        "\n",
        "    shear = random.uniform(-0.2, 0.2)\n",
        "    matrix = np.array([[1, shear, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "\n",
        "    # Shear\n",
        "    img = affine_transform(image, matrix, order=1)\n",
        "    mask_1_new = affine_transform(mask_1, matrix, order=0)\n",
        "    mask_2_new = affine_transform(mask_2, matrix, order=0)\n",
        "    mask_3_new = affine_transform(mask_3, matrix, order=0)\n",
        "    lbl_new = affine_transform(lbl, matrix, order=0)\n",
        "\n",
        "    return img, mask_1_new, mask_2_new, mask_3_new, lbl_new\n",
        "\n",
        "def sample_with_p(p):\n",
        "    # Helper function to return boolean of a sample with given probability p\n",
        "    if random.random() < p:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def get_random_perturbation(voxels, mask1, mask2, mask3, label):\n",
        "    # Generate a random perturbation of the input feature + label\n",
        "    p_rotate = 1\n",
        "    p_scale = 0.6\n",
        "    p_gray = 0.3\n",
        "    p_deform = 0.6\n",
        "    p_translate = 1\n",
        "    p_shear = 0.6\n",
        "\n",
        "    voxel = np.array(voxels.dataobj)\n",
        "    mask_1 = np.array(mask1.dataobj)\n",
        "    mask_2 = np.array(mask2.dataobj)\n",
        "    mask_3 = np.array(mask3.dataobj)\n",
        "    lbl = np.array(label.dataobj)\n",
        "\n",
        "\n",
        "    new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = randomFlip(voxel, mask_1, mask_2, mask_3, lbl)\n",
        "   \n",
        "    new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = rotated(new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl )\n",
        "\n",
        "    if sample_with_p(p_scale):\n",
        "       new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = scale_and_crop( new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "    \n",
        "    if sample_with_p(p_gray):\n",
        "       new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = grayscale_variation(new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "    # if sample_with_p(p_deform):\n",
        "    #     new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = elastic_deformation( new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "    \n",
        "    if sample_with_p(p_shear):\n",
        "        new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = shear( new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "    \n",
        "\n",
        "    new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = translate( new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "\n",
        "    new_voxels = nib.Nifti1Image(new_voxels, voxels.affine)\n",
        "    new_mask_1 = nib.Nifti1Image(new_mask_1, mask1.affine)\n",
        "    new_mask_2 = nib.Nifti1Image(new_mask_2, mask2.affine)\n",
        "    new_mask_3 = nib.Nifti1Image(new_mask_3, mask3.affine)\n",
        "    new_lbl = nib.Nifti1Image(new_lbl, label.affine)\n",
        "    \n",
        "    return new_voxels, new_mask_1,  new_mask_2,  new_mask_3, new_lbl\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou1Zs-t0LCDm"
      },
      "outputs": [],
      "source": [
        "def load_nifti(image, mask_prostate, mask_pz, mask_tz, lbl):\n",
        "    # load the image and label file, get the image content and return a numpy array for each\n",
        "    image = nib.load(image)\n",
        "    prostate = nib.load(mask_prostate)\n",
        "    pz = nib.load(mask_pz)\n",
        "    tz = nib.load(mask_tz)\n",
        "    label = nib.load(lbl)\n",
        "    \n",
        "    return image, prostate, pz, tz, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTqUnZZ9DuYO",
        "outputId": "03c6ca44-3468-4105-88bc-3a2de03ef0ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': './data/sliced/images/ProstateX-0000-Finding1.nii.gz',\n",
              " 'prostate': './data/sliced/prostates/ProstateX-0000-Finding1.nii.gz',\n",
              " 'PZ': './data/sliced/pz_masks/ProstateX-0000-Finding1.nii.gz',\n",
              " 'TZ': './data/sliced/tz_masks/ProstateX-0000-Finding1.nii.gz',\n",
              " 'label': './data/sliced/labels/ProstateX-0000-Finding1.nii.gz',\n",
              " 'name': 'ProstateX-0000-Finding1',\n",
              " 'Clinically Significant': 'True',\n",
              " 'Gleason Grade Group': '3'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "combine_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1kNEQSqupN_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def augment( entry):\n",
        "   \n",
        "    img, mask_prostate, mask_pz, mask_tz, lbl = load_nifti(entry[\"image\"],entry[\"prostate\"],entry[\"PZ\"],entry[\"TZ\"], entry[\"label\"])\n",
        "    \n",
        "    image, prostate, pz, tz, label = get_random_perturbation(img, mask_prostate, mask_pz, mask_tz, lbl)\n",
        "    name  =  entry['name'] + str(random.randint(1, 1000))\n",
        "    image_path = entry[\"image\"].replace(\"data/sliced/\", \"data/augmented/\" ).replace(entry['name'], name)\n",
        "    prostate_path = entry[\"prostate\"].replace(\"data/sliced/\", \"data/augmented/\" ).replace(entry['name'], name)\n",
        "    pz_path = entry[\"PZ\"].replace(\"data/sliced/\", \"data/augmented/\" ).replace(entry['name'], name)\n",
        "    tz_path = entry[\"TZ\"].replace(\"data/sliced/\", \"data/augmented/\" ).replace(entry['name'], name)\n",
        "    label_path = entry[\"label\"].replace(\"data/sliced/\", \"data/augmented/\").replace(entry['name'], name)\n",
        "\n",
        "    \n",
        "    image.to_filename(image_path )\n",
        "    prostate.to_filename(prostate_path)\n",
        "    pz.to_filename(pz_path )\n",
        "    tz.to_filename(tz_path ) \n",
        "    label.to_filename(label_path)\n",
        "\n",
        "    paths = {\n",
        "        \"image\":image_path, \n",
        "        'prostate': prostate_path,\n",
        "        'PZ': pz_path,\n",
        "        'TZ': tz_path,\n",
        "        \"label\":label_path,\n",
        "        'name': name,\n",
        "        'Clinically Significant': entry['Clinically Significant'],\n",
        "        'Gleason Grade Group' : entry['Gleason Grade Group']\n",
        "\n",
        "    }\n",
        "\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WzGdoeNSJkj",
        "outputId": "48b80e61-3d9e-4dd4-d86d-701d4b1e7cfb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': './data/sliced/images/ProstateX-0003-Finding1.nii.gz',\n",
              " 'prostate': './data/sliced/prostates/ProstateX-0003-Finding1.nii.gz',\n",
              " 'PZ': './data/sliced/pz_masks/ProstateX-0003-Finding1.nii.gz',\n",
              " 'TZ': './data/sliced/tz_masks/ProstateX-0003-Finding1.nii.gz',\n",
              " 'label': './data/sliced/labels/ProstateX-0003-Finding1.nii.gz',\n",
              " 'name': 'ProstateX-0003-Finding1',\n",
              " 'Clinically Significant': 'False',\n",
              " 'Gleason Grade Group': '0'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "indexes[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsRmCR3XfwSg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(AUG_OUT_DIR  ):\n",
        "    os.makedirs(AUG_OUT_DIR )\n",
        "if not os.path.exists(AUG_OUT_DIR + \"/images\"):\n",
        "    os.makedirs(AUG_OUT_DIR +  \"/images\")\n",
        "if not os.path.exists(AUG_OUT_DIR +  \"/prostates\"):\n",
        "    os.makedirs(AUG_OUT_DIR + \"/prostates\")\n",
        "if not os.path.exists(AUG_OUT_DIR +  \"/pz_masks\"):\n",
        "    os.makedirs(AUG_OUT_DIR + \"/pz_masks\")\n",
        "if not os.path.exists(AUG_OUT_DIR + \"/tz_masks\"):\n",
        "    os.makedirs(AUG_OUT_DIR +  \"/tz_masks\")\n",
        "if not os.path.exists(AUG_OUT_DIR + \"/labels\"):\n",
        "    os.makedirs(AUG_OUT_DIR +  \"/labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB877yhgUJST",
        "outputId": "8829caf1-b22b-40c2-939e-56f69b5840f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149\n",
            "40\n",
            "38\n",
            "10\n",
            "40\n",
            "80\n",
            "10\n",
            "20\n",
            "40\n",
            "120\n",
            "10\n",
            "30\n",
            "40\n",
            "160\n",
            "10\n",
            "40\n",
            "40\n",
            "200\n",
            "10\n",
            "50\n",
            "40\n",
            "240\n",
            "10\n",
            "60\n"
          ]
        }
      ],
      "source": [
        "train_percent = 0.8\n",
        "train_class_size = 40\n",
        "test_class_size = 10\n",
        "\n",
        "train_data = []\n",
        "test_data = []\n",
        "for data in indexes:\n",
        "  train_size = int(train_percent * len(data))\n",
        "  train_set = data[:train_size]\n",
        "  test_set = data[train_size:]\n",
        "\n",
        "  while train_class_size > len(train_set):\n",
        "    rand_num = random.randint(0, len(train_set)-1)\n",
        "    new_data = augment(train_set[rand_num])\n",
        "    train_set.append(new_data)\n",
        "  train_data.extend(train_set[:40]) #train_data.extend(train_set)\n",
        "  print(len(train_set))\n",
        "  print(len(train_data))\n",
        "\n",
        "  while test_class_size > len(test_set):\n",
        "    rand_num = random.randint(0, len(test_set)-1)\n",
        "    new_data = augment(test_set[rand_num])\n",
        "    test_set.append(new_data)\n",
        "\n",
        "  test_data.extend(test_set[:10]) #test_data.extend(test_set)\n",
        "  print(len(test_set))\n",
        "  print(len(test_data))\n",
        "\n",
        "\n",
        "save_to_json({\"path\": train_data}, AUG_OUT_DIR + 'train_config.json') \n",
        "save_to_json({\"path\": test_data}, AUG_OUT_DIR + 'test_config.json')   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3A1UwZUywsS"
      },
      "source": [
        "# Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-U6kFUWywsS"
      },
      "outputs": [],
      "source": [
        "def get_data_path(path):\n",
        "  f = open( path )\n",
        "  jdata = json.load(f)\n",
        "  f.close()\n",
        "  return jdata[\"path\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0quaw1JsO79R"
      },
      "outputs": [],
      "source": [
        "train_data =  get_data_path(AUG_OUT_DIR + 'train_config.json')\n",
        "test_data = get_data_path(AUG_OUT_DIR + 'test_config.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDqPvFtLywsU",
        "outputId": "3ce20e62-d314-4a67-b72f-40fc31094972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240 60\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data),len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY-TzlbmywsU"
      },
      "outputs": [],
      "source": [
        "pixdim =(1.5, 1.5, 1.0)\n",
        "a_min=0\n",
        "a_max=500\n",
        "spatial_size= [128, 128,16] #[384, 384,18]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mC0MOUMywsU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    AddChanneld,\n",
        "    LoadImaged,\n",
        "    Resized,\n",
        "    ToTensord,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    SqueezeDimd\n",
        ")\n",
        "from monai.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class ConcatImagesd:\n",
        "    def __init__(self, keys):\n",
        "        self.keys = keys\n",
        "\n",
        "    def __call__(self, data):\n",
        "        concat = np.concatenate([np.expand_dims(data[key], axis=0) for key in self.keys], axis=1)\n",
        "        data[self.keys[0]] = concat\n",
        "        for key in self.keys[1:]:\n",
        "            del data[key]\n",
        "        return data\n",
        "\n",
        "\n",
        "def transform(data, a_min, a_max, spatial_size, pixdim):\n",
        "    train_transforms = Compose([\n",
        "        LoadImaged(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\", \"label\"], reader=\"ITKReader\"),\n",
        "        AddChanneld(keys=[\"label\"]),\n",
        "        AddChanneld(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\"]),\n",
        "        Orientationd(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\", \"label\"], axcodes=\"RAS\"),\n",
        "        ScaleIntensityRanged(keys=[\"image\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "        CropForegroundd(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\", \"label\"], source_key=\"image\"),\n",
        "        Resized(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\", \"label\"], spatial_size=spatial_size),\n",
        "        ConcatImagesd(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\"]),\n",
        "        ToTensord(keys=[\"image\", \"label\"]),\n",
        "        SqueezeDimd(keys=[\"image\"]),\n",
        "    ])\n",
        "\n",
        "    ds = Dataset(data=data, transform=train_transforms)\n",
        "    loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C113p2eCywsV"
      },
      "outputs": [],
      "source": [
        "#from utils.transform import transform\n",
        "\n",
        "train_loader = transform(train_data, a_min, a_max, spatial_size, pixdim)\n",
        "test_loader = transform(test_data, a_min, a_max, spatial_size, pixdim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62jsAR9iywsX"
      },
      "outputs": [],
      "source": [
        "model_dir = OUT_DIR \n",
        "data_in = [train_loader, test_loader]\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA0eNCpVywsW"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abhISL-9nK_f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )  \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BB_classifier(nn.Module):\n",
        "    def __init__(self, n_input_channels=256, n_features=64, n_output_channels=6, anchor_stride=2, dim=3):\n",
        "        super(BB_classifier, self).__init__()\n",
        "        self.n_classes = 6\n",
        "        self.dim = dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv3d(n_input_channels, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_output_channels, kernel_size=3, stride=anchor_stride, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "\n",
        "        # Rearrange dimensions based on self.dim\n",
        "        if self.dim == 2:\n",
        "            x = x.permute(0, 2, 3, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "        else:\n",
        "            x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "\n",
        "        # Apply softmax activation\n",
        "        x = nn.functional.softmax(x[0], dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class UNetWithBBClassifier(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dconv_down1 = double_conv(4, 64)\n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        self.dconv_down4 = double_conv(256, 512)\n",
        "\n",
        "        self.maxpool = nn.MaxPool3d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
        "\n",
        "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
        "\n",
        "        self.conv_last = nn.Conv3d(64, 1, 1)\n",
        "\n",
        "        self.bb_classifier = BB_classifier()\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "\n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "\n",
        "        x = self.dconv_down4(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "\n",
        "        x = self.dconv_up3(x)\n",
        "        bb_output = self.bb_classifier(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "       \n",
        "        #bb_output = self.bb_classifier(x.view(x.size(0), -1))\n",
        "\n",
        "        x = self.dconv_up2(x)\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv1], dim=1)\n",
        "\n",
        "        x = self.dconv_up1(x)\n",
        "\n",
        "        out = self.conv_last(x)\n",
        "\n",
        "        return out,  bb_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCwhAKSLydMr"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.conv import Conv3d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )  \n",
        "\n",
        "class MNetWithBBClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.dconv_down1 = double_conv(4, 64)\n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        self.dconv_down4 = double_conv(256, 512)        \n",
        "\n",
        "        self.maxpool = nn.MaxPool3d((2,2,2), stride=(2,2,2))\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)        \n",
        "        \n",
        "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
        "        \n",
        "        self.conv_last = nn.Conv3d(64,num_classes, 1)\n",
        "\n",
        "        self.e1_e3 = nn.Conv3d(64, 256, kernel_size=3, stride=(2,2,2), padding=1)\n",
        "        self.conv_mid2 = nn.Conv3d(512, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.e2_e4 = nn.Conv3d(128, 512, kernel_size=3, stride=(2,2,2), padding=1)\n",
        "        self.conv_mid1 = nn.Conv3d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.bb_classifier = BB_classifier()\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        \n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        \n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3) \n",
        "        \n",
        "        x = self.dconv_down4(x)\n",
        "\n",
        "        mid = self.e2_e4(conv2)\n",
        "        mid = self.maxpool(mid)\n",
        "     \n",
        "        x = torch.cat([mid,x], dim=1)\n",
        "        x = self.conv_mid1(x)\n",
        "\n",
        "\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        mid = self.e1_e3(conv1)\n",
        "        mid = self.maxpool(mid)\n",
        "        mid = torch.cat([conv3,mid], dim=1)\n",
        "        mid = self.conv_mid2(mid)\n",
        "\n",
        "        x = torch.cat([x, mid], dim=1)\n",
        "        \n",
        "        x = self.dconv_up3(x)\n",
        "        bb_output = self.bb_classifier(x)\n",
        "        x = self.upsample(x)        \n",
        "        x = torch.cat([x, conv2], dim=1)       \n",
        "\n",
        "        x = self.dconv_up2(x)\n",
        "        x = self.upsample(x)   \n",
        "        x = torch.cat([x, conv1], dim=1)   \n",
        "        \n",
        "        x = self.dconv_up1(x)\n",
        "        \n",
        "        out = self.conv_last(x)\n",
        "        \n",
        "        return out, bb_output\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOx1e04Z7FdF"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "def load_metrices(path):\n",
        "  metrices_dir = path\n",
        "  train_loss = train_metric = test_loss = test_metric =np.array([])\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'loss_train.npy')):\n",
        "    train_loss = np.load(os.path.join(metrices_dir, 'loss_train.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'metric_train.npy')):\n",
        "    train_metric = np.load(os.path.join(metrices_dir, 'metric_train.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'loss_test.npy')):\n",
        "    test_loss = np.load(os.path.join(metrices_dir, 'loss_test.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'metric_test.npy')):\n",
        "    test_metric = np.load(os.path.join(metrices_dir, 'metric_test.npy'))\n",
        "  return train_loss, train_metric, test_loss, test_metric\n",
        "\n",
        "def dice_metric(predicted, target):\n",
        "    '''\n",
        "    In this function we take `predicted` and `target` (label) to calculate the dice coeficient then we use it \n",
        "    to calculate a metric value for the training and the validation.\n",
        "    '''\n",
        "    dice_value = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
        "    value = 1 - dice_value(predicted, target).item()\n",
        "    return value\n",
        "\n",
        "\n",
        "def get_time():\n",
        "  utc_time= datetime.datetime.now(pytz.utc)\n",
        "  local_time = utc_time.astimezone(pytz.timezone('Asia/Colombo'))\n",
        "  return local_time.strftime(\"%Y:%m:%d %H:%M:%S\")\n",
        "\n",
        "def update_history(data,model_dir):\n",
        "  history_file_path = model_dir + \"history.csv\"\n",
        "  if not os.path.exists(history_file_path):\n",
        "    with open(history_file_path,'a') as fd:\n",
        "        fd.write(\",\".join([\"Start\", \"End\", \"Best Matrix\", \"Best M. At\", \"Time\"]))\n",
        "  with open(history_file_path,'a') as fd:\n",
        "      str_data=[str(x) for x in (data + [get_time()])]\n",
        "      fd.write(\"\\n\" + \",\".join(str_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAjEiPoGr4Yk"
      },
      "outputs": [],
      "source": [
        "# Initialize the U-Net model with bounding box classifier and move to GPU if available\n",
        "in_channels = 4  # Update with the appropriate number of input channels for your data\n",
        "out_channels = 1  # Update with the appropriate number of output channels for your data\n",
        "num_classes = 6  # Update with the appropriate number of Gleason score classes for your data\n",
        "model =  UNetWithBBClassifier(in_channels,out_channels,num_classes ).to(device)\n",
        "\n",
        "#model =  MNetWithBBClassifier().to(device)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Initialize the Gleason score prediction loss function\n",
        "#criterion = UNetWithBBClassifierLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Update with the appropriate learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ9GNVsn60iU"
      },
      "outputs": [],
      "source": [
        "from monai.losses import FocalLoss\n",
        "\n",
        "# Define the Focal Loss function\n",
        "#loss = FocalLoss()\n",
        "#loss = nn.BCELoss()\n",
        "loss = nn.SmoothL1Loss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWialU8Art0K",
        "outputId": "f2258aa8-91c8-48a9-d529-ddc461420e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "223/240, Train_loss: 0.0198 Train_dice: 0.0198\n",
            "Epoch [13/100], Batch [224/240],  Cross Loss: 0.1331\n",
            "224/240, Train_loss: 0.1331 Train_dice: 0.1331\n",
            "Epoch [13/100], Batch [225/240],  Cross Loss: 0.3120\n",
            "225/240, Train_loss: 0.3120 Train_dice: 0.3120\n",
            "Epoch [13/100], Batch [226/240],  Cross Loss: 0.3413\n",
            "226/240, Train_loss: 0.3413 Train_dice: 0.3413\n",
            "Epoch [13/100], Batch [227/240],  Cross Loss: 0.2132\n",
            "227/240, Train_loss: 0.2132 Train_dice: 0.2132\n",
            "Epoch [13/100], Batch [228/240],  Cross Loss: 0.0183\n",
            "228/240, Train_loss: 0.0183 Train_dice: 0.0183\n",
            "Epoch [13/100], Batch [229/240],  Cross Loss: 0.1166\n",
            "229/240, Train_loss: 0.1166 Train_dice: 0.1166\n",
            "Epoch [13/100], Batch [230/240],  Cross Loss: 0.5813\n",
            "230/240, Train_loss: 0.5813 Train_dice: 0.5813\n",
            "Epoch [13/100], Batch [231/240],  Cross Loss: 0.0147\n",
            "231/240, Train_loss: 0.0147 Train_dice: 0.0147\n",
            "Epoch [13/100], Batch [232/240],  Cross Loss: 0.2503\n",
            "232/240, Train_loss: 0.2503 Train_dice: 0.2503\n",
            "Epoch [13/100], Batch [233/240],  Cross Loss: 0.0181\n",
            "233/240, Train_loss: 0.0181 Train_dice: 0.0181\n",
            "Epoch [13/100], Batch [234/240],  Cross Loss: 0.4444\n",
            "234/240, Train_loss: 0.4444 Train_dice: 0.4444\n",
            "Epoch [13/100], Batch [235/240],  Cross Loss: 0.1172\n",
            "235/240, Train_loss: 0.1172 Train_dice: 0.1172\n",
            "Epoch [13/100], Batch [236/240],  Cross Loss: 0.2527\n",
            "236/240, Train_loss: 0.2527 Train_dice: 0.2527\n",
            "Epoch [13/100], Batch [237/240],  Cross Loss: 0.5961\n",
            "237/240, Train_loss: 0.5961 Train_dice: 0.5961\n",
            "Epoch [13/100], Batch [238/240],  Cross Loss: 0.0217\n",
            "238/240, Train_loss: 0.0217 Train_dice: 0.0217\n",
            "Epoch [13/100], Batch [239/240],  Cross Loss: 0.0141\n",
            "239/240, Train_loss: 0.0141 Train_dice: 0.0141\n",
            "Epoch [13/100], Batch [240/240],  Cross Loss: 0.2414\n",
            "240/240, Train_loss: 0.2414 Train_dice: 0.2414\n",
            "--------------------\n",
            "Epoch_loss: 0.2342\n",
            "Epoch_metric: tensor(0.2342, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 14/100\n",
            "Epoch [14/100], Batch [1/240],  Cross Loss: 0.2036\n",
            "1/240, Train_loss: 0.2036 Train_dice: 0.2036\n",
            "Epoch [14/100], Batch [2/240],  Cross Loss: 0.0244\n",
            "2/240, Train_loss: 0.0244 Train_dice: 0.0244\n",
            "Epoch [14/100], Batch [3/240],  Cross Loss: 0.1049\n",
            "3/240, Train_loss: 0.1049 Train_dice: 0.1049\n",
            "Epoch [14/100], Batch [4/240],  Cross Loss: 0.3797\n",
            "4/240, Train_loss: 0.3797 Train_dice: 0.3797\n",
            "Epoch [14/100], Batch [5/240],  Cross Loss: 0.2333\n",
            "5/240, Train_loss: 0.2333 Train_dice: 0.2333\n",
            "Epoch [14/100], Batch [6/240],  Cross Loss: 0.2561\n",
            "6/240, Train_loss: 0.2561 Train_dice: 0.2561\n",
            "Epoch [14/100], Batch [7/240],  Cross Loss: 0.3415\n",
            "7/240, Train_loss: 0.3415 Train_dice: 0.3415\n",
            "Epoch [14/100], Batch [8/240],  Cross Loss: 0.0077\n",
            "8/240, Train_loss: 0.0077 Train_dice: 0.0077\n",
            "Epoch [14/100], Batch [9/240],  Cross Loss: 0.2973\n",
            "9/240, Train_loss: 0.2973 Train_dice: 0.2973\n",
            "Epoch [14/100], Batch [10/240],  Cross Loss: 0.0222\n",
            "10/240, Train_loss: 0.0222 Train_dice: 0.0222\n",
            "Epoch [14/100], Batch [11/240],  Cross Loss: 0.1211\n",
            "11/240, Train_loss: 0.1211 Train_dice: 0.1211\n",
            "Epoch [14/100], Batch [12/240],  Cross Loss: 0.0059\n",
            "12/240, Train_loss: 0.0059 Train_dice: 0.0059\n",
            "Epoch [14/100], Batch [13/240],  Cross Loss: 0.0069\n",
            "13/240, Train_loss: 0.0069 Train_dice: 0.0069\n",
            "Epoch [14/100], Batch [14/240],  Cross Loss: 0.0073\n",
            "14/240, Train_loss: 0.0073 Train_dice: 0.0073\n",
            "Epoch [14/100], Batch [15/240],  Cross Loss: 0.4531\n",
            "15/240, Train_loss: 0.4531 Train_dice: 0.4531\n",
            "Epoch [14/100], Batch [16/240],  Cross Loss: 0.0092\n",
            "16/240, Train_loss: 0.0092 Train_dice: 0.0092\n",
            "Epoch [14/100], Batch [17/240],  Cross Loss: 0.3523\n",
            "17/240, Train_loss: 0.3523 Train_dice: 0.3523\n",
            "Epoch [14/100], Batch [18/240],  Cross Loss: 0.4088\n",
            "18/240, Train_loss: 0.4088 Train_dice: 0.4088\n",
            "Epoch [14/100], Batch [19/240],  Cross Loss: 0.1166\n",
            "19/240, Train_loss: 0.1166 Train_dice: 0.1166\n",
            "Epoch [14/100], Batch [20/240],  Cross Loss: 0.1658\n",
            "20/240, Train_loss: 0.1658 Train_dice: 0.1658\n",
            "Epoch [14/100], Batch [21/240],  Cross Loss: 0.1434\n",
            "21/240, Train_loss: 0.1434 Train_dice: 0.1434\n",
            "Epoch [14/100], Batch [22/240],  Cross Loss: 0.4530\n",
            "22/240, Train_loss: 0.4530 Train_dice: 0.4530\n",
            "Epoch [14/100], Batch [23/240],  Cross Loss: 0.4395\n",
            "23/240, Train_loss: 0.4395 Train_dice: 0.4395\n",
            "Epoch [14/100], Batch [24/240],  Cross Loss: 0.0089\n",
            "24/240, Train_loss: 0.0089 Train_dice: 0.0089\n",
            "Epoch [14/100], Batch [25/240],  Cross Loss: 0.0905\n",
            "25/240, Train_loss: 0.0905 Train_dice: 0.0905\n",
            "Epoch [14/100], Batch [26/240],  Cross Loss: 0.5170\n",
            "26/240, Train_loss: 0.5170 Train_dice: 0.5170\n",
            "Epoch [14/100], Batch [27/240],  Cross Loss: 0.2777\n",
            "27/240, Train_loss: 0.2777 Train_dice: 0.2777\n",
            "Epoch [14/100], Batch [28/240],  Cross Loss: 0.1281\n",
            "28/240, Train_loss: 0.1281 Train_dice: 0.1281\n",
            "Epoch [14/100], Batch [29/240],  Cross Loss: 0.6517\n",
            "29/240, Train_loss: 0.6517 Train_dice: 0.6517\n",
            "Epoch [14/100], Batch [30/240],  Cross Loss: 0.5691\n",
            "30/240, Train_loss: 0.5691 Train_dice: 0.5691\n",
            "Epoch [14/100], Batch [31/240],  Cross Loss: 0.1014\n",
            "31/240, Train_loss: 0.1014 Train_dice: 0.1014\n",
            "Epoch [14/100], Batch [32/240],  Cross Loss: 0.3770\n",
            "32/240, Train_loss: 0.3770 Train_dice: 0.3770\n",
            "Epoch [14/100], Batch [33/240],  Cross Loss: 0.0273\n",
            "33/240, Train_loss: 0.0273 Train_dice: 0.0273\n",
            "Epoch [14/100], Batch [34/240],  Cross Loss: 0.0119\n",
            "34/240, Train_loss: 0.0119 Train_dice: 0.0119\n",
            "Epoch [14/100], Batch [35/240],  Cross Loss: 0.0203\n",
            "35/240, Train_loss: 0.0203 Train_dice: 0.0203\n",
            "Epoch [14/100], Batch [36/240],  Cross Loss: 0.4579\n",
            "36/240, Train_loss: 0.4579 Train_dice: 0.4579\n",
            "Epoch [14/100], Batch [37/240],  Cross Loss: 0.3385\n",
            "37/240, Train_loss: 0.3385 Train_dice: 0.3385\n",
            "Epoch [14/100], Batch [38/240],  Cross Loss: 0.5475\n",
            "38/240, Train_loss: 0.5475 Train_dice: 0.5475\n",
            "Epoch [14/100], Batch [39/240],  Cross Loss: 0.0214\n",
            "39/240, Train_loss: 0.0214 Train_dice: 0.0214\n",
            "Epoch [14/100], Batch [40/240],  Cross Loss: 0.2551\n",
            "40/240, Train_loss: 0.2551 Train_dice: 0.2551\n",
            "Epoch [14/100], Batch [41/240],  Cross Loss: 0.1410\n",
            "41/240, Train_loss: 0.1410 Train_dice: 0.1410\n",
            "Epoch [14/100], Batch [42/240],  Cross Loss: 0.0195\n",
            "42/240, Train_loss: 0.0195 Train_dice: 0.0195\n",
            "Epoch [14/100], Batch [43/240],  Cross Loss: 0.3588\n",
            "43/240, Train_loss: 0.3588 Train_dice: 0.3588\n",
            "Epoch [14/100], Batch [44/240],  Cross Loss: 0.0388\n",
            "44/240, Train_loss: 0.0388 Train_dice: 0.0388\n",
            "Epoch [14/100], Batch [45/240],  Cross Loss: 0.2701\n",
            "45/240, Train_loss: 0.2701 Train_dice: 0.2701\n",
            "Epoch [14/100], Batch [46/240],  Cross Loss: 0.0157\n",
            "46/240, Train_loss: 0.0157 Train_dice: 0.0157\n",
            "Epoch [14/100], Batch [47/240],  Cross Loss: 0.1343\n",
            "47/240, Train_loss: 0.1343 Train_dice: 0.1343\n",
            "Epoch [14/100], Batch [48/240],  Cross Loss: 0.2056\n",
            "48/240, Train_loss: 0.2056 Train_dice: 0.2056\n",
            "Epoch [14/100], Batch [49/240],  Cross Loss: 0.2042\n",
            "49/240, Train_loss: 0.2042 Train_dice: 0.2042\n",
            "Epoch [14/100], Batch [50/240],  Cross Loss: 0.3678\n",
            "50/240, Train_loss: 0.3678 Train_dice: 0.3678\n",
            "Epoch [14/100], Batch [51/240],  Cross Loss: 0.5985\n",
            "51/240, Train_loss: 0.5985 Train_dice: 0.5985\n",
            "Epoch [14/100], Batch [52/240],  Cross Loss: 0.2856\n",
            "52/240, Train_loss: 0.2856 Train_dice: 0.2856\n",
            "Epoch [14/100], Batch [53/240],  Cross Loss: 0.2744\n",
            "53/240, Train_loss: 0.2744 Train_dice: 0.2744\n",
            "Epoch [14/100], Batch [54/240],  Cross Loss: 0.1187\n",
            "54/240, Train_loss: 0.1187 Train_dice: 0.1187\n",
            "Epoch [14/100], Batch [55/240],  Cross Loss: 0.0052\n",
            "55/240, Train_loss: 0.0052 Train_dice: 0.0052\n",
            "Epoch [14/100], Batch [56/240],  Cross Loss: 0.2204\n",
            "56/240, Train_loss: 0.2204 Train_dice: 0.2204\n",
            "Epoch [14/100], Batch [57/240],  Cross Loss: 0.0120\n",
            "57/240, Train_loss: 0.0120 Train_dice: 0.0120\n",
            "Epoch [14/100], Batch [58/240],  Cross Loss: 0.0405\n",
            "58/240, Train_loss: 0.0405 Train_dice: 0.0405\n",
            "Epoch [14/100], Batch [59/240],  Cross Loss: 0.5946\n",
            "59/240, Train_loss: 0.5946 Train_dice: 0.5946\n",
            "Epoch [14/100], Batch [60/240],  Cross Loss: 0.4164\n",
            "60/240, Train_loss: 0.4164 Train_dice: 0.4164\n",
            "Epoch [14/100], Batch [61/240],  Cross Loss: 0.1690\n",
            "61/240, Train_loss: 0.1690 Train_dice: 0.1690\n",
            "Epoch [14/100], Batch [62/240],  Cross Loss: 0.1274\n",
            "62/240, Train_loss: 0.1274 Train_dice: 0.1274\n",
            "Epoch [14/100], Batch [63/240],  Cross Loss: 0.0169\n",
            "63/240, Train_loss: 0.0169 Train_dice: 0.0169\n",
            "Epoch [14/100], Batch [64/240],  Cross Loss: 0.1335\n",
            "64/240, Train_loss: 0.1335 Train_dice: 0.1335\n",
            "Epoch [14/100], Batch [65/240],  Cross Loss: 0.2734\n",
            "65/240, Train_loss: 0.2734 Train_dice: 0.2734\n",
            "Epoch [14/100], Batch [66/240],  Cross Loss: 0.0859\n",
            "66/240, Train_loss: 0.0859 Train_dice: 0.0859\n",
            "Epoch [14/100], Batch [67/240],  Cross Loss: 0.3323\n",
            "67/240, Train_loss: 0.3323 Train_dice: 0.3323\n",
            "Epoch [14/100], Batch [68/240],  Cross Loss: 0.0051\n",
            "68/240, Train_loss: 0.0051 Train_dice: 0.0051\n",
            "Epoch [14/100], Batch [69/240],  Cross Loss: 0.4524\n",
            "69/240, Train_loss: 0.4524 Train_dice: 0.4524\n",
            "Epoch [14/100], Batch [70/240],  Cross Loss: 0.0253\n",
            "70/240, Train_loss: 0.0253 Train_dice: 0.0253\n",
            "Epoch [14/100], Batch [71/240],  Cross Loss: 0.0344\n",
            "71/240, Train_loss: 0.0344 Train_dice: 0.0344\n",
            "Epoch [14/100], Batch [72/240],  Cross Loss: 0.5221\n",
            "72/240, Train_loss: 0.5221 Train_dice: 0.5221\n",
            "Epoch [14/100], Batch [73/240],  Cross Loss: 0.2159\n",
            "73/240, Train_loss: 0.2159 Train_dice: 0.2159\n",
            "Epoch [14/100], Batch [74/240],  Cross Loss: 0.0156\n",
            "74/240, Train_loss: 0.0156 Train_dice: 0.0156\n",
            "Epoch [14/100], Batch [75/240],  Cross Loss: 0.1278\n",
            "75/240, Train_loss: 0.1278 Train_dice: 0.1278\n",
            "Epoch [14/100], Batch [76/240],  Cross Loss: 0.3326\n",
            "76/240, Train_loss: 0.3326 Train_dice: 0.3326\n",
            "Epoch [14/100], Batch [77/240],  Cross Loss: 0.1132\n",
            "77/240, Train_loss: 0.1132 Train_dice: 0.1132\n",
            "Epoch [14/100], Batch [78/240],  Cross Loss: 0.1675\n",
            "78/240, Train_loss: 0.1675 Train_dice: 0.1675\n",
            "Epoch [14/100], Batch [79/240],  Cross Loss: 0.1496\n",
            "79/240, Train_loss: 0.1496 Train_dice: 0.1496\n",
            "Epoch [14/100], Batch [80/240],  Cross Loss: 0.1812\n",
            "80/240, Train_loss: 0.1812 Train_dice: 0.1812\n",
            "Epoch [14/100], Batch [81/240],  Cross Loss: 0.3034\n",
            "81/240, Train_loss: 0.3034 Train_dice: 0.3034\n",
            "Epoch [14/100], Batch [82/240],  Cross Loss: 0.0021\n",
            "82/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [14/100], Batch [83/240],  Cross Loss: 0.3154\n",
            "83/240, Train_loss: 0.3154 Train_dice: 0.3154\n",
            "Epoch [14/100], Batch [84/240],  Cross Loss: 0.0460\n",
            "84/240, Train_loss: 0.0460 Train_dice: 0.0460\n",
            "Epoch [14/100], Batch [85/240],  Cross Loss: 0.0695\n",
            "85/240, Train_loss: 0.0695 Train_dice: 0.0695\n",
            "Epoch [14/100], Batch [86/240],  Cross Loss: 0.3214\n",
            "86/240, Train_loss: 0.3214 Train_dice: 0.3214\n",
            "Epoch [14/100], Batch [87/240],  Cross Loss: 0.2865\n",
            "87/240, Train_loss: 0.2865 Train_dice: 0.2865\n",
            "Epoch [14/100], Batch [88/240],  Cross Loss: 0.4003\n",
            "88/240, Train_loss: 0.4003 Train_dice: 0.4003\n",
            "Epoch [14/100], Batch [89/240],  Cross Loss: 0.0058\n",
            "89/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [14/100], Batch [90/240],  Cross Loss: 0.2043\n",
            "90/240, Train_loss: 0.2043 Train_dice: 0.2043\n",
            "Epoch [14/100], Batch [91/240],  Cross Loss: 0.1026\n",
            "91/240, Train_loss: 0.1026 Train_dice: 0.1026\n",
            "Epoch [14/100], Batch [92/240],  Cross Loss: 0.4537\n",
            "92/240, Train_loss: 0.4537 Train_dice: 0.4537\n",
            "Epoch [14/100], Batch [93/240],  Cross Loss: 0.4793\n",
            "93/240, Train_loss: 0.4793 Train_dice: 0.4793\n",
            "Epoch [14/100], Batch [94/240],  Cross Loss: 0.1382\n",
            "94/240, Train_loss: 0.1382 Train_dice: 0.1382\n",
            "Epoch [14/100], Batch [95/240],  Cross Loss: 0.1511\n",
            "95/240, Train_loss: 0.1511 Train_dice: 0.1511\n",
            "Epoch [14/100], Batch [96/240],  Cross Loss: 0.5592\n",
            "96/240, Train_loss: 0.5592 Train_dice: 0.5592\n",
            "Epoch [14/100], Batch [97/240],  Cross Loss: 0.3898\n",
            "97/240, Train_loss: 0.3898 Train_dice: 0.3898\n",
            "Epoch [14/100], Batch [98/240],  Cross Loss: 0.2732\n",
            "98/240, Train_loss: 0.2732 Train_dice: 0.2732\n",
            "Epoch [14/100], Batch [99/240],  Cross Loss: 0.0093\n",
            "99/240, Train_loss: 0.0093 Train_dice: 0.0093\n",
            "Epoch [14/100], Batch [100/240],  Cross Loss: 0.3280\n",
            "100/240, Train_loss: 0.3280 Train_dice: 0.3280\n",
            "Epoch [14/100], Batch [101/240],  Cross Loss: 0.0344\n",
            "101/240, Train_loss: 0.0344 Train_dice: 0.0344\n",
            "Epoch [14/100], Batch [102/240],  Cross Loss: 0.0030\n",
            "102/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [14/100], Batch [103/240],  Cross Loss: 0.0556\n",
            "103/240, Train_loss: 0.0556 Train_dice: 0.0556\n",
            "Epoch [14/100], Batch [104/240],  Cross Loss: 0.2880\n",
            "104/240, Train_loss: 0.2880 Train_dice: 0.2880\n",
            "Epoch [14/100], Batch [105/240],  Cross Loss: 0.3776\n",
            "105/240, Train_loss: 0.3776 Train_dice: 0.3776\n",
            "Epoch [14/100], Batch [106/240],  Cross Loss: 0.0064\n",
            "106/240, Train_loss: 0.0064 Train_dice: 0.0064\n",
            "Epoch [14/100], Batch [107/240],  Cross Loss: 0.1321\n",
            "107/240, Train_loss: 0.1321 Train_dice: 0.1321\n",
            "Epoch [14/100], Batch [108/240],  Cross Loss: 0.1022\n",
            "108/240, Train_loss: 0.1022 Train_dice: 0.1022\n",
            "Epoch [14/100], Batch [109/240],  Cross Loss: 0.2365\n",
            "109/240, Train_loss: 0.2365 Train_dice: 0.2365\n",
            "Epoch [14/100], Batch [110/240],  Cross Loss: 0.4338\n",
            "110/240, Train_loss: 0.4338 Train_dice: 0.4338\n",
            "Epoch [14/100], Batch [111/240],  Cross Loss: 0.0053\n",
            "111/240, Train_loss: 0.0053 Train_dice: 0.0053\n",
            "Epoch [14/100], Batch [112/240],  Cross Loss: 0.0039\n",
            "112/240, Train_loss: 0.0039 Train_dice: 0.0039\n",
            "Epoch [14/100], Batch [113/240],  Cross Loss: 0.4480\n",
            "113/240, Train_loss: 0.4480 Train_dice: 0.4480\n",
            "Epoch [14/100], Batch [114/240],  Cross Loss: 0.0062\n",
            "114/240, Train_loss: 0.0062 Train_dice: 0.0062\n",
            "Epoch [14/100], Batch [115/240],  Cross Loss: 0.4504\n",
            "115/240, Train_loss: 0.4504 Train_dice: 0.4504\n",
            "Epoch [14/100], Batch [116/240],  Cross Loss: 0.1332\n",
            "116/240, Train_loss: 0.1332 Train_dice: 0.1332\n",
            "Epoch [14/100], Batch [117/240],  Cross Loss: 0.2593\n",
            "117/240, Train_loss: 0.2593 Train_dice: 0.2593\n",
            "Epoch [14/100], Batch [118/240],  Cross Loss: 0.5686\n",
            "118/240, Train_loss: 0.5686 Train_dice: 0.5686\n",
            "Epoch [14/100], Batch [119/240],  Cross Loss: 0.2668\n",
            "119/240, Train_loss: 0.2668 Train_dice: 0.2668\n",
            "Epoch [14/100], Batch [120/240],  Cross Loss: 0.5467\n",
            "120/240, Train_loss: 0.5467 Train_dice: 0.5467\n",
            "Epoch [14/100], Batch [121/240],  Cross Loss: 0.1294\n",
            "121/240, Train_loss: 0.1294 Train_dice: 0.1294\n",
            "Epoch [14/100], Batch [122/240],  Cross Loss: 0.2803\n",
            "122/240, Train_loss: 0.2803 Train_dice: 0.2803\n",
            "Epoch [14/100], Batch [123/240],  Cross Loss: 0.3352\n",
            "123/240, Train_loss: 0.3352 Train_dice: 0.3352\n",
            "Epoch [14/100], Batch [124/240],  Cross Loss: 0.2729\n",
            "124/240, Train_loss: 0.2729 Train_dice: 0.2729\n",
            "Epoch [14/100], Batch [125/240],  Cross Loss: 0.0921\n",
            "125/240, Train_loss: 0.0921 Train_dice: 0.0921\n",
            "Epoch [14/100], Batch [126/240],  Cross Loss: 0.1154\n",
            "126/240, Train_loss: 0.1154 Train_dice: 0.1154\n",
            "Epoch [14/100], Batch [127/240],  Cross Loss: 0.0582\n",
            "127/240, Train_loss: 0.0582 Train_dice: 0.0582\n",
            "Epoch [14/100], Batch [128/240],  Cross Loss: 0.5903\n",
            "128/240, Train_loss: 0.5903 Train_dice: 0.5903\n",
            "Epoch [14/100], Batch [129/240],  Cross Loss: 0.2801\n",
            "129/240, Train_loss: 0.2801 Train_dice: 0.2801\n",
            "Epoch [14/100], Batch [130/240],  Cross Loss: 0.2690\n",
            "130/240, Train_loss: 0.2690 Train_dice: 0.2690\n",
            "Epoch [14/100], Batch [131/240],  Cross Loss: 0.0076\n",
            "131/240, Train_loss: 0.0076 Train_dice: 0.0076\n",
            "Epoch [14/100], Batch [132/240],  Cross Loss: 0.4134\n",
            "132/240, Train_loss: 0.4134 Train_dice: 0.4134\n",
            "Epoch [14/100], Batch [133/240],  Cross Loss: 0.4173\n",
            "133/240, Train_loss: 0.4173 Train_dice: 0.4173\n",
            "Epoch [14/100], Batch [134/240],  Cross Loss: 0.6856\n",
            "134/240, Train_loss: 0.6856 Train_dice: 0.6856\n",
            "Epoch [14/100], Batch [135/240],  Cross Loss: 0.0696\n",
            "135/240, Train_loss: 0.0696 Train_dice: 0.0696\n",
            "Epoch [14/100], Batch [136/240],  Cross Loss: 0.5309\n",
            "136/240, Train_loss: 0.5309 Train_dice: 0.5309\n",
            "Epoch [14/100], Batch [137/240],  Cross Loss: 0.3713\n",
            "137/240, Train_loss: 0.3713 Train_dice: 0.3713\n",
            "Epoch [14/100], Batch [138/240],  Cross Loss: 0.4017\n",
            "138/240, Train_loss: 0.4017 Train_dice: 0.4017\n",
            "Epoch [14/100], Batch [139/240],  Cross Loss: 0.1658\n",
            "139/240, Train_loss: 0.1658 Train_dice: 0.1658\n",
            "Epoch [14/100], Batch [140/240],  Cross Loss: 0.0214\n",
            "140/240, Train_loss: 0.0214 Train_dice: 0.0214\n",
            "Epoch [14/100], Batch [141/240],  Cross Loss: 0.0113\n",
            "141/240, Train_loss: 0.0113 Train_dice: 0.0113\n",
            "Epoch [14/100], Batch [142/240],  Cross Loss: 0.1963\n",
            "142/240, Train_loss: 0.1963 Train_dice: 0.1963\n",
            "Epoch [14/100], Batch [143/240],  Cross Loss: 0.5441\n",
            "143/240, Train_loss: 0.5441 Train_dice: 0.5441\n",
            "Epoch [14/100], Batch [144/240],  Cross Loss: 0.0157\n",
            "144/240, Train_loss: 0.0157 Train_dice: 0.0157\n",
            "Epoch [14/100], Batch [145/240],  Cross Loss: 0.0116\n",
            "145/240, Train_loss: 0.0116 Train_dice: 0.0116\n",
            "Epoch [14/100], Batch [146/240],  Cross Loss: 0.4208\n",
            "146/240, Train_loss: 0.4208 Train_dice: 0.4208\n",
            "Epoch [14/100], Batch [147/240],  Cross Loss: 0.4608\n",
            "147/240, Train_loss: 0.4608 Train_dice: 0.4608\n",
            "Epoch [14/100], Batch [148/240],  Cross Loss: 0.3961\n",
            "148/240, Train_loss: 0.3961 Train_dice: 0.3961\n",
            "Epoch [14/100], Batch [149/240],  Cross Loss: 0.3151\n",
            "149/240, Train_loss: 0.3151 Train_dice: 0.3151\n",
            "Epoch [14/100], Batch [150/240],  Cross Loss: 0.3765\n",
            "150/240, Train_loss: 0.3765 Train_dice: 0.3765\n",
            "Epoch [14/100], Batch [151/240],  Cross Loss: 0.3480\n",
            "151/240, Train_loss: 0.3480 Train_dice: 0.3480\n",
            "Epoch [14/100], Batch [152/240],  Cross Loss: 0.0044\n",
            "152/240, Train_loss: 0.0044 Train_dice: 0.0044\n",
            "Epoch [14/100], Batch [153/240],  Cross Loss: 0.1328\n",
            "153/240, Train_loss: 0.1328 Train_dice: 0.1328\n",
            "Epoch [14/100], Batch [154/240],  Cross Loss: 0.0378\n",
            "154/240, Train_loss: 0.0378 Train_dice: 0.0378\n",
            "Epoch [14/100], Batch [155/240],  Cross Loss: 0.2669\n",
            "155/240, Train_loss: 0.2669 Train_dice: 0.2669\n",
            "Epoch [14/100], Batch [156/240],  Cross Loss: 0.4591\n",
            "156/240, Train_loss: 0.4591 Train_dice: 0.4591\n",
            "Epoch [14/100], Batch [157/240],  Cross Loss: 0.2945\n",
            "157/240, Train_loss: 0.2945 Train_dice: 0.2945\n",
            "Epoch [14/100], Batch [158/240],  Cross Loss: 0.3462\n",
            "158/240, Train_loss: 0.3462 Train_dice: 0.3462\n",
            "Epoch [14/100], Batch [159/240],  Cross Loss: 0.0411\n",
            "159/240, Train_loss: 0.0411 Train_dice: 0.0411\n",
            "Epoch [14/100], Batch [160/240],  Cross Loss: 0.4229\n",
            "160/240, Train_loss: 0.4229 Train_dice: 0.4229\n",
            "Epoch [14/100], Batch [161/240],  Cross Loss: 0.0502\n",
            "161/240, Train_loss: 0.0502 Train_dice: 0.0502\n",
            "Epoch [14/100], Batch [162/240],  Cross Loss: 0.2626\n",
            "162/240, Train_loss: 0.2626 Train_dice: 0.2626\n",
            "Epoch [14/100], Batch [163/240],  Cross Loss: 0.4636\n",
            "163/240, Train_loss: 0.4636 Train_dice: 0.4636\n",
            "Epoch [14/100], Batch [164/240],  Cross Loss: 0.0308\n",
            "164/240, Train_loss: 0.0308 Train_dice: 0.0308\n",
            "Epoch [14/100], Batch [165/240],  Cross Loss: 0.1586\n",
            "165/240, Train_loss: 0.1586 Train_dice: 0.1586\n",
            "Epoch [14/100], Batch [166/240],  Cross Loss: 0.5920\n",
            "166/240, Train_loss: 0.5920 Train_dice: 0.5920\n",
            "Epoch [14/100], Batch [167/240],  Cross Loss: 0.2615\n",
            "167/240, Train_loss: 0.2615 Train_dice: 0.2615\n",
            "Epoch [14/100], Batch [168/240],  Cross Loss: 0.2288\n",
            "168/240, Train_loss: 0.2288 Train_dice: 0.2288\n",
            "Epoch [14/100], Batch [169/240],  Cross Loss: 0.2806\n",
            "169/240, Train_loss: 0.2806 Train_dice: 0.2806\n",
            "Epoch [14/100], Batch [170/240],  Cross Loss: 0.0106\n",
            "170/240, Train_loss: 0.0106 Train_dice: 0.0106\n",
            "Epoch [14/100], Batch [171/240],  Cross Loss: 0.1682\n",
            "171/240, Train_loss: 0.1682 Train_dice: 0.1682\n",
            "Epoch [14/100], Batch [172/240],  Cross Loss: 0.6463\n",
            "172/240, Train_loss: 0.6463 Train_dice: 0.6463\n",
            "Epoch [14/100], Batch [173/240],  Cross Loss: 0.2641\n",
            "173/240, Train_loss: 0.2641 Train_dice: 0.2641\n",
            "Epoch [14/100], Batch [174/240],  Cross Loss: 0.0461\n",
            "174/240, Train_loss: 0.0461 Train_dice: 0.0461\n",
            "Epoch [14/100], Batch [175/240],  Cross Loss: 0.2994\n",
            "175/240, Train_loss: 0.2994 Train_dice: 0.2994\n",
            "Epoch [14/100], Batch [176/240],  Cross Loss: 0.3776\n",
            "176/240, Train_loss: 0.3776 Train_dice: 0.3776\n",
            "Epoch [14/100], Batch [177/240],  Cross Loss: 0.1559\n",
            "177/240, Train_loss: 0.1559 Train_dice: 0.1559\n",
            "Epoch [14/100], Batch [178/240],  Cross Loss: 0.3058\n",
            "178/240, Train_loss: 0.3058 Train_dice: 0.3058\n",
            "Epoch [14/100], Batch [179/240],  Cross Loss: 0.1745\n",
            "179/240, Train_loss: 0.1745 Train_dice: 0.1745\n",
            "Epoch [14/100], Batch [180/240],  Cross Loss: 0.0590\n",
            "180/240, Train_loss: 0.0590 Train_dice: 0.0590\n",
            "Epoch [14/100], Batch [181/240],  Cross Loss: 0.3662\n",
            "181/240, Train_loss: 0.3662 Train_dice: 0.3662\n",
            "Epoch [14/100], Batch [182/240],  Cross Loss: 0.2212\n",
            "182/240, Train_loss: 0.2212 Train_dice: 0.2212\n",
            "Epoch [14/100], Batch [183/240],  Cross Loss: 0.1497\n",
            "183/240, Train_loss: 0.1497 Train_dice: 0.1497\n",
            "Epoch [14/100], Batch [184/240],  Cross Loss: 0.3109\n",
            "184/240, Train_loss: 0.3109 Train_dice: 0.3109\n",
            "Epoch [14/100], Batch [185/240],  Cross Loss: 0.0643\n",
            "185/240, Train_loss: 0.0643 Train_dice: 0.0643\n",
            "Epoch [14/100], Batch [186/240],  Cross Loss: 0.0222\n",
            "186/240, Train_loss: 0.0222 Train_dice: 0.0222\n",
            "Epoch [14/100], Batch [187/240],  Cross Loss: 0.3058\n",
            "187/240, Train_loss: 0.3058 Train_dice: 0.3058\n",
            "Epoch [14/100], Batch [188/240],  Cross Loss: 0.0065\n",
            "188/240, Train_loss: 0.0065 Train_dice: 0.0065\n",
            "Epoch [14/100], Batch [189/240],  Cross Loss: 0.3361\n",
            "189/240, Train_loss: 0.3361 Train_dice: 0.3361\n",
            "Epoch [14/100], Batch [190/240],  Cross Loss: 0.0500\n",
            "190/240, Train_loss: 0.0500 Train_dice: 0.0500\n",
            "Epoch [14/100], Batch [191/240],  Cross Loss: 0.4592\n",
            "191/240, Train_loss: 0.4592 Train_dice: 0.4592\n",
            "Epoch [14/100], Batch [192/240],  Cross Loss: 0.2078\n",
            "192/240, Train_loss: 0.2078 Train_dice: 0.2078\n",
            "Epoch [14/100], Batch [193/240],  Cross Loss: 0.0040\n",
            "193/240, Train_loss: 0.0040 Train_dice: 0.0040\n",
            "Epoch [14/100], Batch [194/240],  Cross Loss: 0.0066\n",
            "194/240, Train_loss: 0.0066 Train_dice: 0.0066\n",
            "Epoch [14/100], Batch [195/240],  Cross Loss: 0.0221\n",
            "195/240, Train_loss: 0.0221 Train_dice: 0.0221\n",
            "Epoch [14/100], Batch [196/240],  Cross Loss: 0.4393\n",
            "196/240, Train_loss: 0.4393 Train_dice: 0.4393\n",
            "Epoch [14/100], Batch [197/240],  Cross Loss: 0.1669\n",
            "197/240, Train_loss: 0.1669 Train_dice: 0.1669\n",
            "Epoch [14/100], Batch [198/240],  Cross Loss: 0.6921\n",
            "198/240, Train_loss: 0.6921 Train_dice: 0.6921\n",
            "Epoch [14/100], Batch [199/240],  Cross Loss: 0.2748\n",
            "199/240, Train_loss: 0.2748 Train_dice: 0.2748\n",
            "Epoch [14/100], Batch [200/240],  Cross Loss: 0.1291\n",
            "200/240, Train_loss: 0.1291 Train_dice: 0.1291\n",
            "Epoch [14/100], Batch [201/240],  Cross Loss: 0.1924\n",
            "201/240, Train_loss: 0.1924 Train_dice: 0.1924\n",
            "Epoch [14/100], Batch [202/240],  Cross Loss: 0.0155\n",
            "202/240, Train_loss: 0.0155 Train_dice: 0.0155\n",
            "Epoch [14/100], Batch [203/240],  Cross Loss: 0.0927\n",
            "203/240, Train_loss: 0.0927 Train_dice: 0.0927\n",
            "Epoch [14/100], Batch [204/240],  Cross Loss: 0.2527\n",
            "204/240, Train_loss: 0.2527 Train_dice: 0.2527\n",
            "Epoch [14/100], Batch [205/240],  Cross Loss: 0.2277\n",
            "205/240, Train_loss: 0.2277 Train_dice: 0.2277\n",
            "Epoch [14/100], Batch [206/240],  Cross Loss: 0.0081\n",
            "206/240, Train_loss: 0.0081 Train_dice: 0.0081\n",
            "Epoch [14/100], Batch [207/240],  Cross Loss: 0.0174\n",
            "207/240, Train_loss: 0.0174 Train_dice: 0.0174\n",
            "Epoch [14/100], Batch [208/240],  Cross Loss: 0.2072\n",
            "208/240, Train_loss: 0.2072 Train_dice: 0.2072\n",
            "Epoch [14/100], Batch [209/240],  Cross Loss: 0.0496\n",
            "209/240, Train_loss: 0.0496 Train_dice: 0.0496\n",
            "Epoch [14/100], Batch [210/240],  Cross Loss: 0.1679\n",
            "210/240, Train_loss: 0.1679 Train_dice: 0.1679\n",
            "Epoch [14/100], Batch [211/240],  Cross Loss: 0.5363\n",
            "211/240, Train_loss: 0.5363 Train_dice: 0.5363\n",
            "Epoch [14/100], Batch [212/240],  Cross Loss: 0.0169\n",
            "212/240, Train_loss: 0.0169 Train_dice: 0.0169\n",
            "Epoch [14/100], Batch [213/240],  Cross Loss: 0.0242\n",
            "213/240, Train_loss: 0.0242 Train_dice: 0.0242\n",
            "Epoch [14/100], Batch [214/240],  Cross Loss: 0.4686\n",
            "214/240, Train_loss: 0.4686 Train_dice: 0.4686\n",
            "Epoch [14/100], Batch [215/240],  Cross Loss: 0.2360\n",
            "215/240, Train_loss: 0.2360 Train_dice: 0.2360\n",
            "Epoch [14/100], Batch [216/240],  Cross Loss: 0.0489\n",
            "216/240, Train_loss: 0.0489 Train_dice: 0.0489\n",
            "Epoch [14/100], Batch [217/240],  Cross Loss: 0.0870\n",
            "217/240, Train_loss: 0.0870 Train_dice: 0.0870\n",
            "Epoch [14/100], Batch [218/240],  Cross Loss: 0.4438\n",
            "218/240, Train_loss: 0.4438 Train_dice: 0.4438\n",
            "Epoch [14/100], Batch [219/240],  Cross Loss: 0.0048\n",
            "219/240, Train_loss: 0.0048 Train_dice: 0.0048\n",
            "Epoch [14/100], Batch [220/240],  Cross Loss: 0.1587\n",
            "220/240, Train_loss: 0.1587 Train_dice: 0.1587\n",
            "Epoch [14/100], Batch [221/240],  Cross Loss: 0.1551\n",
            "221/240, Train_loss: 0.1551 Train_dice: 0.1551\n",
            "Epoch [14/100], Batch [222/240],  Cross Loss: 0.0282\n",
            "222/240, Train_loss: 0.0282 Train_dice: 0.0282\n",
            "Epoch [14/100], Batch [223/240],  Cross Loss: 0.1879\n",
            "223/240, Train_loss: 0.1879 Train_dice: 0.1879\n",
            "Epoch [14/100], Batch [224/240],  Cross Loss: 0.1191\n",
            "224/240, Train_loss: 0.1191 Train_dice: 0.1191\n",
            "Epoch [14/100], Batch [225/240],  Cross Loss: 0.1537\n",
            "225/240, Train_loss: 0.1537 Train_dice: 0.1537\n",
            "Epoch [14/100], Batch [226/240],  Cross Loss: 0.4897\n",
            "226/240, Train_loss: 0.4897 Train_dice: 0.4897\n",
            "Epoch [14/100], Batch [227/240],  Cross Loss: 0.4419\n",
            "227/240, Train_loss: 0.4419 Train_dice: 0.4419\n",
            "Epoch [14/100], Batch [228/240],  Cross Loss: 0.4689\n",
            "228/240, Train_loss: 0.4689 Train_dice: 0.4689\n",
            "Epoch [14/100], Batch [229/240],  Cross Loss: 0.0650\n",
            "229/240, Train_loss: 0.0650 Train_dice: 0.0650\n",
            "Epoch [14/100], Batch [230/240],  Cross Loss: 0.4847\n",
            "230/240, Train_loss: 0.4847 Train_dice: 0.4847\n",
            "Epoch [14/100], Batch [231/240],  Cross Loss: 0.0081\n",
            "231/240, Train_loss: 0.0081 Train_dice: 0.0081\n",
            "Epoch [14/100], Batch [232/240],  Cross Loss: 0.0065\n",
            "232/240, Train_loss: 0.0065 Train_dice: 0.0065\n",
            "Epoch [14/100], Batch [233/240],  Cross Loss: 0.2361\n",
            "233/240, Train_loss: 0.2361 Train_dice: 0.2361\n",
            "Epoch [14/100], Batch [234/240],  Cross Loss: 0.4427\n",
            "234/240, Train_loss: 0.4427 Train_dice: 0.4427\n",
            "Epoch [14/100], Batch [235/240],  Cross Loss: 0.0862\n",
            "235/240, Train_loss: 0.0862 Train_dice: 0.0862\n",
            "Epoch [14/100], Batch [236/240],  Cross Loss: 0.2807\n",
            "236/240, Train_loss: 0.2807 Train_dice: 0.2807\n",
            "Epoch [14/100], Batch [237/240],  Cross Loss: 0.0026\n",
            "237/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [14/100], Batch [238/240],  Cross Loss: 0.0027\n",
            "238/240, Train_loss: 0.0027 Train_dice: 0.0027\n",
            "Epoch [14/100], Batch [239/240],  Cross Loss: 0.0564\n",
            "239/240, Train_loss: 0.0564 Train_dice: 0.0564\n",
            "Epoch [14/100], Batch [240/240],  Cross Loss: 0.0141\n",
            "240/240, Train_loss: 0.0141 Train_dice: 0.0141\n",
            "--------------------\n",
            "Epoch_loss: 0.2193\n",
            "Epoch_metric: tensor(0.2193, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.4357\n",
            "test_dice_epoch: tensor(0.4357, device='cuda:0')\n",
            "current epoch: 14 current mean dice: tensor(0.4357, device='cuda:0')\n",
            "best mean dice: tensor(0.4407, device='cuda:0') at epoch: 12\n",
            "----------\n",
            "epoch 15/100\n",
            "Epoch [15/100], Batch [1/240],  Cross Loss: 0.0093\n",
            "1/240, Train_loss: 0.0093 Train_dice: 0.0093\n",
            "Epoch [15/100], Batch [2/240],  Cross Loss: 0.0064\n",
            "2/240, Train_loss: 0.0064 Train_dice: 0.0064\n",
            "Epoch [15/100], Batch [3/240],  Cross Loss: 0.3610\n",
            "3/240, Train_loss: 0.3610 Train_dice: 0.3610\n",
            "Epoch [15/100], Batch [4/240],  Cross Loss: 0.3756\n",
            "4/240, Train_loss: 0.3756 Train_dice: 0.3756\n",
            "Epoch [15/100], Batch [5/240],  Cross Loss: 0.3593\n",
            "5/240, Train_loss: 0.3593 Train_dice: 0.3593\n",
            "Epoch [15/100], Batch [6/240],  Cross Loss: 0.0068\n",
            "6/240, Train_loss: 0.0068 Train_dice: 0.0068\n",
            "Epoch [15/100], Batch [7/240],  Cross Loss: 0.0909\n",
            "7/240, Train_loss: 0.0909 Train_dice: 0.0909\n",
            "Epoch [15/100], Batch [8/240],  Cross Loss: 0.0527\n",
            "8/240, Train_loss: 0.0527 Train_dice: 0.0527\n",
            "Epoch [15/100], Batch [9/240],  Cross Loss: 0.2579\n",
            "9/240, Train_loss: 0.2579 Train_dice: 0.2579\n",
            "Epoch [15/100], Batch [10/240],  Cross Loss: 0.0298\n",
            "10/240, Train_loss: 0.0298 Train_dice: 0.0298\n",
            "Epoch [15/100], Batch [11/240],  Cross Loss: 0.0058\n",
            "11/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [15/100], Batch [12/240],  Cross Loss: 0.0933\n",
            "12/240, Train_loss: 0.0933 Train_dice: 0.0933\n",
            "Epoch [15/100], Batch [13/240],  Cross Loss: 0.2807\n",
            "13/240, Train_loss: 0.2807 Train_dice: 0.2807\n",
            "Epoch [15/100], Batch [14/240],  Cross Loss: 0.5502\n",
            "14/240, Train_loss: 0.5502 Train_dice: 0.5502\n",
            "Epoch [15/100], Batch [15/240],  Cross Loss: 0.0984\n",
            "15/240, Train_loss: 0.0984 Train_dice: 0.0984\n",
            "Epoch [15/100], Batch [16/240],  Cross Loss: 0.1600\n",
            "16/240, Train_loss: 0.1600 Train_dice: 0.1600\n",
            "Epoch [15/100], Batch [17/240],  Cross Loss: 0.2006\n",
            "17/240, Train_loss: 0.2006 Train_dice: 0.2006\n",
            "Epoch [15/100], Batch [18/240],  Cross Loss: 0.1878\n",
            "18/240, Train_loss: 0.1878 Train_dice: 0.1878\n",
            "Epoch [15/100], Batch [19/240],  Cross Loss: 0.1835\n",
            "19/240, Train_loss: 0.1835 Train_dice: 0.1835\n",
            "Epoch [15/100], Batch [20/240],  Cross Loss: 0.0115\n",
            "20/240, Train_loss: 0.0115 Train_dice: 0.0115\n",
            "Epoch [15/100], Batch [21/240],  Cross Loss: 0.2018\n",
            "21/240, Train_loss: 0.2018 Train_dice: 0.2018\n",
            "Epoch [15/100], Batch [22/240],  Cross Loss: 0.0072\n",
            "22/240, Train_loss: 0.0072 Train_dice: 0.0072\n",
            "Epoch [15/100], Batch [23/240],  Cross Loss: 0.2536\n",
            "23/240, Train_loss: 0.2536 Train_dice: 0.2536\n",
            "Epoch [15/100], Batch [24/240],  Cross Loss: 0.4716\n",
            "24/240, Train_loss: 0.4716 Train_dice: 0.4716\n",
            "Epoch [15/100], Batch [25/240],  Cross Loss: 0.1329\n",
            "25/240, Train_loss: 0.1329 Train_dice: 0.1329\n",
            "Epoch [15/100], Batch [26/240],  Cross Loss: 0.4170\n",
            "26/240, Train_loss: 0.4170 Train_dice: 0.4170\n",
            "Epoch [15/100], Batch [27/240],  Cross Loss: 0.0033\n",
            "27/240, Train_loss: 0.0033 Train_dice: 0.0033\n",
            "Epoch [15/100], Batch [28/240],  Cross Loss: 0.1376\n",
            "28/240, Train_loss: 0.1376 Train_dice: 0.1376\n",
            "Epoch [15/100], Batch [29/240],  Cross Loss: 0.6197\n",
            "29/240, Train_loss: 0.6197 Train_dice: 0.6197\n",
            "Epoch [15/100], Batch [30/240],  Cross Loss: 0.1385\n",
            "30/240, Train_loss: 0.1385 Train_dice: 0.1385\n",
            "Epoch [15/100], Batch [31/240],  Cross Loss: 0.0129\n",
            "31/240, Train_loss: 0.0129 Train_dice: 0.0129\n",
            "Epoch [15/100], Batch [32/240],  Cross Loss: 0.0058\n",
            "32/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [15/100], Batch [33/240],  Cross Loss: 0.4540\n",
            "33/240, Train_loss: 0.4540 Train_dice: 0.4540\n",
            "Epoch [15/100], Batch [34/240],  Cross Loss: 0.1010\n",
            "34/240, Train_loss: 0.1010 Train_dice: 0.1010\n",
            "Epoch [15/100], Batch [35/240],  Cross Loss: 0.0736\n",
            "35/240, Train_loss: 0.0736 Train_dice: 0.0736\n",
            "Epoch [15/100], Batch [36/240],  Cross Loss: 0.4032\n",
            "36/240, Train_loss: 0.4032 Train_dice: 0.4032\n",
            "Epoch [15/100], Batch [37/240],  Cross Loss: 0.0128\n",
            "37/240, Train_loss: 0.0128 Train_dice: 0.0128\n",
            "Epoch [15/100], Batch [38/240],  Cross Loss: 0.0416\n",
            "38/240, Train_loss: 0.0416 Train_dice: 0.0416\n",
            "Epoch [15/100], Batch [39/240],  Cross Loss: 0.0070\n",
            "39/240, Train_loss: 0.0070 Train_dice: 0.0070\n",
            "Epoch [15/100], Batch [40/240],  Cross Loss: 0.5456\n",
            "40/240, Train_loss: 0.5456 Train_dice: 0.5456\n",
            "Epoch [15/100], Batch [41/240],  Cross Loss: 0.0116\n",
            "41/240, Train_loss: 0.0116 Train_dice: 0.0116\n",
            "Epoch [15/100], Batch [42/240],  Cross Loss: 0.0045\n",
            "42/240, Train_loss: 0.0045 Train_dice: 0.0045\n",
            "Epoch [15/100], Batch [43/240],  Cross Loss: 0.5280\n",
            "43/240, Train_loss: 0.5280 Train_dice: 0.5280\n",
            "Epoch [15/100], Batch [44/240],  Cross Loss: 0.3829\n",
            "44/240, Train_loss: 0.3829 Train_dice: 0.3829\n",
            "Epoch [15/100], Batch [45/240],  Cross Loss: 0.0868\n",
            "45/240, Train_loss: 0.0868 Train_dice: 0.0868\n",
            "Epoch [15/100], Batch [46/240],  Cross Loss: 0.0122\n",
            "46/240, Train_loss: 0.0122 Train_dice: 0.0122\n",
            "Epoch [15/100], Batch [47/240],  Cross Loss: 0.4517\n",
            "47/240, Train_loss: 0.4517 Train_dice: 0.4517\n",
            "Epoch [15/100], Batch [48/240],  Cross Loss: 0.4476\n",
            "48/240, Train_loss: 0.4476 Train_dice: 0.4476\n",
            "Epoch [15/100], Batch [49/240],  Cross Loss: 0.0116\n",
            "49/240, Train_loss: 0.0116 Train_dice: 0.0116\n",
            "Epoch [15/100], Batch [50/240],  Cross Loss: 0.0494\n",
            "50/240, Train_loss: 0.0494 Train_dice: 0.0494\n",
            "Epoch [15/100], Batch [51/240],  Cross Loss: 0.0450\n",
            "51/240, Train_loss: 0.0450 Train_dice: 0.0450\n",
            "Epoch [15/100], Batch [52/240],  Cross Loss: 0.0091\n",
            "52/240, Train_loss: 0.0091 Train_dice: 0.0091\n",
            "Epoch [15/100], Batch [53/240],  Cross Loss: 0.0068\n",
            "53/240, Train_loss: 0.0068 Train_dice: 0.0068\n",
            "Epoch [15/100], Batch [54/240],  Cross Loss: 0.4140\n",
            "54/240, Train_loss: 0.4140 Train_dice: 0.4140\n",
            "Epoch [15/100], Batch [55/240],  Cross Loss: 0.0025\n",
            "55/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [15/100], Batch [56/240],  Cross Loss: 0.0855\n",
            "56/240, Train_loss: 0.0855 Train_dice: 0.0855\n",
            "Epoch [15/100], Batch [57/240],  Cross Loss: 0.2475\n",
            "57/240, Train_loss: 0.2475 Train_dice: 0.2475\n",
            "Epoch [15/100], Batch [58/240],  Cross Loss: 0.4292\n",
            "58/240, Train_loss: 0.4292 Train_dice: 0.4292\n",
            "Epoch [15/100], Batch [59/240],  Cross Loss: 0.4477\n",
            "59/240, Train_loss: 0.4477 Train_dice: 0.4477\n",
            "Epoch [15/100], Batch [60/240],  Cross Loss: 0.6157\n",
            "60/240, Train_loss: 0.6157 Train_dice: 0.6157\n",
            "Epoch [15/100], Batch [61/240],  Cross Loss: 0.0121\n",
            "61/240, Train_loss: 0.0121 Train_dice: 0.0121\n",
            "Epoch [15/100], Batch [62/240],  Cross Loss: 0.0032\n",
            "62/240, Train_loss: 0.0032 Train_dice: 0.0032\n",
            "Epoch [15/100], Batch [63/240],  Cross Loss: 0.3881\n",
            "63/240, Train_loss: 0.3881 Train_dice: 0.3881\n",
            "Epoch [15/100], Batch [64/240],  Cross Loss: 0.0589\n",
            "64/240, Train_loss: 0.0589 Train_dice: 0.0589\n",
            "Epoch [15/100], Batch [65/240],  Cross Loss: 0.1354\n",
            "65/240, Train_loss: 0.1354 Train_dice: 0.1354\n",
            "Epoch [15/100], Batch [66/240],  Cross Loss: 0.3007\n",
            "66/240, Train_loss: 0.3007 Train_dice: 0.3007\n",
            "Epoch [15/100], Batch [67/240],  Cross Loss: 0.0930\n",
            "67/240, Train_loss: 0.0930 Train_dice: 0.0930\n",
            "Epoch [15/100], Batch [68/240],  Cross Loss: 0.1428\n",
            "68/240, Train_loss: 0.1428 Train_dice: 0.1428\n",
            "Epoch [15/100], Batch [69/240],  Cross Loss: 0.5496\n",
            "69/240, Train_loss: 0.5496 Train_dice: 0.5496\n",
            "Epoch [15/100], Batch [70/240],  Cross Loss: 0.0209\n",
            "70/240, Train_loss: 0.0209 Train_dice: 0.0209\n",
            "Epoch [15/100], Batch [71/240],  Cross Loss: 0.1830\n",
            "71/240, Train_loss: 0.1830 Train_dice: 0.1830\n",
            "Epoch [15/100], Batch [72/240],  Cross Loss: 0.0050\n",
            "72/240, Train_loss: 0.0050 Train_dice: 0.0050\n",
            "Epoch [15/100], Batch [73/240],  Cross Loss: 0.0060\n",
            "73/240, Train_loss: 0.0060 Train_dice: 0.0060\n",
            "Epoch [15/100], Batch [74/240],  Cross Loss: 0.0902\n",
            "74/240, Train_loss: 0.0902 Train_dice: 0.0902\n",
            "Epoch [15/100], Batch [75/240],  Cross Loss: 0.1406\n",
            "75/240, Train_loss: 0.1406 Train_dice: 0.1406\n",
            "Epoch [15/100], Batch [76/240],  Cross Loss: 0.2971\n",
            "76/240, Train_loss: 0.2971 Train_dice: 0.2971\n",
            "Epoch [15/100], Batch [77/240],  Cross Loss: 0.0128\n",
            "77/240, Train_loss: 0.0128 Train_dice: 0.0128\n",
            "Epoch [15/100], Batch [78/240],  Cross Loss: 0.3234\n",
            "78/240, Train_loss: 0.3234 Train_dice: 0.3234\n",
            "Epoch [15/100], Batch [79/240],  Cross Loss: 0.0870\n",
            "79/240, Train_loss: 0.0870 Train_dice: 0.0870\n",
            "Epoch [15/100], Batch [80/240],  Cross Loss: 0.5991\n",
            "80/240, Train_loss: 0.5991 Train_dice: 0.5991\n",
            "Epoch [15/100], Batch [81/240],  Cross Loss: 0.1269\n",
            "81/240, Train_loss: 0.1269 Train_dice: 0.1269\n",
            "Epoch [15/100], Batch [82/240],  Cross Loss: 0.0159\n",
            "82/240, Train_loss: 0.0159 Train_dice: 0.0159\n",
            "Epoch [15/100], Batch [83/240],  Cross Loss: 0.2971\n",
            "83/240, Train_loss: 0.2971 Train_dice: 0.2971\n",
            "Epoch [15/100], Batch [84/240],  Cross Loss: 0.0049\n",
            "84/240, Train_loss: 0.0049 Train_dice: 0.0049\n",
            "Epoch [15/100], Batch [85/240],  Cross Loss: 0.0539\n",
            "85/240, Train_loss: 0.0539 Train_dice: 0.0539\n",
            "Epoch [15/100], Batch [86/240],  Cross Loss: 0.0033\n",
            "86/240, Train_loss: 0.0033 Train_dice: 0.0033\n",
            "Epoch [15/100], Batch [87/240],  Cross Loss: 0.2695\n",
            "87/240, Train_loss: 0.2695 Train_dice: 0.2695\n",
            "Epoch [15/100], Batch [88/240],  Cross Loss: 0.0351\n",
            "88/240, Train_loss: 0.0351 Train_dice: 0.0351\n",
            "Epoch [15/100], Batch [89/240],  Cross Loss: 0.1354\n",
            "89/240, Train_loss: 0.1354 Train_dice: 0.1354\n",
            "Epoch [15/100], Batch [90/240],  Cross Loss: 0.2827\n",
            "90/240, Train_loss: 0.2827 Train_dice: 0.2827\n",
            "Epoch [15/100], Batch [91/240],  Cross Loss: 0.5819\n",
            "91/240, Train_loss: 0.5819 Train_dice: 0.5819\n",
            "Epoch [15/100], Batch [92/240],  Cross Loss: 0.0026\n",
            "92/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [15/100], Batch [93/240],  Cross Loss: 0.3099\n",
            "93/240, Train_loss: 0.3099 Train_dice: 0.3099\n",
            "Epoch [15/100], Batch [94/240],  Cross Loss: 0.0042\n",
            "94/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [15/100], Batch [95/240],  Cross Loss: 0.1095\n",
            "95/240, Train_loss: 0.1095 Train_dice: 0.1095\n",
            "Epoch [15/100], Batch [96/240],  Cross Loss: 0.0134\n",
            "96/240, Train_loss: 0.0134 Train_dice: 0.0134\n",
            "Epoch [15/100], Batch [97/240],  Cross Loss: 0.1599\n",
            "97/240, Train_loss: 0.1599 Train_dice: 0.1599\n",
            "Epoch [15/100], Batch [98/240],  Cross Loss: 0.5104\n",
            "98/240, Train_loss: 0.5104 Train_dice: 0.5104\n",
            "Epoch [15/100], Batch [99/240],  Cross Loss: 0.1110\n",
            "99/240, Train_loss: 0.1110 Train_dice: 0.1110\n",
            "Epoch [15/100], Batch [100/240],  Cross Loss: 0.2641\n",
            "100/240, Train_loss: 0.2641 Train_dice: 0.2641\n",
            "Epoch [15/100], Batch [101/240],  Cross Loss: 0.3810\n",
            "101/240, Train_loss: 0.3810 Train_dice: 0.3810\n",
            "Epoch [15/100], Batch [102/240],  Cross Loss: 0.3528\n",
            "102/240, Train_loss: 0.3528 Train_dice: 0.3528\n",
            "Epoch [15/100], Batch [103/240],  Cross Loss: 0.2813\n",
            "103/240, Train_loss: 0.2813 Train_dice: 0.2813\n",
            "Epoch [15/100], Batch [104/240],  Cross Loss: 0.0124\n",
            "104/240, Train_loss: 0.0124 Train_dice: 0.0124\n",
            "Epoch [15/100], Batch [105/240],  Cross Loss: 0.1304\n",
            "105/240, Train_loss: 0.1304 Train_dice: 0.1304\n",
            "Epoch [15/100], Batch [106/240],  Cross Loss: 0.0053\n",
            "106/240, Train_loss: 0.0053 Train_dice: 0.0053\n",
            "Epoch [15/100], Batch [107/240],  Cross Loss: 0.2219\n",
            "107/240, Train_loss: 0.2219 Train_dice: 0.2219\n",
            "Epoch [15/100], Batch [108/240],  Cross Loss: 0.2011\n",
            "108/240, Train_loss: 0.2011 Train_dice: 0.2011\n",
            "Epoch [15/100], Batch [109/240],  Cross Loss: 0.4973\n",
            "109/240, Train_loss: 0.4973 Train_dice: 0.4973\n",
            "Epoch [15/100], Batch [110/240],  Cross Loss: 0.0410\n",
            "110/240, Train_loss: 0.0410 Train_dice: 0.0410\n",
            "Epoch [15/100], Batch [111/240],  Cross Loss: 0.3662\n",
            "111/240, Train_loss: 0.3662 Train_dice: 0.3662\n",
            "Epoch [15/100], Batch [112/240],  Cross Loss: 0.5931\n",
            "112/240, Train_loss: 0.5931 Train_dice: 0.5931\n",
            "Epoch [15/100], Batch [113/240],  Cross Loss: 0.1611\n",
            "113/240, Train_loss: 0.1611 Train_dice: 0.1611\n",
            "Epoch [15/100], Batch [114/240],  Cross Loss: 0.3075\n",
            "114/240, Train_loss: 0.3075 Train_dice: 0.3075\n",
            "Epoch [15/100], Batch [115/240],  Cross Loss: 0.3517\n",
            "115/240, Train_loss: 0.3517 Train_dice: 0.3517\n",
            "Epoch [15/100], Batch [116/240],  Cross Loss: 0.0025\n",
            "116/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [15/100], Batch [117/240],  Cross Loss: 0.2316\n",
            "117/240, Train_loss: 0.2316 Train_dice: 0.2316\n",
            "Epoch [15/100], Batch [118/240],  Cross Loss: 0.0150\n",
            "118/240, Train_loss: 0.0150 Train_dice: 0.0150\n",
            "Epoch [15/100], Batch [119/240],  Cross Loss: 0.1598\n",
            "119/240, Train_loss: 0.1598 Train_dice: 0.1598\n",
            "Epoch [15/100], Batch [120/240],  Cross Loss: 0.3671\n",
            "120/240, Train_loss: 0.3671 Train_dice: 0.3671\n",
            "Epoch [15/100], Batch [121/240],  Cross Loss: 0.5581\n",
            "121/240, Train_loss: 0.5581 Train_dice: 0.5581\n",
            "Epoch [15/100], Batch [122/240],  Cross Loss: 0.2677\n",
            "122/240, Train_loss: 0.2677 Train_dice: 0.2677\n",
            "Epoch [15/100], Batch [123/240],  Cross Loss: 0.1051\n",
            "123/240, Train_loss: 0.1051 Train_dice: 0.1051\n",
            "Epoch [15/100], Batch [124/240],  Cross Loss: 0.1232\n",
            "124/240, Train_loss: 0.1232 Train_dice: 0.1232\n",
            "Epoch [15/100], Batch [125/240],  Cross Loss: 0.1208\n",
            "125/240, Train_loss: 0.1208 Train_dice: 0.1208\n",
            "Epoch [15/100], Batch [126/240],  Cross Loss: 0.0050\n",
            "126/240, Train_loss: 0.0050 Train_dice: 0.0050\n",
            "Epoch [15/100], Batch [127/240],  Cross Loss: 0.1382\n",
            "127/240, Train_loss: 0.1382 Train_dice: 0.1382\n",
            "Epoch [15/100], Batch [128/240],  Cross Loss: 0.0236\n",
            "128/240, Train_loss: 0.0236 Train_dice: 0.0236\n",
            "Epoch [15/100], Batch [129/240],  Cross Loss: 0.7048\n",
            "129/240, Train_loss: 0.7048 Train_dice: 0.7048\n",
            "Epoch [15/100], Batch [130/240],  Cross Loss: 0.2675\n",
            "130/240, Train_loss: 0.2675 Train_dice: 0.2675\n",
            "Epoch [15/100], Batch [131/240],  Cross Loss: 0.3588\n",
            "131/240, Train_loss: 0.3588 Train_dice: 0.3588\n",
            "Epoch [15/100], Batch [132/240],  Cross Loss: 0.3065\n",
            "132/240, Train_loss: 0.3065 Train_dice: 0.3065\n",
            "Epoch [15/100], Batch [133/240],  Cross Loss: 0.0176\n",
            "133/240, Train_loss: 0.0176 Train_dice: 0.0176\n",
            "Epoch [15/100], Batch [134/240],  Cross Loss: 0.1079\n",
            "134/240, Train_loss: 0.1079 Train_dice: 0.1079\n",
            "Epoch [15/100], Batch [135/240],  Cross Loss: 0.0445\n",
            "135/240, Train_loss: 0.0445 Train_dice: 0.0445\n",
            "Epoch [15/100], Batch [136/240],  Cross Loss: 0.0024\n",
            "136/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [15/100], Batch [137/240],  Cross Loss: 0.4987\n",
            "137/240, Train_loss: 0.4987 Train_dice: 0.4987\n",
            "Epoch [15/100], Batch [138/240],  Cross Loss: 0.3647\n",
            "138/240, Train_loss: 0.3647 Train_dice: 0.3647\n",
            "Epoch [15/100], Batch [139/240],  Cross Loss: 0.0123\n",
            "139/240, Train_loss: 0.0123 Train_dice: 0.0123\n",
            "Epoch [15/100], Batch [140/240],  Cross Loss: 0.4405\n",
            "140/240, Train_loss: 0.4405 Train_dice: 0.4405\n",
            "Epoch [15/100], Batch [141/240],  Cross Loss: 0.1442\n",
            "141/240, Train_loss: 0.1442 Train_dice: 0.1442\n",
            "Epoch [15/100], Batch [142/240],  Cross Loss: 0.0054\n",
            "142/240, Train_loss: 0.0054 Train_dice: 0.0054\n",
            "Epoch [15/100], Batch [143/240],  Cross Loss: 0.1254\n",
            "143/240, Train_loss: 0.1254 Train_dice: 0.1254\n",
            "Epoch [15/100], Batch [144/240],  Cross Loss: 0.0842\n",
            "144/240, Train_loss: 0.0842 Train_dice: 0.0842\n",
            "Epoch [15/100], Batch [145/240],  Cross Loss: 0.4461\n",
            "145/240, Train_loss: 0.4461 Train_dice: 0.4461\n",
            "Epoch [15/100], Batch [146/240],  Cross Loss: 0.0846\n",
            "146/240, Train_loss: 0.0846 Train_dice: 0.0846\n",
            "Epoch [15/100], Batch [147/240],  Cross Loss: 0.6087\n",
            "147/240, Train_loss: 0.6087 Train_dice: 0.6087\n",
            "Epoch [15/100], Batch [148/240],  Cross Loss: 0.0123\n",
            "148/240, Train_loss: 0.0123 Train_dice: 0.0123\n",
            "Epoch [15/100], Batch [149/240],  Cross Loss: 0.4804\n",
            "149/240, Train_loss: 0.4804 Train_dice: 0.4804\n",
            "Epoch [15/100], Batch [150/240],  Cross Loss: 0.2416\n",
            "150/240, Train_loss: 0.2416 Train_dice: 0.2416\n",
            "Epoch [15/100], Batch [151/240],  Cross Loss: 0.1184\n",
            "151/240, Train_loss: 0.1184 Train_dice: 0.1184\n",
            "Epoch [15/100], Batch [152/240],  Cross Loss: 0.2420\n",
            "152/240, Train_loss: 0.2420 Train_dice: 0.2420\n",
            "Epoch [15/100], Batch [153/240],  Cross Loss: 0.0067\n",
            "153/240, Train_loss: 0.0067 Train_dice: 0.0067\n",
            "Epoch [15/100], Batch [154/240],  Cross Loss: 0.0649\n",
            "154/240, Train_loss: 0.0649 Train_dice: 0.0649\n",
            "Epoch [15/100], Batch [155/240],  Cross Loss: 0.2354\n",
            "155/240, Train_loss: 0.2354 Train_dice: 0.2354\n",
            "Epoch [15/100], Batch [156/240],  Cross Loss: 0.2608\n",
            "156/240, Train_loss: 0.2608 Train_dice: 0.2608\n",
            "Epoch [15/100], Batch [157/240],  Cross Loss: 0.1882\n",
            "157/240, Train_loss: 0.1882 Train_dice: 0.1882\n",
            "Epoch [15/100], Batch [158/240],  Cross Loss: 0.3011\n",
            "158/240, Train_loss: 0.3011 Train_dice: 0.3011\n",
            "Epoch [15/100], Batch [159/240],  Cross Loss: 0.4138\n",
            "159/240, Train_loss: 0.4138 Train_dice: 0.4138\n",
            "Epoch [15/100], Batch [160/240],  Cross Loss: 0.2831\n",
            "160/240, Train_loss: 0.2831 Train_dice: 0.2831\n",
            "Epoch [15/100], Batch [161/240],  Cross Loss: 0.0159\n",
            "161/240, Train_loss: 0.0159 Train_dice: 0.0159\n",
            "Epoch [15/100], Batch [162/240],  Cross Loss: 0.1785\n",
            "162/240, Train_loss: 0.1785 Train_dice: 0.1785\n",
            "Epoch [15/100], Batch [163/240],  Cross Loss: 0.0026\n",
            "163/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [15/100], Batch [164/240],  Cross Loss: 0.0132\n",
            "164/240, Train_loss: 0.0132 Train_dice: 0.0132\n",
            "Epoch [15/100], Batch [165/240],  Cross Loss: 0.1748\n",
            "165/240, Train_loss: 0.1748 Train_dice: 0.1748\n",
            "Epoch [15/100], Batch [166/240],  Cross Loss: 0.0285\n",
            "166/240, Train_loss: 0.0285 Train_dice: 0.0285\n",
            "Epoch [15/100], Batch [167/240],  Cross Loss: 0.3815\n",
            "167/240, Train_loss: 0.3815 Train_dice: 0.3815\n",
            "Epoch [15/100], Batch [168/240],  Cross Loss: 0.4634\n",
            "168/240, Train_loss: 0.4634 Train_dice: 0.4634\n",
            "Epoch [15/100], Batch [169/240],  Cross Loss: 0.0699\n",
            "169/240, Train_loss: 0.0699 Train_dice: 0.0699\n",
            "Epoch [15/100], Batch [170/240],  Cross Loss: 0.0137\n",
            "170/240, Train_loss: 0.0137 Train_dice: 0.0137\n",
            "Epoch [15/100], Batch [171/240],  Cross Loss: 0.1238\n",
            "171/240, Train_loss: 0.1238 Train_dice: 0.1238\n",
            "Epoch [15/100], Batch [172/240],  Cross Loss: 0.3786\n",
            "172/240, Train_loss: 0.3786 Train_dice: 0.3786\n",
            "Epoch [15/100], Batch [173/240],  Cross Loss: 0.2016\n",
            "173/240, Train_loss: 0.2016 Train_dice: 0.2016\n",
            "Epoch [15/100], Batch [174/240],  Cross Loss: 0.2286\n",
            "174/240, Train_loss: 0.2286 Train_dice: 0.2286\n",
            "Epoch [15/100], Batch [175/240],  Cross Loss: 0.2650\n",
            "175/240, Train_loss: 0.2650 Train_dice: 0.2650\n",
            "Epoch [15/100], Batch [176/240],  Cross Loss: 0.0151\n",
            "176/240, Train_loss: 0.0151 Train_dice: 0.0151\n",
            "Epoch [15/100], Batch [177/240],  Cross Loss: 0.0732\n",
            "177/240, Train_loss: 0.0732 Train_dice: 0.0732\n",
            "Epoch [15/100], Batch [178/240],  Cross Loss: 0.6225\n",
            "178/240, Train_loss: 0.6225 Train_dice: 0.6225\n",
            "Epoch [15/100], Batch [179/240],  Cross Loss: 0.0201\n",
            "179/240, Train_loss: 0.0201 Train_dice: 0.0201\n",
            "Epoch [15/100], Batch [180/240],  Cross Loss: 0.5722\n",
            "180/240, Train_loss: 0.5722 Train_dice: 0.5722\n",
            "Epoch [15/100], Batch [181/240],  Cross Loss: 0.2563\n",
            "181/240, Train_loss: 0.2563 Train_dice: 0.2563\n",
            "Epoch [15/100], Batch [182/240],  Cross Loss: 0.1596\n",
            "182/240, Train_loss: 0.1596 Train_dice: 0.1596\n",
            "Epoch [15/100], Batch [183/240],  Cross Loss: 0.4051\n",
            "183/240, Train_loss: 0.4051 Train_dice: 0.4051\n",
            "Epoch [15/100], Batch [184/240],  Cross Loss: 0.3515\n",
            "184/240, Train_loss: 0.3515 Train_dice: 0.3515\n",
            "Epoch [15/100], Batch [185/240],  Cross Loss: 0.0755\n",
            "185/240, Train_loss: 0.0755 Train_dice: 0.0755\n",
            "Epoch [15/100], Batch [186/240],  Cross Loss: 0.0162\n",
            "186/240, Train_loss: 0.0162 Train_dice: 0.0162\n",
            "Epoch [15/100], Batch [187/240],  Cross Loss: 0.0401\n",
            "187/240, Train_loss: 0.0401 Train_dice: 0.0401\n",
            "Epoch [15/100], Batch [188/240],  Cross Loss: 0.0130\n",
            "188/240, Train_loss: 0.0130 Train_dice: 0.0130\n",
            "Epoch [15/100], Batch [189/240],  Cross Loss: 0.2045\n",
            "189/240, Train_loss: 0.2045 Train_dice: 0.2045\n",
            "Epoch [15/100], Batch [190/240],  Cross Loss: 0.0122\n",
            "190/240, Train_loss: 0.0122 Train_dice: 0.0122\n",
            "Epoch [15/100], Batch [191/240],  Cross Loss: 0.0078\n",
            "191/240, Train_loss: 0.0078 Train_dice: 0.0078\n",
            "Epoch [15/100], Batch [192/240],  Cross Loss: 0.0308\n",
            "192/240, Train_loss: 0.0308 Train_dice: 0.0308\n",
            "Epoch [15/100], Batch [193/240],  Cross Loss: 0.1586\n",
            "193/240, Train_loss: 0.1586 Train_dice: 0.1586\n",
            "Epoch [15/100], Batch [194/240],  Cross Loss: 0.2226\n",
            "194/240, Train_loss: 0.2226 Train_dice: 0.2226\n",
            "Epoch [15/100], Batch [195/240],  Cross Loss: 0.6415\n",
            "195/240, Train_loss: 0.6415 Train_dice: 0.6415\n",
            "Epoch [15/100], Batch [196/240],  Cross Loss: 0.0855\n",
            "196/240, Train_loss: 0.0855 Train_dice: 0.0855\n",
            "Epoch [15/100], Batch [197/240],  Cross Loss: 0.1316\n",
            "197/240, Train_loss: 0.1316 Train_dice: 0.1316\n",
            "Epoch [15/100], Batch [198/240],  Cross Loss: 0.1337\n",
            "198/240, Train_loss: 0.1337 Train_dice: 0.1337\n",
            "Epoch [15/100], Batch [199/240],  Cross Loss: 0.4544\n",
            "199/240, Train_loss: 0.4544 Train_dice: 0.4544\n",
            "Epoch [15/100], Batch [200/240],  Cross Loss: 0.0764\n",
            "200/240, Train_loss: 0.0764 Train_dice: 0.0764\n",
            "Epoch [15/100], Batch [201/240],  Cross Loss: 0.4885\n",
            "201/240, Train_loss: 0.4885 Train_dice: 0.4885\n",
            "Epoch [15/100], Batch [202/240],  Cross Loss: 0.0930\n",
            "202/240, Train_loss: 0.0930 Train_dice: 0.0930\n",
            "Epoch [15/100], Batch [203/240],  Cross Loss: 0.3697\n",
            "203/240, Train_loss: 0.3697 Train_dice: 0.3697\n",
            "Epoch [15/100], Batch [204/240],  Cross Loss: 0.0081\n",
            "204/240, Train_loss: 0.0081 Train_dice: 0.0081\n",
            "Epoch [15/100], Batch [205/240],  Cross Loss: 0.0086\n",
            "205/240, Train_loss: 0.0086 Train_dice: 0.0086\n",
            "Epoch [15/100], Batch [206/240],  Cross Loss: 0.0646\n",
            "206/240, Train_loss: 0.0646 Train_dice: 0.0646\n",
            "Epoch [15/100], Batch [207/240],  Cross Loss: 0.5770\n",
            "207/240, Train_loss: 0.5770 Train_dice: 0.5770\n",
            "Epoch [15/100], Batch [208/240],  Cross Loss: 0.5282\n",
            "208/240, Train_loss: 0.5282 Train_dice: 0.5282\n",
            "Epoch [15/100], Batch [209/240],  Cross Loss: 0.0055\n",
            "209/240, Train_loss: 0.0055 Train_dice: 0.0055\n",
            "Epoch [15/100], Batch [210/240],  Cross Loss: 0.3815\n",
            "210/240, Train_loss: 0.3815 Train_dice: 0.3815\n",
            "Epoch [15/100], Batch [211/240],  Cross Loss: 0.0845\n",
            "211/240, Train_loss: 0.0845 Train_dice: 0.0845\n",
            "Epoch [15/100], Batch [212/240],  Cross Loss: 0.4054\n",
            "212/240, Train_loss: 0.4054 Train_dice: 0.4054\n",
            "Epoch [15/100], Batch [213/240],  Cross Loss: 0.5983\n",
            "213/240, Train_loss: 0.5983 Train_dice: 0.5983\n",
            "Epoch [15/100], Batch [214/240],  Cross Loss: 0.3937\n",
            "214/240, Train_loss: 0.3937 Train_dice: 0.3937\n",
            "Epoch [15/100], Batch [215/240],  Cross Loss: 0.1145\n",
            "215/240, Train_loss: 0.1145 Train_dice: 0.1145\n",
            "Epoch [15/100], Batch [216/240],  Cross Loss: 0.0525\n",
            "216/240, Train_loss: 0.0525 Train_dice: 0.0525\n",
            "Epoch [15/100], Batch [217/240],  Cross Loss: 0.0343\n",
            "217/240, Train_loss: 0.0343 Train_dice: 0.0343\n",
            "Epoch [15/100], Batch [218/240],  Cross Loss: 0.5376\n",
            "218/240, Train_loss: 0.5376 Train_dice: 0.5376\n",
            "Epoch [15/100], Batch [219/240],  Cross Loss: 0.0910\n",
            "219/240, Train_loss: 0.0910 Train_dice: 0.0910\n",
            "Epoch [15/100], Batch [220/240],  Cross Loss: 0.0603\n",
            "220/240, Train_loss: 0.0603 Train_dice: 0.0603\n",
            "Epoch [15/100], Batch [221/240],  Cross Loss: 0.0139\n",
            "221/240, Train_loss: 0.0139 Train_dice: 0.0139\n",
            "Epoch [15/100], Batch [222/240],  Cross Loss: 0.0082\n",
            "222/240, Train_loss: 0.0082 Train_dice: 0.0082\n",
            "Epoch [15/100], Batch [223/240],  Cross Loss: 0.5493\n",
            "223/240, Train_loss: 0.5493 Train_dice: 0.5493\n",
            "Epoch [15/100], Batch [224/240],  Cross Loss: 0.4016\n",
            "224/240, Train_loss: 0.4016 Train_dice: 0.4016\n",
            "Epoch [15/100], Batch [225/240],  Cross Loss: 0.1347\n",
            "225/240, Train_loss: 0.1347 Train_dice: 0.1347\n",
            "Epoch [15/100], Batch [226/240],  Cross Loss: 0.3348\n",
            "226/240, Train_loss: 0.3348 Train_dice: 0.3348\n",
            "Epoch [15/100], Batch [227/240],  Cross Loss: 0.0199\n",
            "227/240, Train_loss: 0.0199 Train_dice: 0.0199\n",
            "Epoch [15/100], Batch [228/240],  Cross Loss: 0.1306\n",
            "228/240, Train_loss: 0.1306 Train_dice: 0.1306\n",
            "Epoch [15/100], Batch [229/240],  Cross Loss: 0.5126\n",
            "229/240, Train_loss: 0.5126 Train_dice: 0.5126\n",
            "Epoch [15/100], Batch [230/240],  Cross Loss: 0.0927\n",
            "230/240, Train_loss: 0.0927 Train_dice: 0.0927\n",
            "Epoch [15/100], Batch [231/240],  Cross Loss: 0.3889\n",
            "231/240, Train_loss: 0.3889 Train_dice: 0.3889\n",
            "Epoch [15/100], Batch [232/240],  Cross Loss: 0.0627\n",
            "232/240, Train_loss: 0.0627 Train_dice: 0.0627\n",
            "Epoch [15/100], Batch [233/240],  Cross Loss: 0.2849\n",
            "233/240, Train_loss: 0.2849 Train_dice: 0.2849\n",
            "Epoch [15/100], Batch [234/240],  Cross Loss: 0.4139\n",
            "234/240, Train_loss: 0.4139 Train_dice: 0.4139\n",
            "Epoch [15/100], Batch [235/240],  Cross Loss: 0.0240\n",
            "235/240, Train_loss: 0.0240 Train_dice: 0.0240\n",
            "Epoch [15/100], Batch [236/240],  Cross Loss: 0.0096\n",
            "236/240, Train_loss: 0.0096 Train_dice: 0.0096\n",
            "Epoch [15/100], Batch [237/240],  Cross Loss: 0.3982\n",
            "237/240, Train_loss: 0.3982 Train_dice: 0.3982\n",
            "Epoch [15/100], Batch [238/240],  Cross Loss: 0.2837\n",
            "238/240, Train_loss: 0.2837 Train_dice: 0.2837\n",
            "Epoch [15/100], Batch [239/240],  Cross Loss: 0.3319\n",
            "239/240, Train_loss: 0.3319 Train_dice: 0.3319\n",
            "Epoch [15/100], Batch [240/240],  Cross Loss: 0.0906\n",
            "240/240, Train_loss: 0.0906 Train_dice: 0.0906\n",
            "--------------------\n",
            "Epoch_loss: 0.2025\n",
            "Epoch_metric: tensor(0.2025, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 16/100\n",
            "Epoch [16/100], Batch [1/240],  Cross Loss: 0.1185\n",
            "1/240, Train_loss: 0.1185 Train_dice: 0.1185\n",
            "Epoch [16/100], Batch [2/240],  Cross Loss: 0.4377\n",
            "2/240, Train_loss: 0.4377 Train_dice: 0.4377\n",
            "Epoch [16/100], Batch [3/240],  Cross Loss: 0.1426\n",
            "3/240, Train_loss: 0.1426 Train_dice: 0.1426\n",
            "Epoch [16/100], Batch [4/240],  Cross Loss: 0.1257\n",
            "4/240, Train_loss: 0.1257 Train_dice: 0.1257\n",
            "Epoch [16/100], Batch [5/240],  Cross Loss: 0.5210\n",
            "5/240, Train_loss: 0.5210 Train_dice: 0.5210\n",
            "Epoch [16/100], Batch [6/240],  Cross Loss: 0.0318\n",
            "6/240, Train_loss: 0.0318 Train_dice: 0.0318\n",
            "Epoch [16/100], Batch [7/240],  Cross Loss: 0.0144\n",
            "7/240, Train_loss: 0.0144 Train_dice: 0.0144\n",
            "Epoch [16/100], Batch [8/240],  Cross Loss: 0.3254\n",
            "8/240, Train_loss: 0.3254 Train_dice: 0.3254\n",
            "Epoch [16/100], Batch [9/240],  Cross Loss: 0.1143\n",
            "9/240, Train_loss: 0.1143 Train_dice: 0.1143\n",
            "Epoch [16/100], Batch [10/240],  Cross Loss: 0.3848\n",
            "10/240, Train_loss: 0.3848 Train_dice: 0.3848\n",
            "Epoch [16/100], Batch [11/240],  Cross Loss: 0.3452\n",
            "11/240, Train_loss: 0.3452 Train_dice: 0.3452\n",
            "Epoch [16/100], Batch [12/240],  Cross Loss: 0.4897\n",
            "12/240, Train_loss: 0.4897 Train_dice: 0.4897\n",
            "Epoch [16/100], Batch [13/240],  Cross Loss: 0.0747\n",
            "13/240, Train_loss: 0.0747 Train_dice: 0.0747\n",
            "Epoch [16/100], Batch [14/240],  Cross Loss: 0.5175\n",
            "14/240, Train_loss: 0.5175 Train_dice: 0.5175\n",
            "Epoch [16/100], Batch [15/240],  Cross Loss: 0.0601\n",
            "15/240, Train_loss: 0.0601 Train_dice: 0.0601\n",
            "Epoch [16/100], Batch [16/240],  Cross Loss: 0.4970\n",
            "16/240, Train_loss: 0.4970 Train_dice: 0.4970\n",
            "Epoch [16/100], Batch [17/240],  Cross Loss: 0.0621\n",
            "17/240, Train_loss: 0.0621 Train_dice: 0.0621\n",
            "Epoch [16/100], Batch [18/240],  Cross Loss: 0.0239\n",
            "18/240, Train_loss: 0.0239 Train_dice: 0.0239\n",
            "Epoch [16/100], Batch [19/240],  Cross Loss: 0.0104\n",
            "19/240, Train_loss: 0.0104 Train_dice: 0.0104\n",
            "Epoch [16/100], Batch [20/240],  Cross Loss: 0.3343\n",
            "20/240, Train_loss: 0.3343 Train_dice: 0.3343\n",
            "Epoch [16/100], Batch [21/240],  Cross Loss: 0.0464\n",
            "21/240, Train_loss: 0.0464 Train_dice: 0.0464\n",
            "Epoch [16/100], Batch [22/240],  Cross Loss: 0.2491\n",
            "22/240, Train_loss: 0.2491 Train_dice: 0.2491\n",
            "Epoch [16/100], Batch [23/240],  Cross Loss: 0.0213\n",
            "23/240, Train_loss: 0.0213 Train_dice: 0.0213\n",
            "Epoch [16/100], Batch [24/240],  Cross Loss: 0.4449\n",
            "24/240, Train_loss: 0.4449 Train_dice: 0.4449\n",
            "Epoch [16/100], Batch [25/240],  Cross Loss: 0.0688\n",
            "25/240, Train_loss: 0.0688 Train_dice: 0.0688\n",
            "Epoch [16/100], Batch [26/240],  Cross Loss: 0.3529\n",
            "26/240, Train_loss: 0.3529 Train_dice: 0.3529\n",
            "Epoch [16/100], Batch [27/240],  Cross Loss: 0.2091\n",
            "27/240, Train_loss: 0.2091 Train_dice: 0.2091\n",
            "Epoch [16/100], Batch [28/240],  Cross Loss: 0.4329\n",
            "28/240, Train_loss: 0.4329 Train_dice: 0.4329\n",
            "Epoch [16/100], Batch [29/240],  Cross Loss: 0.3278\n",
            "29/240, Train_loss: 0.3278 Train_dice: 0.3278\n",
            "Epoch [16/100], Batch [30/240],  Cross Loss: 0.2081\n",
            "30/240, Train_loss: 0.2081 Train_dice: 0.2081\n",
            "Epoch [16/100], Batch [31/240],  Cross Loss: 0.3712\n",
            "31/240, Train_loss: 0.3712 Train_dice: 0.3712\n",
            "Epoch [16/100], Batch [32/240],  Cross Loss: 0.0682\n",
            "32/240, Train_loss: 0.0682 Train_dice: 0.0682\n",
            "Epoch [16/100], Batch [33/240],  Cross Loss: 0.0612\n",
            "33/240, Train_loss: 0.0612 Train_dice: 0.0612\n",
            "Epoch [16/100], Batch [34/240],  Cross Loss: 0.2214\n",
            "34/240, Train_loss: 0.2214 Train_dice: 0.2214\n",
            "Epoch [16/100], Batch [35/240],  Cross Loss: 0.0285\n",
            "35/240, Train_loss: 0.0285 Train_dice: 0.0285\n",
            "Epoch [16/100], Batch [36/240],  Cross Loss: 0.0306\n",
            "36/240, Train_loss: 0.0306 Train_dice: 0.0306\n",
            "Epoch [16/100], Batch [37/240],  Cross Loss: 0.0993\n",
            "37/240, Train_loss: 0.0993 Train_dice: 0.0993\n",
            "Epoch [16/100], Batch [38/240],  Cross Loss: 0.0943\n",
            "38/240, Train_loss: 0.0943 Train_dice: 0.0943\n",
            "Epoch [16/100], Batch [39/240],  Cross Loss: 0.0436\n",
            "39/240, Train_loss: 0.0436 Train_dice: 0.0436\n",
            "Epoch [16/100], Batch [40/240],  Cross Loss: 0.0041\n",
            "40/240, Train_loss: 0.0041 Train_dice: 0.0041\n",
            "Epoch [16/100], Batch [41/240],  Cross Loss: 0.2433\n",
            "41/240, Train_loss: 0.2433 Train_dice: 0.2433\n",
            "Epoch [16/100], Batch [42/240],  Cross Loss: 0.1988\n",
            "42/240, Train_loss: 0.1988 Train_dice: 0.1988\n",
            "Epoch [16/100], Batch [43/240],  Cross Loss: 0.0245\n",
            "43/240, Train_loss: 0.0245 Train_dice: 0.0245\n",
            "Epoch [16/100], Batch [44/240],  Cross Loss: 0.0225\n",
            "44/240, Train_loss: 0.0225 Train_dice: 0.0225\n",
            "Epoch [16/100], Batch [45/240],  Cross Loss: 0.0025\n",
            "45/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [16/100], Batch [46/240],  Cross Loss: 0.6040\n",
            "46/240, Train_loss: 0.6040 Train_dice: 0.6040\n",
            "Epoch [16/100], Batch [47/240],  Cross Loss: 0.2471\n",
            "47/240, Train_loss: 0.2471 Train_dice: 0.2471\n",
            "Epoch [16/100], Batch [48/240],  Cross Loss: 0.4598\n",
            "48/240, Train_loss: 0.4598 Train_dice: 0.4598\n",
            "Epoch [16/100], Batch [49/240],  Cross Loss: 0.2116\n",
            "49/240, Train_loss: 0.2116 Train_dice: 0.2116\n",
            "Epoch [16/100], Batch [50/240],  Cross Loss: 0.2693\n",
            "50/240, Train_loss: 0.2693 Train_dice: 0.2693\n",
            "Epoch [16/100], Batch [51/240],  Cross Loss: 0.3573\n",
            "51/240, Train_loss: 0.3573 Train_dice: 0.3573\n",
            "Epoch [16/100], Batch [52/240],  Cross Loss: 0.4393\n",
            "52/240, Train_loss: 0.4393 Train_dice: 0.4393\n",
            "Epoch [16/100], Batch [53/240],  Cross Loss: 0.0798\n",
            "53/240, Train_loss: 0.0798 Train_dice: 0.0798\n",
            "Epoch [16/100], Batch [54/240],  Cross Loss: 0.2046\n",
            "54/240, Train_loss: 0.2046 Train_dice: 0.2046\n",
            "Epoch [16/100], Batch [55/240],  Cross Loss: 0.0088\n",
            "55/240, Train_loss: 0.0088 Train_dice: 0.0088\n",
            "Epoch [16/100], Batch [56/240],  Cross Loss: 0.2489\n",
            "56/240, Train_loss: 0.2489 Train_dice: 0.2489\n",
            "Epoch [16/100], Batch [57/240],  Cross Loss: 0.5507\n",
            "57/240, Train_loss: 0.5507 Train_dice: 0.5507\n",
            "Epoch [16/100], Batch [58/240],  Cross Loss: 0.3082\n",
            "58/240, Train_loss: 0.3082 Train_dice: 0.3082\n",
            "Epoch [16/100], Batch [59/240],  Cross Loss: 0.0428\n",
            "59/240, Train_loss: 0.0428 Train_dice: 0.0428\n",
            "Epoch [16/100], Batch [60/240],  Cross Loss: 0.2139\n",
            "60/240, Train_loss: 0.2139 Train_dice: 0.2139\n",
            "Epoch [16/100], Batch [61/240],  Cross Loss: 0.0061\n",
            "61/240, Train_loss: 0.0061 Train_dice: 0.0061\n",
            "Epoch [16/100], Batch [62/240],  Cross Loss: 0.0172\n",
            "62/240, Train_loss: 0.0172 Train_dice: 0.0172\n",
            "Epoch [16/100], Batch [63/240],  Cross Loss: 0.1042\n",
            "63/240, Train_loss: 0.1042 Train_dice: 0.1042\n",
            "Epoch [16/100], Batch [64/240],  Cross Loss: 0.0086\n",
            "64/240, Train_loss: 0.0086 Train_dice: 0.0086\n",
            "Epoch [16/100], Batch [65/240],  Cross Loss: 0.0062\n",
            "65/240, Train_loss: 0.0062 Train_dice: 0.0062\n",
            "Epoch [16/100], Batch [66/240],  Cross Loss: 0.4459\n",
            "66/240, Train_loss: 0.4459 Train_dice: 0.4459\n",
            "Epoch [16/100], Batch [67/240],  Cross Loss: 0.0051\n",
            "67/240, Train_loss: 0.0051 Train_dice: 0.0051\n",
            "Epoch [16/100], Batch [68/240],  Cross Loss: 0.4975\n",
            "68/240, Train_loss: 0.4975 Train_dice: 0.4975\n",
            "Epoch [16/100], Batch [69/240],  Cross Loss: 0.0014\n",
            "69/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [16/100], Batch [70/240],  Cross Loss: 0.0037\n",
            "70/240, Train_loss: 0.0037 Train_dice: 0.0037\n",
            "Epoch [16/100], Batch [71/240],  Cross Loss: 0.3580\n",
            "71/240, Train_loss: 0.3580 Train_dice: 0.3580\n",
            "Epoch [16/100], Batch [72/240],  Cross Loss: 0.0260\n",
            "72/240, Train_loss: 0.0260 Train_dice: 0.0260\n",
            "Epoch [16/100], Batch [73/240],  Cross Loss: 0.2431\n",
            "73/240, Train_loss: 0.2431 Train_dice: 0.2431\n",
            "Epoch [16/100], Batch [74/240],  Cross Loss: 0.0035\n",
            "74/240, Train_loss: 0.0035 Train_dice: 0.0035\n",
            "Epoch [16/100], Batch [75/240],  Cross Loss: 0.3861\n",
            "75/240, Train_loss: 0.3861 Train_dice: 0.3861\n",
            "Epoch [16/100], Batch [76/240],  Cross Loss: 0.1178\n",
            "76/240, Train_loss: 0.1178 Train_dice: 0.1178\n",
            "Epoch [16/100], Batch [77/240],  Cross Loss: 0.1883\n",
            "77/240, Train_loss: 0.1883 Train_dice: 0.1883\n",
            "Epoch [16/100], Batch [78/240],  Cross Loss: 0.1063\n",
            "78/240, Train_loss: 0.1063 Train_dice: 0.1063\n",
            "Epoch [16/100], Batch [79/240],  Cross Loss: 0.0041\n",
            "79/240, Train_loss: 0.0041 Train_dice: 0.0041\n",
            "Epoch [16/100], Batch [80/240],  Cross Loss: 0.0788\n",
            "80/240, Train_loss: 0.0788 Train_dice: 0.0788\n",
            "Epoch [16/100], Batch [81/240],  Cross Loss: 0.0085\n",
            "81/240, Train_loss: 0.0085 Train_dice: 0.0085\n",
            "Epoch [16/100], Batch [82/240],  Cross Loss: 0.3287\n",
            "82/240, Train_loss: 0.3287 Train_dice: 0.3287\n",
            "Epoch [16/100], Batch [83/240],  Cross Loss: 0.3656\n",
            "83/240, Train_loss: 0.3656 Train_dice: 0.3656\n",
            "Epoch [16/100], Batch [84/240],  Cross Loss: 0.1931\n",
            "84/240, Train_loss: 0.1931 Train_dice: 0.1931\n",
            "Epoch [16/100], Batch [85/240],  Cross Loss: 0.4911\n",
            "85/240, Train_loss: 0.4911 Train_dice: 0.4911\n",
            "Epoch [16/100], Batch [86/240],  Cross Loss: 0.0055\n",
            "86/240, Train_loss: 0.0055 Train_dice: 0.0055\n",
            "Epoch [16/100], Batch [87/240],  Cross Loss: 0.3229\n",
            "87/240, Train_loss: 0.3229 Train_dice: 0.3229\n",
            "Epoch [16/100], Batch [88/240],  Cross Loss: 0.0379\n",
            "88/240, Train_loss: 0.0379 Train_dice: 0.0379\n",
            "Epoch [16/100], Batch [89/240],  Cross Loss: 0.0035\n",
            "89/240, Train_loss: 0.0035 Train_dice: 0.0035\n",
            "Epoch [16/100], Batch [90/240],  Cross Loss: 0.3626\n",
            "90/240, Train_loss: 0.3626 Train_dice: 0.3626\n",
            "Epoch [16/100], Batch [91/240],  Cross Loss: 0.0018\n",
            "91/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [16/100], Batch [92/240],  Cross Loss: 0.0741\n",
            "92/240, Train_loss: 0.0741 Train_dice: 0.0741\n",
            "Epoch [16/100], Batch [93/240],  Cross Loss: 0.1304\n",
            "93/240, Train_loss: 0.1304 Train_dice: 0.1304\n",
            "Epoch [16/100], Batch [94/240],  Cross Loss: 0.5880\n",
            "94/240, Train_loss: 0.5880 Train_dice: 0.5880\n",
            "Epoch [16/100], Batch [95/240],  Cross Loss: 0.1179\n",
            "95/240, Train_loss: 0.1179 Train_dice: 0.1179\n",
            "Epoch [16/100], Batch [96/240],  Cross Loss: 0.2594\n",
            "96/240, Train_loss: 0.2594 Train_dice: 0.2594\n",
            "Epoch [16/100], Batch [97/240],  Cross Loss: 0.0054\n",
            "97/240, Train_loss: 0.0054 Train_dice: 0.0054\n",
            "Epoch [16/100], Batch [98/240],  Cross Loss: 0.0201\n",
            "98/240, Train_loss: 0.0201 Train_dice: 0.0201\n",
            "Epoch [16/100], Batch [99/240],  Cross Loss: 0.5500\n",
            "99/240, Train_loss: 0.5500 Train_dice: 0.5500\n",
            "Epoch [16/100], Batch [100/240],  Cross Loss: 0.2487\n",
            "100/240, Train_loss: 0.2487 Train_dice: 0.2487\n",
            "Epoch [16/100], Batch [101/240],  Cross Loss: 0.2907\n",
            "101/240, Train_loss: 0.2907 Train_dice: 0.2907\n",
            "Epoch [16/100], Batch [102/240],  Cross Loss: 0.3802\n",
            "102/240, Train_loss: 0.3802 Train_dice: 0.3802\n",
            "Epoch [16/100], Batch [103/240],  Cross Loss: 0.0411\n",
            "103/240, Train_loss: 0.0411 Train_dice: 0.0411\n",
            "Epoch [16/100], Batch [104/240],  Cross Loss: 0.1095\n",
            "104/240, Train_loss: 0.1095 Train_dice: 0.1095\n",
            "Epoch [16/100], Batch [105/240],  Cross Loss: 0.0173\n",
            "105/240, Train_loss: 0.0173 Train_dice: 0.0173\n",
            "Epoch [16/100], Batch [106/240],  Cross Loss: 0.2839\n",
            "106/240, Train_loss: 0.2839 Train_dice: 0.2839\n",
            "Epoch [16/100], Batch [107/240],  Cross Loss: 0.0645\n",
            "107/240, Train_loss: 0.0645 Train_dice: 0.0645\n",
            "Epoch [16/100], Batch [108/240],  Cross Loss: 0.0384\n",
            "108/240, Train_loss: 0.0384 Train_dice: 0.0384\n",
            "Epoch [16/100], Batch [109/240],  Cross Loss: 0.0900\n",
            "109/240, Train_loss: 0.0900 Train_dice: 0.0900\n",
            "Epoch [16/100], Batch [110/240],  Cross Loss: 0.2401\n",
            "110/240, Train_loss: 0.2401 Train_dice: 0.2401\n",
            "Epoch [16/100], Batch [111/240],  Cross Loss: 0.1264\n",
            "111/240, Train_loss: 0.1264 Train_dice: 0.1264\n",
            "Epoch [16/100], Batch [112/240],  Cross Loss: 0.2242\n",
            "112/240, Train_loss: 0.2242 Train_dice: 0.2242\n",
            "Epoch [16/100], Batch [113/240],  Cross Loss: 0.2493\n",
            "113/240, Train_loss: 0.2493 Train_dice: 0.2493\n",
            "Epoch [16/100], Batch [114/240],  Cross Loss: 0.0947\n",
            "114/240, Train_loss: 0.0947 Train_dice: 0.0947\n",
            "Epoch [16/100], Batch [115/240],  Cross Loss: 0.0104\n",
            "115/240, Train_loss: 0.0104 Train_dice: 0.0104\n",
            "Epoch [16/100], Batch [116/240],  Cross Loss: 0.0581\n",
            "116/240, Train_loss: 0.0581 Train_dice: 0.0581\n",
            "Epoch [16/100], Batch [117/240],  Cross Loss: 0.0045\n",
            "117/240, Train_loss: 0.0045 Train_dice: 0.0045\n",
            "Epoch [16/100], Batch [118/240],  Cross Loss: 0.2222\n",
            "118/240, Train_loss: 0.2222 Train_dice: 0.2222\n",
            "Epoch [16/100], Batch [119/240],  Cross Loss: 0.0057\n",
            "119/240, Train_loss: 0.0057 Train_dice: 0.0057\n",
            "Epoch [16/100], Batch [120/240],  Cross Loss: 0.2580\n",
            "120/240, Train_loss: 0.2580 Train_dice: 0.2580\n",
            "Epoch [16/100], Batch [121/240],  Cross Loss: 0.1290\n",
            "121/240, Train_loss: 0.1290 Train_dice: 0.1290\n",
            "Epoch [16/100], Batch [122/240],  Cross Loss: 0.2797\n",
            "122/240, Train_loss: 0.2797 Train_dice: 0.2797\n",
            "Epoch [16/100], Batch [123/240],  Cross Loss: 0.0115\n",
            "123/240, Train_loss: 0.0115 Train_dice: 0.0115\n",
            "Epoch [16/100], Batch [124/240],  Cross Loss: 0.1329\n",
            "124/240, Train_loss: 0.1329 Train_dice: 0.1329\n",
            "Epoch [16/100], Batch [125/240],  Cross Loss: 0.0038\n",
            "125/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [16/100], Batch [126/240],  Cross Loss: 0.0038\n",
            "126/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [16/100], Batch [127/240],  Cross Loss: 0.0720\n",
            "127/240, Train_loss: 0.0720 Train_dice: 0.0720\n",
            "Epoch [16/100], Batch [128/240],  Cross Loss: 0.0077\n",
            "128/240, Train_loss: 0.0077 Train_dice: 0.0077\n",
            "Epoch [16/100], Batch [129/240],  Cross Loss: 0.0058\n",
            "129/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [16/100], Batch [130/240],  Cross Loss: 0.0038\n",
            "130/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [16/100], Batch [131/240],  Cross Loss: 0.6336\n",
            "131/240, Train_loss: 0.6336 Train_dice: 0.6336\n",
            "Epoch [16/100], Batch [132/240],  Cross Loss: 0.0357\n",
            "132/240, Train_loss: 0.0357 Train_dice: 0.0357\n",
            "Epoch [16/100], Batch [133/240],  Cross Loss: 0.0738\n",
            "133/240, Train_loss: 0.0738 Train_dice: 0.0738\n",
            "Epoch [16/100], Batch [134/240],  Cross Loss: 0.0774\n",
            "134/240, Train_loss: 0.0774 Train_dice: 0.0774\n",
            "Epoch [16/100], Batch [135/240],  Cross Loss: 0.0426\n",
            "135/240, Train_loss: 0.0426 Train_dice: 0.0426\n",
            "Epoch [16/100], Batch [136/240],  Cross Loss: 0.0214\n",
            "136/240, Train_loss: 0.0214 Train_dice: 0.0214\n",
            "Epoch [16/100], Batch [137/240],  Cross Loss: 0.0207\n",
            "137/240, Train_loss: 0.0207 Train_dice: 0.0207\n",
            "Epoch [16/100], Batch [138/240],  Cross Loss: 0.1277\n",
            "138/240, Train_loss: 0.1277 Train_dice: 0.1277\n",
            "Epoch [16/100], Batch [139/240],  Cross Loss: 0.0240\n",
            "139/240, Train_loss: 0.0240 Train_dice: 0.0240\n",
            "Epoch [16/100], Batch [140/240],  Cross Loss: 0.0032\n",
            "140/240, Train_loss: 0.0032 Train_dice: 0.0032\n",
            "Epoch [16/100], Batch [141/240],  Cross Loss: 0.3959\n",
            "141/240, Train_loss: 0.3959 Train_dice: 0.3959\n",
            "Epoch [16/100], Batch [142/240],  Cross Loss: 0.3043\n",
            "142/240, Train_loss: 0.3043 Train_dice: 0.3043\n",
            "Epoch [16/100], Batch [143/240],  Cross Loss: 0.0865\n",
            "143/240, Train_loss: 0.0865 Train_dice: 0.0865\n",
            "Epoch [16/100], Batch [144/240],  Cross Loss: 0.2680\n",
            "144/240, Train_loss: 0.2680 Train_dice: 0.2680\n",
            "Epoch [16/100], Batch [145/240],  Cross Loss: 0.4257\n",
            "145/240, Train_loss: 0.4257 Train_dice: 0.4257\n",
            "Epoch [16/100], Batch [146/240],  Cross Loss: 0.0076\n",
            "146/240, Train_loss: 0.0076 Train_dice: 0.0076\n",
            "Epoch [16/100], Batch [147/240],  Cross Loss: 0.0028\n",
            "147/240, Train_loss: 0.0028 Train_dice: 0.0028\n",
            "Epoch [16/100], Batch [148/240],  Cross Loss: 0.0606\n",
            "148/240, Train_loss: 0.0606 Train_dice: 0.0606\n",
            "Epoch [16/100], Batch [149/240],  Cross Loss: 0.0074\n",
            "149/240, Train_loss: 0.0074 Train_dice: 0.0074\n",
            "Epoch [16/100], Batch [150/240],  Cross Loss: 0.4022\n",
            "150/240, Train_loss: 0.4022 Train_dice: 0.4022\n",
            "Epoch [16/100], Batch [151/240],  Cross Loss: 0.2649\n",
            "151/240, Train_loss: 0.2649 Train_dice: 0.2649\n",
            "Epoch [16/100], Batch [152/240],  Cross Loss: 0.3857\n",
            "152/240, Train_loss: 0.3857 Train_dice: 0.3857\n",
            "Epoch [16/100], Batch [153/240],  Cross Loss: 0.3198\n",
            "153/240, Train_loss: 0.3198 Train_dice: 0.3198\n",
            "Epoch [16/100], Batch [154/240],  Cross Loss: 0.0031\n",
            "154/240, Train_loss: 0.0031 Train_dice: 0.0031\n",
            "Epoch [16/100], Batch [155/240],  Cross Loss: 0.6731\n",
            "155/240, Train_loss: 0.6731 Train_dice: 0.6731\n",
            "Epoch [16/100], Batch [156/240],  Cross Loss: 0.0019\n",
            "156/240, Train_loss: 0.0019 Train_dice: 0.0019\n",
            "Epoch [16/100], Batch [157/240],  Cross Loss: 0.5026\n",
            "157/240, Train_loss: 0.5026 Train_dice: 0.5026\n",
            "Epoch [16/100], Batch [158/240],  Cross Loss: 0.1805\n",
            "158/240, Train_loss: 0.1805 Train_dice: 0.1805\n",
            "Epoch [16/100], Batch [159/240],  Cross Loss: 0.3848\n",
            "159/240, Train_loss: 0.3848 Train_dice: 0.3848\n",
            "Epoch [16/100], Batch [160/240],  Cross Loss: 0.0131\n",
            "160/240, Train_loss: 0.0131 Train_dice: 0.0131\n",
            "Epoch [16/100], Batch [161/240],  Cross Loss: 0.0092\n",
            "161/240, Train_loss: 0.0092 Train_dice: 0.0092\n",
            "Epoch [16/100], Batch [162/240],  Cross Loss: 0.0019\n",
            "162/240, Train_loss: 0.0019 Train_dice: 0.0019\n",
            "Epoch [16/100], Batch [163/240],  Cross Loss: 0.3298\n",
            "163/240, Train_loss: 0.3298 Train_dice: 0.3298\n",
            "Epoch [16/100], Batch [164/240],  Cross Loss: 0.5078\n",
            "164/240, Train_loss: 0.5078 Train_dice: 0.5078\n",
            "Epoch [16/100], Batch [165/240],  Cross Loss: 0.3775\n",
            "165/240, Train_loss: 0.3775 Train_dice: 0.3775\n",
            "Epoch [16/100], Batch [166/240],  Cross Loss: 0.2621\n",
            "166/240, Train_loss: 0.2621 Train_dice: 0.2621\n",
            "Epoch [16/100], Batch [167/240],  Cross Loss: 0.0607\n",
            "167/240, Train_loss: 0.0607 Train_dice: 0.0607\n",
            "Epoch [16/100], Batch [168/240],  Cross Loss: 0.3144\n",
            "168/240, Train_loss: 0.3144 Train_dice: 0.3144\n",
            "Epoch [16/100], Batch [169/240],  Cross Loss: 0.2449\n",
            "169/240, Train_loss: 0.2449 Train_dice: 0.2449\n",
            "Epoch [16/100], Batch [170/240],  Cross Loss: 0.7829\n",
            "170/240, Train_loss: 0.7829 Train_dice: 0.7829\n",
            "Epoch [16/100], Batch [171/240],  Cross Loss: 0.4549\n",
            "171/240, Train_loss: 0.4549 Train_dice: 0.4549\n",
            "Epoch [16/100], Batch [172/240],  Cross Loss: 0.0371\n",
            "172/240, Train_loss: 0.0371 Train_dice: 0.0371\n",
            "Epoch [16/100], Batch [173/240],  Cross Loss: 0.3120\n",
            "173/240, Train_loss: 0.3120 Train_dice: 0.3120\n",
            "Epoch [16/100], Batch [174/240],  Cross Loss: 0.1230\n",
            "174/240, Train_loss: 0.1230 Train_dice: 0.1230\n",
            "Epoch [16/100], Batch [175/240],  Cross Loss: 0.4261\n",
            "175/240, Train_loss: 0.4261 Train_dice: 0.4261\n",
            "Epoch [16/100], Batch [176/240],  Cross Loss: 0.3899\n",
            "176/240, Train_loss: 0.3899 Train_dice: 0.3899\n",
            "Epoch [16/100], Batch [177/240],  Cross Loss: 0.0025\n",
            "177/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [16/100], Batch [178/240],  Cross Loss: 0.2134\n",
            "178/240, Train_loss: 0.2134 Train_dice: 0.2134\n",
            "Epoch [16/100], Batch [179/240],  Cross Loss: 0.0020\n",
            "179/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [16/100], Batch [180/240],  Cross Loss: 0.3896\n",
            "180/240, Train_loss: 0.3896 Train_dice: 0.3896\n",
            "Epoch [16/100], Batch [181/240],  Cross Loss: 0.0026\n",
            "181/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [16/100], Batch [182/240],  Cross Loss: 0.1853\n",
            "182/240, Train_loss: 0.1853 Train_dice: 0.1853\n",
            "Epoch [16/100], Batch [183/240],  Cross Loss: 0.0016\n",
            "183/240, Train_loss: 0.0016 Train_dice: 0.0016\n",
            "Epoch [16/100], Batch [184/240],  Cross Loss: 0.3121\n",
            "184/240, Train_loss: 0.3121 Train_dice: 0.3121\n",
            "Epoch [16/100], Batch [185/240],  Cross Loss: 0.0322\n",
            "185/240, Train_loss: 0.0322 Train_dice: 0.0322\n",
            "Epoch [16/100], Batch [186/240],  Cross Loss: 0.2818\n",
            "186/240, Train_loss: 0.2818 Train_dice: 0.2818\n",
            "Epoch [16/100], Batch [187/240],  Cross Loss: 0.3379\n",
            "187/240, Train_loss: 0.3379 Train_dice: 0.3379\n",
            "Epoch [16/100], Batch [188/240],  Cross Loss: 0.0049\n",
            "188/240, Train_loss: 0.0049 Train_dice: 0.0049\n",
            "Epoch [16/100], Batch [189/240],  Cross Loss: 0.1725\n",
            "189/240, Train_loss: 0.1725 Train_dice: 0.1725\n",
            "Epoch [16/100], Batch [190/240],  Cross Loss: 0.0035\n",
            "190/240, Train_loss: 0.0035 Train_dice: 0.0035\n",
            "Epoch [16/100], Batch [191/240],  Cross Loss: 0.0852\n",
            "191/240, Train_loss: 0.0852 Train_dice: 0.0852\n",
            "Epoch [16/100], Batch [192/240],  Cross Loss: 0.0021\n",
            "192/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [16/100], Batch [193/240],  Cross Loss: 0.3093\n",
            "193/240, Train_loss: 0.3093 Train_dice: 0.3093\n",
            "Epoch [16/100], Batch [194/240],  Cross Loss: 0.0982\n",
            "194/240, Train_loss: 0.0982 Train_dice: 0.0982\n",
            "Epoch [16/100], Batch [195/240],  Cross Loss: 0.4037\n",
            "195/240, Train_loss: 0.4037 Train_dice: 0.4037\n",
            "Epoch [16/100], Batch [196/240],  Cross Loss: 0.0083\n",
            "196/240, Train_loss: 0.0083 Train_dice: 0.0083\n",
            "Epoch [16/100], Batch [197/240],  Cross Loss: 0.3037\n",
            "197/240, Train_loss: 0.3037 Train_dice: 0.3037\n",
            "Epoch [16/100], Batch [198/240],  Cross Loss: 0.1380\n",
            "198/240, Train_loss: 0.1380 Train_dice: 0.1380\n",
            "Epoch [16/100], Batch [199/240],  Cross Loss: 0.1790\n",
            "199/240, Train_loss: 0.1790 Train_dice: 0.1790\n",
            "Epoch [16/100], Batch [200/240],  Cross Loss: 0.1363\n",
            "200/240, Train_loss: 0.1363 Train_dice: 0.1363\n",
            "Epoch [16/100], Batch [201/240],  Cross Loss: 0.1371\n",
            "201/240, Train_loss: 0.1371 Train_dice: 0.1371\n",
            "Epoch [16/100], Batch [202/240],  Cross Loss: 0.0337\n",
            "202/240, Train_loss: 0.0337 Train_dice: 0.0337\n",
            "Epoch [16/100], Batch [203/240],  Cross Loss: 0.4800\n",
            "203/240, Train_loss: 0.4800 Train_dice: 0.4800\n",
            "Epoch [16/100], Batch [204/240],  Cross Loss: 0.0038\n",
            "204/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [16/100], Batch [205/240],  Cross Loss: 0.5707\n",
            "205/240, Train_loss: 0.5707 Train_dice: 0.5707\n",
            "Epoch [16/100], Batch [206/240],  Cross Loss: 0.0125\n",
            "206/240, Train_loss: 0.0125 Train_dice: 0.0125\n",
            "Epoch [16/100], Batch [207/240],  Cross Loss: 0.0069\n",
            "207/240, Train_loss: 0.0069 Train_dice: 0.0069\n",
            "Epoch [16/100], Batch [208/240],  Cross Loss: 0.0083\n",
            "208/240, Train_loss: 0.0083 Train_dice: 0.0083\n",
            "Epoch [16/100], Batch [209/240],  Cross Loss: 0.0833\n",
            "209/240, Train_loss: 0.0833 Train_dice: 0.0833\n",
            "Epoch [16/100], Batch [210/240],  Cross Loss: 0.3107\n",
            "210/240, Train_loss: 0.3107 Train_dice: 0.3107\n",
            "Epoch [16/100], Batch [211/240],  Cross Loss: 0.0634\n",
            "211/240, Train_loss: 0.0634 Train_dice: 0.0634\n",
            "Epoch [16/100], Batch [212/240],  Cross Loss: 0.6380\n",
            "212/240, Train_loss: 0.6380 Train_dice: 0.6380\n",
            "Epoch [16/100], Batch [213/240],  Cross Loss: 0.0544\n",
            "213/240, Train_loss: 0.0544 Train_dice: 0.0544\n",
            "Epoch [16/100], Batch [214/240],  Cross Loss: 0.3471\n",
            "214/240, Train_loss: 0.3471 Train_dice: 0.3471\n",
            "Epoch [16/100], Batch [215/240],  Cross Loss: 0.5078\n",
            "215/240, Train_loss: 0.5078 Train_dice: 0.5078\n",
            "Epoch [16/100], Batch [216/240],  Cross Loss: 0.3134\n",
            "216/240, Train_loss: 0.3134 Train_dice: 0.3134\n",
            "Epoch [16/100], Batch [217/240],  Cross Loss: 0.0405\n",
            "217/240, Train_loss: 0.0405 Train_dice: 0.0405\n",
            "Epoch [16/100], Batch [218/240],  Cross Loss: 0.3694\n",
            "218/240, Train_loss: 0.3694 Train_dice: 0.3694\n",
            "Epoch [16/100], Batch [219/240],  Cross Loss: 0.0217\n",
            "219/240, Train_loss: 0.0217 Train_dice: 0.0217\n",
            "Epoch [16/100], Batch [220/240],  Cross Loss: 0.2728\n",
            "220/240, Train_loss: 0.2728 Train_dice: 0.2728\n",
            "Epoch [16/100], Batch [221/240],  Cross Loss: 0.0097\n",
            "221/240, Train_loss: 0.0097 Train_dice: 0.0097\n",
            "Epoch [16/100], Batch [222/240],  Cross Loss: 0.1040\n",
            "222/240, Train_loss: 0.1040 Train_dice: 0.1040\n",
            "Epoch [16/100], Batch [223/240],  Cross Loss: 0.2274\n",
            "223/240, Train_loss: 0.2274 Train_dice: 0.2274\n",
            "Epoch [16/100], Batch [224/240],  Cross Loss: 0.3385\n",
            "224/240, Train_loss: 0.3385 Train_dice: 0.3385\n",
            "Epoch [16/100], Batch [225/240],  Cross Loss: 0.0851\n",
            "225/240, Train_loss: 0.0851 Train_dice: 0.0851\n",
            "Epoch [16/100], Batch [226/240],  Cross Loss: 0.0015\n",
            "226/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [16/100], Batch [227/240],  Cross Loss: 0.5499\n",
            "227/240, Train_loss: 0.5499 Train_dice: 0.5499\n",
            "Epoch [16/100], Batch [228/240],  Cross Loss: 0.5891\n",
            "228/240, Train_loss: 0.5891 Train_dice: 0.5891\n",
            "Epoch [16/100], Batch [229/240],  Cross Loss: 0.3773\n",
            "229/240, Train_loss: 0.3773 Train_dice: 0.3773\n",
            "Epoch [16/100], Batch [230/240],  Cross Loss: 0.1961\n",
            "230/240, Train_loss: 0.1961 Train_dice: 0.1961\n",
            "Epoch [16/100], Batch [231/240],  Cross Loss: 0.0058\n",
            "231/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [16/100], Batch [232/240],  Cross Loss: 0.0058\n",
            "232/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [16/100], Batch [233/240],  Cross Loss: 0.2259\n",
            "233/240, Train_loss: 0.2259 Train_dice: 0.2259\n",
            "Epoch [16/100], Batch [234/240],  Cross Loss: 0.1014\n",
            "234/240, Train_loss: 0.1014 Train_dice: 0.1014\n",
            "Epoch [16/100], Batch [235/240],  Cross Loss: 0.4391\n",
            "235/240, Train_loss: 0.4391 Train_dice: 0.4391\n",
            "Epoch [16/100], Batch [236/240],  Cross Loss: 0.0363\n",
            "236/240, Train_loss: 0.0363 Train_dice: 0.0363\n",
            "Epoch [16/100], Batch [237/240],  Cross Loss: 0.2406\n",
            "237/240, Train_loss: 0.2406 Train_dice: 0.2406\n",
            "Epoch [16/100], Batch [238/240],  Cross Loss: 0.5531\n",
            "238/240, Train_loss: 0.5531 Train_dice: 0.5531\n",
            "Epoch [16/100], Batch [239/240],  Cross Loss: 0.4488\n",
            "239/240, Train_loss: 0.4488 Train_dice: 0.4488\n",
            "Epoch [16/100], Batch [240/240],  Cross Loss: 0.0036\n",
            "240/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "--------------------\n",
            "Epoch_loss: 0.1906\n",
            "Epoch_metric: tensor(0.1906, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.4315\n",
            "test_dice_epoch: tensor(0.4315, device='cuda:0')\n",
            "current epoch: 16 current mean dice: tensor(0.4315, device='cuda:0')\n",
            "best mean dice: tensor(0.4407, device='cuda:0') at epoch: 12\n",
            "----------\n",
            "epoch 17/100\n",
            "Epoch [17/100], Batch [1/240],  Cross Loss: 0.1321\n",
            "1/240, Train_loss: 0.1321 Train_dice: 0.1321\n",
            "Epoch [17/100], Batch [2/240],  Cross Loss: 0.1070\n",
            "2/240, Train_loss: 0.1070 Train_dice: 0.1070\n",
            "Epoch [17/100], Batch [3/240],  Cross Loss: 0.2460\n",
            "3/240, Train_loss: 0.2460 Train_dice: 0.2460\n",
            "Epoch [17/100], Batch [4/240],  Cross Loss: 0.1193\n",
            "4/240, Train_loss: 0.1193 Train_dice: 0.1193\n",
            "Epoch [17/100], Batch [5/240],  Cross Loss: 0.0038\n",
            "5/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [17/100], Batch [6/240],  Cross Loss: 0.5926\n",
            "6/240, Train_loss: 0.5926 Train_dice: 0.5926\n",
            "Epoch [17/100], Batch [7/240],  Cross Loss: 0.4567\n",
            "7/240, Train_loss: 0.4567 Train_dice: 0.4567\n",
            "Epoch [17/100], Batch [8/240],  Cross Loss: 0.2601\n",
            "8/240, Train_loss: 0.2601 Train_dice: 0.2601\n",
            "Epoch [17/100], Batch [9/240],  Cross Loss: 0.3576\n",
            "9/240, Train_loss: 0.3576 Train_dice: 0.3576\n",
            "Epoch [17/100], Batch [10/240],  Cross Loss: 0.0249\n",
            "10/240, Train_loss: 0.0249 Train_dice: 0.0249\n",
            "Epoch [17/100], Batch [11/240],  Cross Loss: 0.0017\n",
            "11/240, Train_loss: 0.0017 Train_dice: 0.0017\n",
            "Epoch [17/100], Batch [12/240],  Cross Loss: 0.4476\n",
            "12/240, Train_loss: 0.4476 Train_dice: 0.4476\n",
            "Epoch [17/100], Batch [13/240],  Cross Loss: 0.0061\n",
            "13/240, Train_loss: 0.0061 Train_dice: 0.0061\n",
            "Epoch [17/100], Batch [14/240],  Cross Loss: 0.0032\n",
            "14/240, Train_loss: 0.0032 Train_dice: 0.0032\n",
            "Epoch [17/100], Batch [15/240],  Cross Loss: 0.0035\n",
            "15/240, Train_loss: 0.0035 Train_dice: 0.0035\n",
            "Epoch [17/100], Batch [16/240],  Cross Loss: 0.6462\n",
            "16/240, Train_loss: 0.6462 Train_dice: 0.6462\n",
            "Epoch [17/100], Batch [17/240],  Cross Loss: 0.1043\n",
            "17/240, Train_loss: 0.1043 Train_dice: 0.1043\n",
            "Epoch [17/100], Batch [18/240],  Cross Loss: 0.1916\n",
            "18/240, Train_loss: 0.1916 Train_dice: 0.1916\n",
            "Epoch [17/100], Batch [19/240],  Cross Loss: 0.0080\n",
            "19/240, Train_loss: 0.0080 Train_dice: 0.0080\n",
            "Epoch [17/100], Batch [20/240],  Cross Loss: 0.0061\n",
            "20/240, Train_loss: 0.0061 Train_dice: 0.0061\n",
            "Epoch [17/100], Batch [21/240],  Cross Loss: 0.0266\n",
            "21/240, Train_loss: 0.0266 Train_dice: 0.0266\n",
            "Epoch [17/100], Batch [22/240],  Cross Loss: 0.2116\n",
            "22/240, Train_loss: 0.2116 Train_dice: 0.2116\n",
            "Epoch [17/100], Batch [23/240],  Cross Loss: 0.0059\n",
            "23/240, Train_loss: 0.0059 Train_dice: 0.0059\n",
            "Epoch [17/100], Batch [24/240],  Cross Loss: 0.0018\n",
            "24/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [17/100], Batch [25/240],  Cross Loss: 0.0582\n",
            "25/240, Train_loss: 0.0582 Train_dice: 0.0582\n",
            "Epoch [17/100], Batch [26/240],  Cross Loss: 0.0580\n",
            "26/240, Train_loss: 0.0580 Train_dice: 0.0580\n",
            "Epoch [17/100], Batch [27/240],  Cross Loss: 0.1757\n",
            "27/240, Train_loss: 0.1757 Train_dice: 0.1757\n",
            "Epoch [17/100], Batch [28/240],  Cross Loss: 0.0615\n",
            "28/240, Train_loss: 0.0615 Train_dice: 0.0615\n",
            "Epoch [17/100], Batch [29/240],  Cross Loss: 0.4625\n",
            "29/240, Train_loss: 0.4625 Train_dice: 0.4625\n",
            "Epoch [17/100], Batch [30/240],  Cross Loss: 0.0544\n",
            "30/240, Train_loss: 0.0544 Train_dice: 0.0544\n",
            "Epoch [17/100], Batch [31/240],  Cross Loss: 0.0363\n",
            "31/240, Train_loss: 0.0363 Train_dice: 0.0363\n",
            "Epoch [17/100], Batch [32/240],  Cross Loss: 0.0016\n",
            "32/240, Train_loss: 0.0016 Train_dice: 0.0016\n",
            "Epoch [17/100], Batch [33/240],  Cross Loss: 0.4379\n",
            "33/240, Train_loss: 0.4379 Train_dice: 0.4379\n",
            "Epoch [17/100], Batch [34/240],  Cross Loss: 0.4391\n",
            "34/240, Train_loss: 0.4391 Train_dice: 0.4391\n",
            "Epoch [17/100], Batch [35/240],  Cross Loss: 0.3314\n",
            "35/240, Train_loss: 0.3314 Train_dice: 0.3314\n",
            "Epoch [17/100], Batch [36/240],  Cross Loss: 0.0667\n",
            "36/240, Train_loss: 0.0667 Train_dice: 0.0667\n",
            "Epoch [17/100], Batch [37/240],  Cross Loss: 0.1028\n",
            "37/240, Train_loss: 0.1028 Train_dice: 0.1028\n",
            "Epoch [17/100], Batch [38/240],  Cross Loss: 0.0726\n",
            "38/240, Train_loss: 0.0726 Train_dice: 0.0726\n",
            "Epoch [17/100], Batch [39/240],  Cross Loss: 0.0800\n",
            "39/240, Train_loss: 0.0800 Train_dice: 0.0800\n",
            "Epoch [17/100], Batch [40/240],  Cross Loss: 0.2716\n",
            "40/240, Train_loss: 0.2716 Train_dice: 0.2716\n",
            "Epoch [17/100], Batch [41/240],  Cross Loss: 0.0698\n",
            "41/240, Train_loss: 0.0698 Train_dice: 0.0698\n",
            "Epoch [17/100], Batch [42/240],  Cross Loss: 0.0626\n",
            "42/240, Train_loss: 0.0626 Train_dice: 0.0626\n",
            "Epoch [17/100], Batch [43/240],  Cross Loss: 0.0656\n",
            "43/240, Train_loss: 0.0656 Train_dice: 0.0656\n",
            "Epoch [17/100], Batch [44/240],  Cross Loss: 0.3519\n",
            "44/240, Train_loss: 0.3519 Train_dice: 0.3519\n",
            "Epoch [17/100], Batch [45/240],  Cross Loss: 0.0601\n",
            "45/240, Train_loss: 0.0601 Train_dice: 0.0601\n",
            "Epoch [17/100], Batch [46/240],  Cross Loss: 0.2754\n",
            "46/240, Train_loss: 0.2754 Train_dice: 0.2754\n",
            "Epoch [17/100], Batch [47/240],  Cross Loss: 0.0083\n",
            "47/240, Train_loss: 0.0083 Train_dice: 0.0083\n",
            "Epoch [17/100], Batch [48/240],  Cross Loss: 0.3791\n",
            "48/240, Train_loss: 0.3791 Train_dice: 0.3791\n",
            "Epoch [17/100], Batch [49/240],  Cross Loss: 0.4118\n",
            "49/240, Train_loss: 0.4118 Train_dice: 0.4118\n",
            "Epoch [17/100], Batch [50/240],  Cross Loss: 0.3510\n",
            "50/240, Train_loss: 0.3510 Train_dice: 0.3510\n",
            "Epoch [17/100], Batch [51/240],  Cross Loss: 0.2964\n",
            "51/240, Train_loss: 0.2964 Train_dice: 0.2964\n",
            "Epoch [17/100], Batch [52/240],  Cross Loss: 0.0068\n",
            "52/240, Train_loss: 0.0068 Train_dice: 0.0068\n",
            "Epoch [17/100], Batch [53/240],  Cross Loss: 0.0457\n",
            "53/240, Train_loss: 0.0457 Train_dice: 0.0457\n",
            "Epoch [17/100], Batch [54/240],  Cross Loss: 0.0083\n",
            "54/240, Train_loss: 0.0083 Train_dice: 0.0083\n",
            "Epoch [17/100], Batch [55/240],  Cross Loss: 0.3345\n",
            "55/240, Train_loss: 0.3345 Train_dice: 0.3345\n",
            "Epoch [17/100], Batch [56/240],  Cross Loss: 0.0086\n",
            "56/240, Train_loss: 0.0086 Train_dice: 0.0086\n",
            "Epoch [17/100], Batch [57/240],  Cross Loss: 0.0105\n",
            "57/240, Train_loss: 0.0105 Train_dice: 0.0105\n",
            "Epoch [17/100], Batch [58/240],  Cross Loss: 0.0659\n",
            "58/240, Train_loss: 0.0659 Train_dice: 0.0659\n",
            "Epoch [17/100], Batch [59/240],  Cross Loss: 0.2867\n",
            "59/240, Train_loss: 0.2867 Train_dice: 0.2867\n",
            "Epoch [17/100], Batch [60/240],  Cross Loss: 0.5387\n",
            "60/240, Train_loss: 0.5387 Train_dice: 0.5387\n",
            "Epoch [17/100], Batch [61/240],  Cross Loss: 0.0649\n",
            "61/240, Train_loss: 0.0649 Train_dice: 0.0649\n",
            "Epoch [17/100], Batch [62/240],  Cross Loss: 0.0046\n",
            "62/240, Train_loss: 0.0046 Train_dice: 0.0046\n",
            "Epoch [17/100], Batch [63/240],  Cross Loss: 0.0176\n",
            "63/240, Train_loss: 0.0176 Train_dice: 0.0176\n",
            "Epoch [17/100], Batch [64/240],  Cross Loss: 0.0510\n",
            "64/240, Train_loss: 0.0510 Train_dice: 0.0510\n",
            "Epoch [17/100], Batch [65/240],  Cross Loss: 0.1603\n",
            "65/240, Train_loss: 0.1603 Train_dice: 0.1603\n",
            "Epoch [17/100], Batch [66/240],  Cross Loss: 0.1243\n",
            "66/240, Train_loss: 0.1243 Train_dice: 0.1243\n",
            "Epoch [17/100], Batch [67/240],  Cross Loss: 0.1027\n",
            "67/240, Train_loss: 0.1027 Train_dice: 0.1027\n",
            "Epoch [17/100], Batch [68/240],  Cross Loss: 0.0898\n",
            "68/240, Train_loss: 0.0898 Train_dice: 0.0898\n",
            "Epoch [17/100], Batch [69/240],  Cross Loss: 0.2713\n",
            "69/240, Train_loss: 0.2713 Train_dice: 0.2713\n",
            "Epoch [17/100], Batch [70/240],  Cross Loss: 0.0607\n",
            "70/240, Train_loss: 0.0607 Train_dice: 0.0607\n",
            "Epoch [17/100], Batch [71/240],  Cross Loss: 0.0630\n",
            "71/240, Train_loss: 0.0630 Train_dice: 0.0630\n",
            "Epoch [17/100], Batch [72/240],  Cross Loss: 0.2455\n",
            "72/240, Train_loss: 0.2455 Train_dice: 0.2455\n",
            "Epoch [17/100], Batch [73/240],  Cross Loss: 0.1513\n",
            "73/240, Train_loss: 0.1513 Train_dice: 0.1513\n",
            "Epoch [17/100], Batch [74/240],  Cross Loss: 0.0042\n",
            "74/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [17/100], Batch [75/240],  Cross Loss: 0.1308\n",
            "75/240, Train_loss: 0.1308 Train_dice: 0.1308\n",
            "Epoch [17/100], Batch [76/240],  Cross Loss: 0.1050\n",
            "76/240, Train_loss: 0.1050 Train_dice: 0.1050\n",
            "Epoch [17/100], Batch [77/240],  Cross Loss: 0.1560\n",
            "77/240, Train_loss: 0.1560 Train_dice: 0.1560\n",
            "Epoch [17/100], Batch [78/240],  Cross Loss: 0.0091\n",
            "78/240, Train_loss: 0.0091 Train_dice: 0.0091\n",
            "Epoch [17/100], Batch [79/240],  Cross Loss: 0.0027\n",
            "79/240, Train_loss: 0.0027 Train_dice: 0.0027\n",
            "Epoch [17/100], Batch [80/240],  Cross Loss: 0.3707\n",
            "80/240, Train_loss: 0.3707 Train_dice: 0.3707\n",
            "Epoch [17/100], Batch [81/240],  Cross Loss: 0.0115\n",
            "81/240, Train_loss: 0.0115 Train_dice: 0.0115\n",
            "Epoch [17/100], Batch [82/240],  Cross Loss: 0.3913\n",
            "82/240, Train_loss: 0.3913 Train_dice: 0.3913\n",
            "Epoch [17/100], Batch [83/240],  Cross Loss: 0.5081\n",
            "83/240, Train_loss: 0.5081 Train_dice: 0.5081\n",
            "Epoch [17/100], Batch [84/240],  Cross Loss: 0.0102\n",
            "84/240, Train_loss: 0.0102 Train_dice: 0.0102\n",
            "Epoch [17/100], Batch [85/240],  Cross Loss: 0.0030\n",
            "85/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [17/100], Batch [86/240],  Cross Loss: 0.0030\n",
            "86/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [17/100], Batch [87/240],  Cross Loss: 0.2788\n",
            "87/240, Train_loss: 0.2788 Train_dice: 0.2788\n",
            "Epoch [17/100], Batch [88/240],  Cross Loss: 0.0740\n",
            "88/240, Train_loss: 0.0740 Train_dice: 0.0740\n",
            "Epoch [17/100], Batch [89/240],  Cross Loss: 0.0493\n",
            "89/240, Train_loss: 0.0493 Train_dice: 0.0493\n",
            "Epoch [17/100], Batch [90/240],  Cross Loss: 0.0031\n",
            "90/240, Train_loss: 0.0031 Train_dice: 0.0031\n",
            "Epoch [17/100], Batch [91/240],  Cross Loss: 0.2904\n",
            "91/240, Train_loss: 0.2904 Train_dice: 0.2904\n",
            "Epoch [17/100], Batch [92/240],  Cross Loss: 0.0995\n",
            "92/240, Train_loss: 0.0995 Train_dice: 0.0995\n",
            "Epoch [17/100], Batch [93/240],  Cross Loss: 0.0145\n",
            "93/240, Train_loss: 0.0145 Train_dice: 0.0145\n",
            "Epoch [17/100], Batch [94/240],  Cross Loss: 0.1775\n",
            "94/240, Train_loss: 0.1775 Train_dice: 0.1775\n",
            "Epoch [17/100], Batch [95/240],  Cross Loss: 0.5835\n",
            "95/240, Train_loss: 0.5835 Train_dice: 0.5835\n",
            "Epoch [17/100], Batch [96/240],  Cross Loss: 0.0917\n",
            "96/240, Train_loss: 0.0917 Train_dice: 0.0917\n",
            "Epoch [17/100], Batch [97/240],  Cross Loss: 0.0338\n",
            "97/240, Train_loss: 0.0338 Train_dice: 0.0338\n",
            "Epoch [17/100], Batch [98/240],  Cross Loss: 0.0058\n",
            "98/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [17/100], Batch [99/240],  Cross Loss: 0.0120\n",
            "99/240, Train_loss: 0.0120 Train_dice: 0.0120\n",
            "Epoch [17/100], Batch [100/240],  Cross Loss: 0.1551\n",
            "100/240, Train_loss: 0.1551 Train_dice: 0.1551\n",
            "Epoch [17/100], Batch [101/240],  Cross Loss: 0.0157\n",
            "101/240, Train_loss: 0.0157 Train_dice: 0.0157\n",
            "Epoch [17/100], Batch [102/240],  Cross Loss: 0.6524\n",
            "102/240, Train_loss: 0.6524 Train_dice: 0.6524\n",
            "Epoch [17/100], Batch [103/240],  Cross Loss: 0.0078\n",
            "103/240, Train_loss: 0.0078 Train_dice: 0.0078\n",
            "Epoch [17/100], Batch [104/240],  Cross Loss: 0.1074\n",
            "104/240, Train_loss: 0.1074 Train_dice: 0.1074\n",
            "Epoch [17/100], Batch [105/240],  Cross Loss: 0.4262\n",
            "105/240, Train_loss: 0.4262 Train_dice: 0.4262\n",
            "Epoch [17/100], Batch [106/240],  Cross Loss: 0.0025\n",
            "106/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [17/100], Batch [107/240],  Cross Loss: 0.3431\n",
            "107/240, Train_loss: 0.3431 Train_dice: 0.3431\n",
            "Epoch [17/100], Batch [108/240],  Cross Loss: 0.0434\n",
            "108/240, Train_loss: 0.0434 Train_dice: 0.0434\n",
            "Epoch [17/100], Batch [109/240],  Cross Loss: 0.2762\n",
            "109/240, Train_loss: 0.2762 Train_dice: 0.2762\n",
            "Epoch [17/100], Batch [110/240],  Cross Loss: 0.4535\n",
            "110/240, Train_loss: 0.4535 Train_dice: 0.4535\n",
            "Epoch [17/100], Batch [111/240],  Cross Loss: 0.1464\n",
            "111/240, Train_loss: 0.1464 Train_dice: 0.1464\n",
            "Epoch [17/100], Batch [112/240],  Cross Loss: 0.0030\n",
            "112/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [17/100], Batch [113/240],  Cross Loss: 0.0274\n",
            "113/240, Train_loss: 0.0274 Train_dice: 0.0274\n",
            "Epoch [17/100], Batch [114/240],  Cross Loss: 0.3900\n",
            "114/240, Train_loss: 0.3900 Train_dice: 0.3900\n",
            "Epoch [17/100], Batch [115/240],  Cross Loss: 0.3296\n",
            "115/240, Train_loss: 0.3296 Train_dice: 0.3296\n",
            "Epoch [17/100], Batch [116/240],  Cross Loss: 0.0020\n",
            "116/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [17/100], Batch [117/240],  Cross Loss: 0.0463\n",
            "117/240, Train_loss: 0.0463 Train_dice: 0.0463\n",
            "Epoch [17/100], Batch [118/240],  Cross Loss: 0.1088\n",
            "118/240, Train_loss: 0.1088 Train_dice: 0.1088\n",
            "Epoch [17/100], Batch [119/240],  Cross Loss: 0.5118\n",
            "119/240, Train_loss: 0.5118 Train_dice: 0.5118\n",
            "Epoch [17/100], Batch [120/240],  Cross Loss: 0.7220\n",
            "120/240, Train_loss: 0.7220 Train_dice: 0.7220\n",
            "Epoch [17/100], Batch [121/240],  Cross Loss: 0.0812\n",
            "121/240, Train_loss: 0.0812 Train_dice: 0.0812\n",
            "Epoch [17/100], Batch [122/240],  Cross Loss: 0.1000\n",
            "122/240, Train_loss: 0.1000 Train_dice: 0.1000\n",
            "Epoch [17/100], Batch [123/240],  Cross Loss: 0.0392\n",
            "123/240, Train_loss: 0.0392 Train_dice: 0.0392\n",
            "Epoch [17/100], Batch [124/240],  Cross Loss: 0.0061\n",
            "124/240, Train_loss: 0.0061 Train_dice: 0.0061\n",
            "Epoch [17/100], Batch [125/240],  Cross Loss: 0.0094\n",
            "125/240, Train_loss: 0.0094 Train_dice: 0.0094\n",
            "Epoch [17/100], Batch [126/240],  Cross Loss: 0.0030\n",
            "126/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [17/100], Batch [127/240],  Cross Loss: 0.2803\n",
            "127/240, Train_loss: 0.2803 Train_dice: 0.2803\n",
            "Epoch [17/100], Batch [128/240],  Cross Loss: 0.0128\n",
            "128/240, Train_loss: 0.0128 Train_dice: 0.0128\n",
            "Epoch [17/100], Batch [129/240],  Cross Loss: 0.1005\n",
            "129/240, Train_loss: 0.1005 Train_dice: 0.1005\n",
            "Epoch [17/100], Batch [130/240],  Cross Loss: 0.3110\n",
            "130/240, Train_loss: 0.3110 Train_dice: 0.3110\n",
            "Epoch [17/100], Batch [131/240],  Cross Loss: 0.2432\n",
            "131/240, Train_loss: 0.2432 Train_dice: 0.2432\n",
            "Epoch [17/100], Batch [132/240],  Cross Loss: 0.1969\n",
            "132/240, Train_loss: 0.1969 Train_dice: 0.1969\n",
            "Epoch [17/100], Batch [133/240],  Cross Loss: 0.0071\n",
            "133/240, Train_loss: 0.0071 Train_dice: 0.0071\n",
            "Epoch [17/100], Batch [134/240],  Cross Loss: 0.4554\n",
            "134/240, Train_loss: 0.4554 Train_dice: 0.4554\n",
            "Epoch [17/100], Batch [135/240],  Cross Loss: 0.1921\n",
            "135/240, Train_loss: 0.1921 Train_dice: 0.1921\n",
            "Epoch [17/100], Batch [136/240],  Cross Loss: 0.3116\n",
            "136/240, Train_loss: 0.3116 Train_dice: 0.3116\n",
            "Epoch [17/100], Batch [137/240],  Cross Loss: 0.0049\n",
            "137/240, Train_loss: 0.0049 Train_dice: 0.0049\n",
            "Epoch [17/100], Batch [138/240],  Cross Loss: 0.0047\n",
            "138/240, Train_loss: 0.0047 Train_dice: 0.0047\n",
            "Epoch [17/100], Batch [139/240],  Cross Loss: 0.1152\n",
            "139/240, Train_loss: 0.1152 Train_dice: 0.1152\n",
            "Epoch [17/100], Batch [140/240],  Cross Loss: 0.3352\n",
            "140/240, Train_loss: 0.3352 Train_dice: 0.3352\n",
            "Epoch [17/100], Batch [141/240],  Cross Loss: 0.3131\n",
            "141/240, Train_loss: 0.3131 Train_dice: 0.3131\n",
            "Epoch [17/100], Batch [142/240],  Cross Loss: 0.2349\n",
            "142/240, Train_loss: 0.2349 Train_dice: 0.2349\n",
            "Epoch [17/100], Batch [143/240],  Cross Loss: 0.0088\n",
            "143/240, Train_loss: 0.0088 Train_dice: 0.0088\n",
            "Epoch [17/100], Batch [144/240],  Cross Loss: 0.1616\n",
            "144/240, Train_loss: 0.1616 Train_dice: 0.1616\n",
            "Epoch [17/100], Batch [145/240],  Cross Loss: 0.0732\n",
            "145/240, Train_loss: 0.0732 Train_dice: 0.0732\n",
            "Epoch [17/100], Batch [146/240],  Cross Loss: 0.0144\n",
            "146/240, Train_loss: 0.0144 Train_dice: 0.0144\n",
            "Epoch [17/100], Batch [147/240],  Cross Loss: 0.4397\n",
            "147/240, Train_loss: 0.4397 Train_dice: 0.4397\n",
            "Epoch [17/100], Batch [148/240],  Cross Loss: 0.0135\n",
            "148/240, Train_loss: 0.0135 Train_dice: 0.0135\n",
            "Epoch [17/100], Batch [149/240],  Cross Loss: 0.0942\n",
            "149/240, Train_loss: 0.0942 Train_dice: 0.0942\n",
            "Epoch [17/100], Batch [150/240],  Cross Loss: 0.2320\n",
            "150/240, Train_loss: 0.2320 Train_dice: 0.2320\n",
            "Epoch [17/100], Batch [151/240],  Cross Loss: 0.2168\n",
            "151/240, Train_loss: 0.2168 Train_dice: 0.2168\n",
            "Epoch [17/100], Batch [152/240],  Cross Loss: 0.0438\n",
            "152/240, Train_loss: 0.0438 Train_dice: 0.0438\n",
            "Epoch [17/100], Batch [153/240],  Cross Loss: 0.0039\n",
            "153/240, Train_loss: 0.0039 Train_dice: 0.0039\n",
            "Epoch [17/100], Batch [154/240],  Cross Loss: 0.0686\n",
            "154/240, Train_loss: 0.0686 Train_dice: 0.0686\n",
            "Epoch [17/100], Batch [155/240],  Cross Loss: 0.0059\n",
            "155/240, Train_loss: 0.0059 Train_dice: 0.0059\n",
            "Epoch [17/100], Batch [156/240],  Cross Loss: 0.0430\n",
            "156/240, Train_loss: 0.0430 Train_dice: 0.0430\n",
            "Epoch [17/100], Batch [157/240],  Cross Loss: 0.1402\n",
            "157/240, Train_loss: 0.1402 Train_dice: 0.1402\n",
            "Epoch [17/100], Batch [158/240],  Cross Loss: 0.3478\n",
            "158/240, Train_loss: 0.3478 Train_dice: 0.3478\n",
            "Epoch [17/100], Batch [159/240],  Cross Loss: 0.2004\n",
            "159/240, Train_loss: 0.2004 Train_dice: 0.2004\n",
            "Epoch [17/100], Batch [160/240],  Cross Loss: 0.2293\n",
            "160/240, Train_loss: 0.2293 Train_dice: 0.2293\n",
            "Epoch [17/100], Batch [161/240],  Cross Loss: 0.1539\n",
            "161/240, Train_loss: 0.1539 Train_dice: 0.1539\n",
            "Epoch [17/100], Batch [162/240],  Cross Loss: 0.0151\n",
            "162/240, Train_loss: 0.0151 Train_dice: 0.0151\n",
            "Epoch [17/100], Batch [163/240],  Cross Loss: 0.0152\n",
            "163/240, Train_loss: 0.0152 Train_dice: 0.0152\n",
            "Epoch [17/100], Batch [164/240],  Cross Loss: 0.2671\n",
            "164/240, Train_loss: 0.2671 Train_dice: 0.2671\n",
            "Epoch [17/100], Batch [165/240],  Cross Loss: 0.0085\n",
            "165/240, Train_loss: 0.0085 Train_dice: 0.0085\n",
            "Epoch [17/100], Batch [166/240],  Cross Loss: 0.2418\n",
            "166/240, Train_loss: 0.2418 Train_dice: 0.2418\n",
            "Epoch [17/100], Batch [167/240],  Cross Loss: 0.0053\n",
            "167/240, Train_loss: 0.0053 Train_dice: 0.0053\n",
            "Epoch [17/100], Batch [168/240],  Cross Loss: 0.0166\n",
            "168/240, Train_loss: 0.0166 Train_dice: 0.0166\n",
            "Epoch [17/100], Batch [169/240],  Cross Loss: 0.1738\n",
            "169/240, Train_loss: 0.1738 Train_dice: 0.1738\n",
            "Epoch [17/100], Batch [170/240],  Cross Loss: 0.3252\n",
            "170/240, Train_loss: 0.3252 Train_dice: 0.3252\n",
            "Epoch [17/100], Batch [171/240],  Cross Loss: 0.1112\n",
            "171/240, Train_loss: 0.1112 Train_dice: 0.1112\n",
            "Epoch [17/100], Batch [172/240],  Cross Loss: 0.0032\n",
            "172/240, Train_loss: 0.0032 Train_dice: 0.0032\n",
            "Epoch [17/100], Batch [173/240],  Cross Loss: 0.3307\n",
            "173/240, Train_loss: 0.3307 Train_dice: 0.3307\n",
            "Epoch [17/100], Batch [174/240],  Cross Loss: 0.0052\n",
            "174/240, Train_loss: 0.0052 Train_dice: 0.0052\n",
            "Epoch [17/100], Batch [175/240],  Cross Loss: 0.5490\n",
            "175/240, Train_loss: 0.5490 Train_dice: 0.5490\n",
            "Epoch [17/100], Batch [176/240],  Cross Loss: 0.1867\n",
            "176/240, Train_loss: 0.1867 Train_dice: 0.1867\n",
            "Epoch [17/100], Batch [177/240],  Cross Loss: 0.3177\n",
            "177/240, Train_loss: 0.3177 Train_dice: 0.3177\n",
            "Epoch [17/100], Batch [178/240],  Cross Loss: 0.5257\n",
            "178/240, Train_loss: 0.5257 Train_dice: 0.5257\n",
            "Epoch [17/100], Batch [179/240],  Cross Loss: 0.2026\n",
            "179/240, Train_loss: 0.2026 Train_dice: 0.2026\n",
            "Epoch [17/100], Batch [180/240],  Cross Loss: 0.1369\n",
            "180/240, Train_loss: 0.1369 Train_dice: 0.1369\n",
            "Epoch [17/100], Batch [181/240],  Cross Loss: 0.0035\n",
            "181/240, Train_loss: 0.0035 Train_dice: 0.0035\n",
            "Epoch [17/100], Batch [182/240],  Cross Loss: 0.0055\n",
            "182/240, Train_loss: 0.0055 Train_dice: 0.0055\n",
            "Epoch [17/100], Batch [183/240],  Cross Loss: 0.0108\n",
            "183/240, Train_loss: 0.0108 Train_dice: 0.0108\n",
            "Epoch [17/100], Batch [184/240],  Cross Loss: 0.3774\n",
            "184/240, Train_loss: 0.3774 Train_dice: 0.3774\n",
            "Epoch [17/100], Batch [185/240],  Cross Loss: 0.3920\n",
            "185/240, Train_loss: 0.3920 Train_dice: 0.3920\n",
            "Epoch [17/100], Batch [186/240],  Cross Loss: 0.0028\n",
            "186/240, Train_loss: 0.0028 Train_dice: 0.0028\n",
            "Epoch [17/100], Batch [187/240],  Cross Loss: 0.6238\n",
            "187/240, Train_loss: 0.6238 Train_dice: 0.6238\n",
            "Epoch [17/100], Batch [188/240],  Cross Loss: 0.0375\n",
            "188/240, Train_loss: 0.0375 Train_dice: 0.0375\n",
            "Epoch [17/100], Batch [189/240],  Cross Loss: 0.5224\n",
            "189/240, Train_loss: 0.5224 Train_dice: 0.5224\n",
            "Epoch [17/100], Batch [190/240],  Cross Loss: 0.3702\n",
            "190/240, Train_loss: 0.3702 Train_dice: 0.3702\n",
            "Epoch [17/100], Batch [191/240],  Cross Loss: 0.0019\n",
            "191/240, Train_loss: 0.0019 Train_dice: 0.0019\n",
            "Epoch [17/100], Batch [192/240],  Cross Loss: 0.6036\n",
            "192/240, Train_loss: 0.6036 Train_dice: 0.6036\n",
            "Epoch [17/100], Batch [193/240],  Cross Loss: 0.0117\n",
            "193/240, Train_loss: 0.0117 Train_dice: 0.0117\n",
            "Epoch [17/100], Batch [194/240],  Cross Loss: 0.0032\n",
            "194/240, Train_loss: 0.0032 Train_dice: 0.0032\n",
            "Epoch [17/100], Batch [195/240],  Cross Loss: 0.1702\n",
            "195/240, Train_loss: 0.1702 Train_dice: 0.1702\n",
            "Epoch [17/100], Batch [196/240],  Cross Loss: 0.0161\n",
            "196/240, Train_loss: 0.0161 Train_dice: 0.0161\n",
            "Epoch [17/100], Batch [197/240],  Cross Loss: 0.0397\n",
            "197/240, Train_loss: 0.0397 Train_dice: 0.0397\n",
            "Epoch [17/100], Batch [198/240],  Cross Loss: 0.0312\n",
            "198/240, Train_loss: 0.0312 Train_dice: 0.0312\n",
            "Epoch [17/100], Batch [199/240],  Cross Loss: 0.0048\n",
            "199/240, Train_loss: 0.0048 Train_dice: 0.0048\n",
            "Epoch [17/100], Batch [200/240],  Cross Loss: 0.0720\n",
            "200/240, Train_loss: 0.0720 Train_dice: 0.0720\n",
            "Epoch [17/100], Batch [201/240],  Cross Loss: 0.3392\n",
            "201/240, Train_loss: 0.3392 Train_dice: 0.3392\n",
            "Epoch [17/100], Batch [202/240],  Cross Loss: 0.0120\n",
            "202/240, Train_loss: 0.0120 Train_dice: 0.0120\n",
            "Epoch [17/100], Batch [203/240],  Cross Loss: 0.0670\n",
            "203/240, Train_loss: 0.0670 Train_dice: 0.0670\n",
            "Epoch [17/100], Batch [204/240],  Cross Loss: 0.1305\n",
            "204/240, Train_loss: 0.1305 Train_dice: 0.1305\n",
            "Epoch [17/100], Batch [205/240],  Cross Loss: 0.0512\n",
            "205/240, Train_loss: 0.0512 Train_dice: 0.0512\n",
            "Epoch [17/100], Batch [206/240],  Cross Loss: 0.2057\n",
            "206/240, Train_loss: 0.2057 Train_dice: 0.2057\n",
            "Epoch [17/100], Batch [207/240],  Cross Loss: 0.2683\n",
            "207/240, Train_loss: 0.2683 Train_dice: 0.2683\n",
            "Epoch [17/100], Batch [208/240],  Cross Loss: 0.5257\n",
            "208/240, Train_loss: 0.5257 Train_dice: 0.5257\n",
            "Epoch [17/100], Batch [209/240],  Cross Loss: 0.3216\n",
            "209/240, Train_loss: 0.3216 Train_dice: 0.3216\n",
            "Epoch [17/100], Batch [210/240],  Cross Loss: 0.0397\n",
            "210/240, Train_loss: 0.0397 Train_dice: 0.0397\n",
            "Epoch [17/100], Batch [211/240],  Cross Loss: 0.0759\n",
            "211/240, Train_loss: 0.0759 Train_dice: 0.0759\n",
            "Epoch [17/100], Batch [212/240],  Cross Loss: 0.2059\n",
            "212/240, Train_loss: 0.2059 Train_dice: 0.2059\n",
            "Epoch [17/100], Batch [213/240],  Cross Loss: 0.4842\n",
            "213/240, Train_loss: 0.4842 Train_dice: 0.4842\n",
            "Epoch [17/100], Batch [214/240],  Cross Loss: 0.2236\n",
            "214/240, Train_loss: 0.2236 Train_dice: 0.2236\n",
            "Epoch [17/100], Batch [215/240],  Cross Loss: 0.1840\n",
            "215/240, Train_loss: 0.1840 Train_dice: 0.1840\n",
            "Epoch [17/100], Batch [216/240],  Cross Loss: 0.2437\n",
            "216/240, Train_loss: 0.2437 Train_dice: 0.2437\n",
            "Epoch [17/100], Batch [217/240],  Cross Loss: 0.0097\n",
            "217/240, Train_loss: 0.0097 Train_dice: 0.0097\n",
            "Epoch [17/100], Batch [218/240],  Cross Loss: 0.0057\n",
            "218/240, Train_loss: 0.0057 Train_dice: 0.0057\n",
            "Epoch [17/100], Batch [219/240],  Cross Loss: 0.0104\n",
            "219/240, Train_loss: 0.0104 Train_dice: 0.0104\n",
            "Epoch [17/100], Batch [220/240],  Cross Loss: 0.2721\n",
            "220/240, Train_loss: 0.2721 Train_dice: 0.2721\n",
            "Epoch [17/100], Batch [221/240],  Cross Loss: 0.2494\n",
            "221/240, Train_loss: 0.2494 Train_dice: 0.2494\n",
            "Epoch [17/100], Batch [222/240],  Cross Loss: 0.0029\n",
            "222/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [17/100], Batch [223/240],  Cross Loss: 0.2429\n",
            "223/240, Train_loss: 0.2429 Train_dice: 0.2429\n",
            "Epoch [17/100], Batch [224/240],  Cross Loss: 0.0066\n",
            "224/240, Train_loss: 0.0066 Train_dice: 0.0066\n",
            "Epoch [17/100], Batch [225/240],  Cross Loss: 0.7711\n",
            "225/240, Train_loss: 0.7711 Train_dice: 0.7711\n",
            "Epoch [17/100], Batch [226/240],  Cross Loss: 0.1834\n",
            "226/240, Train_loss: 0.1834 Train_dice: 0.1834\n",
            "Epoch [17/100], Batch [227/240],  Cross Loss: 0.0073\n",
            "227/240, Train_loss: 0.0073 Train_dice: 0.0073\n",
            "Epoch [17/100], Batch [228/240],  Cross Loss: 0.0060\n",
            "228/240, Train_loss: 0.0060 Train_dice: 0.0060\n",
            "Epoch [17/100], Batch [229/240],  Cross Loss: 0.4636\n",
            "229/240, Train_loss: 0.4636 Train_dice: 0.4636\n",
            "Epoch [17/100], Batch [230/240],  Cross Loss: 0.3145\n",
            "230/240, Train_loss: 0.3145 Train_dice: 0.3145\n",
            "Epoch [17/100], Batch [231/240],  Cross Loss: 0.5956\n",
            "231/240, Train_loss: 0.5956 Train_dice: 0.5956\n",
            "Epoch [17/100], Batch [232/240],  Cross Loss: 0.2295\n",
            "232/240, Train_loss: 0.2295 Train_dice: 0.2295\n",
            "Epoch [17/100], Batch [233/240],  Cross Loss: 0.5829\n",
            "233/240, Train_loss: 0.5829 Train_dice: 0.5829\n",
            "Epoch [17/100], Batch [234/240],  Cross Loss: 0.2330\n",
            "234/240, Train_loss: 0.2330 Train_dice: 0.2330\n",
            "Epoch [17/100], Batch [235/240],  Cross Loss: 0.0388\n",
            "235/240, Train_loss: 0.0388 Train_dice: 0.0388\n",
            "Epoch [17/100], Batch [236/240],  Cross Loss: 0.0488\n",
            "236/240, Train_loss: 0.0488 Train_dice: 0.0488\n",
            "Epoch [17/100], Batch [237/240],  Cross Loss: 0.0992\n",
            "237/240, Train_loss: 0.0992 Train_dice: 0.0992\n",
            "Epoch [17/100], Batch [238/240],  Cross Loss: 0.0221\n",
            "238/240, Train_loss: 0.0221 Train_dice: 0.0221\n",
            "Epoch [17/100], Batch [239/240],  Cross Loss: 0.0357\n",
            "239/240, Train_loss: 0.0357 Train_dice: 0.0357\n",
            "Epoch [17/100], Batch [240/240],  Cross Loss: 0.0064\n",
            "240/240, Train_loss: 0.0064 Train_dice: 0.0064\n",
            "--------------------\n",
            "Epoch_loss: 0.1680\n",
            "Epoch_metric: tensor(0.1680, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 18/100\n",
            "Epoch [18/100], Batch [1/240],  Cross Loss: 0.0382\n",
            "1/240, Train_loss: 0.0382 Train_dice: 0.0382\n",
            "Epoch [18/100], Batch [2/240],  Cross Loss: 0.0131\n",
            "2/240, Train_loss: 0.0131 Train_dice: 0.0131\n",
            "Epoch [18/100], Batch [3/240],  Cross Loss: 0.1236\n",
            "3/240, Train_loss: 0.1236 Train_dice: 0.1236\n",
            "Epoch [18/100], Batch [4/240],  Cross Loss: 0.0019\n",
            "4/240, Train_loss: 0.0019 Train_dice: 0.0019\n",
            "Epoch [18/100], Batch [5/240],  Cross Loss: 0.0362\n",
            "5/240, Train_loss: 0.0362 Train_dice: 0.0362\n",
            "Epoch [18/100], Batch [6/240],  Cross Loss: 0.0220\n",
            "6/240, Train_loss: 0.0220 Train_dice: 0.0220\n",
            "Epoch [18/100], Batch [7/240],  Cross Loss: 0.0893\n",
            "7/240, Train_loss: 0.0893 Train_dice: 0.0893\n",
            "Epoch [18/100], Batch [8/240],  Cross Loss: 0.0014\n",
            "8/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [18/100], Batch [9/240],  Cross Loss: 0.1534\n",
            "9/240, Train_loss: 0.1534 Train_dice: 0.1534\n",
            "Epoch [18/100], Batch [10/240],  Cross Loss: 0.0038\n",
            "10/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [18/100], Batch [11/240],  Cross Loss: 0.0686\n",
            "11/240, Train_loss: 0.0686 Train_dice: 0.0686\n",
            "Epoch [18/100], Batch [12/240],  Cross Loss: 0.1426\n",
            "12/240, Train_loss: 0.1426 Train_dice: 0.1426\n",
            "Epoch [18/100], Batch [13/240],  Cross Loss: 0.2948\n",
            "13/240, Train_loss: 0.2948 Train_dice: 0.2948\n",
            "Epoch [18/100], Batch [14/240],  Cross Loss: 0.2352\n",
            "14/240, Train_loss: 0.2352 Train_dice: 0.2352\n",
            "Epoch [18/100], Batch [15/240],  Cross Loss: 0.1869\n",
            "15/240, Train_loss: 0.1869 Train_dice: 0.1869\n",
            "Epoch [18/100], Batch [16/240],  Cross Loss: 0.0051\n",
            "16/240, Train_loss: 0.0051 Train_dice: 0.0051\n",
            "Epoch [18/100], Batch [17/240],  Cross Loss: 0.3404\n",
            "17/240, Train_loss: 0.3404 Train_dice: 0.3404\n",
            "Epoch [18/100], Batch [18/240],  Cross Loss: 0.4491\n",
            "18/240, Train_loss: 0.4491 Train_dice: 0.4491\n",
            "Epoch [18/100], Batch [19/240],  Cross Loss: 0.0499\n",
            "19/240, Train_loss: 0.0499 Train_dice: 0.0499\n",
            "Epoch [18/100], Batch [20/240],  Cross Loss: 0.4200\n",
            "20/240, Train_loss: 0.4200 Train_dice: 0.4200\n",
            "Epoch [18/100], Batch [21/240],  Cross Loss: 0.3778\n",
            "21/240, Train_loss: 0.3778 Train_dice: 0.3778\n",
            "Epoch [18/100], Batch [22/240],  Cross Loss: 0.5506\n",
            "22/240, Train_loss: 0.5506 Train_dice: 0.5506\n",
            "Epoch [18/100], Batch [23/240],  Cross Loss: 0.1197\n",
            "23/240, Train_loss: 0.1197 Train_dice: 0.1197\n",
            "Epoch [18/100], Batch [24/240],  Cross Loss: 0.4697\n",
            "24/240, Train_loss: 0.4697 Train_dice: 0.4697\n",
            "Epoch [18/100], Batch [25/240],  Cross Loss: 0.0058\n",
            "25/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [18/100], Batch [26/240],  Cross Loss: 0.0032\n",
            "26/240, Train_loss: 0.0032 Train_dice: 0.0032\n",
            "Epoch [18/100], Batch [27/240],  Cross Loss: 0.1262\n",
            "27/240, Train_loss: 0.1262 Train_dice: 0.1262\n",
            "Epoch [18/100], Batch [28/240],  Cross Loss: 0.0536\n",
            "28/240, Train_loss: 0.0536 Train_dice: 0.0536\n",
            "Epoch [18/100], Batch [29/240],  Cross Loss: 0.1971\n",
            "29/240, Train_loss: 0.1971 Train_dice: 0.1971\n",
            "Epoch [18/100], Batch [30/240],  Cross Loss: 0.0124\n",
            "30/240, Train_loss: 0.0124 Train_dice: 0.0124\n",
            "Epoch [18/100], Batch [31/240],  Cross Loss: 0.0022\n",
            "31/240, Train_loss: 0.0022 Train_dice: 0.0022\n",
            "Epoch [18/100], Batch [32/240],  Cross Loss: 0.3222\n",
            "32/240, Train_loss: 0.3222 Train_dice: 0.3222\n",
            "Epoch [18/100], Batch [33/240],  Cross Loss: 0.3460\n",
            "33/240, Train_loss: 0.3460 Train_dice: 0.3460\n",
            "Epoch [18/100], Batch [34/240],  Cross Loss: 0.0012\n",
            "34/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [18/100], Batch [35/240],  Cross Loss: 0.0027\n",
            "35/240, Train_loss: 0.0027 Train_dice: 0.0027\n",
            "Epoch [18/100], Batch [36/240],  Cross Loss: 0.0109\n",
            "36/240, Train_loss: 0.0109 Train_dice: 0.0109\n",
            "Epoch [18/100], Batch [37/240],  Cross Loss: 0.0017\n",
            "37/240, Train_loss: 0.0017 Train_dice: 0.0017\n",
            "Epoch [18/100], Batch [38/240],  Cross Loss: 0.1178\n",
            "38/240, Train_loss: 0.1178 Train_dice: 0.1178\n",
            "Epoch [18/100], Batch [39/240],  Cross Loss: 0.2440\n",
            "39/240, Train_loss: 0.2440 Train_dice: 0.2440\n",
            "Epoch [18/100], Batch [40/240],  Cross Loss: 0.5365\n",
            "40/240, Train_loss: 0.5365 Train_dice: 0.5365\n",
            "Epoch [18/100], Batch [41/240],  Cross Loss: 0.1364\n",
            "41/240, Train_loss: 0.1364 Train_dice: 0.1364\n",
            "Epoch [18/100], Batch [42/240],  Cross Loss: 0.0891\n",
            "42/240, Train_loss: 0.0891 Train_dice: 0.0891\n",
            "Epoch [18/100], Batch [43/240],  Cross Loss: 0.0046\n",
            "43/240, Train_loss: 0.0046 Train_dice: 0.0046\n",
            "Epoch [18/100], Batch [44/240],  Cross Loss: 0.0346\n",
            "44/240, Train_loss: 0.0346 Train_dice: 0.0346\n",
            "Epoch [18/100], Batch [45/240],  Cross Loss: 0.5243\n",
            "45/240, Train_loss: 0.5243 Train_dice: 0.5243\n",
            "Epoch [18/100], Batch [46/240],  Cross Loss: 0.1762\n",
            "46/240, Train_loss: 0.1762 Train_dice: 0.1762\n",
            "Epoch [18/100], Batch [47/240],  Cross Loss: 0.5130\n",
            "47/240, Train_loss: 0.5130 Train_dice: 0.5130\n",
            "Epoch [18/100], Batch [48/240],  Cross Loss: 0.0120\n",
            "48/240, Train_loss: 0.0120 Train_dice: 0.0120\n",
            "Epoch [18/100], Batch [49/240],  Cross Loss: 0.3206\n",
            "49/240, Train_loss: 0.3206 Train_dice: 0.3206\n",
            "Epoch [18/100], Batch [50/240],  Cross Loss: 0.0192\n",
            "50/240, Train_loss: 0.0192 Train_dice: 0.0192\n",
            "Epoch [18/100], Batch [51/240],  Cross Loss: 0.3042\n",
            "51/240, Train_loss: 0.3042 Train_dice: 0.3042\n",
            "Epoch [18/100], Batch [52/240],  Cross Loss: 0.0345\n",
            "52/240, Train_loss: 0.0345 Train_dice: 0.0345\n",
            "Epoch [18/100], Batch [53/240],  Cross Loss: 0.0090\n",
            "53/240, Train_loss: 0.0090 Train_dice: 0.0090\n",
            "Epoch [18/100], Batch [54/240],  Cross Loss: 0.1617\n",
            "54/240, Train_loss: 0.1617 Train_dice: 0.1617\n",
            "Epoch [18/100], Batch [55/240],  Cross Loss: 0.1816\n",
            "55/240, Train_loss: 0.1816 Train_dice: 0.1816\n",
            "Epoch [18/100], Batch [56/240],  Cross Loss: 0.0471\n",
            "56/240, Train_loss: 0.0471 Train_dice: 0.0471\n",
            "Epoch [18/100], Batch [57/240],  Cross Loss: 0.0699\n",
            "57/240, Train_loss: 0.0699 Train_dice: 0.0699\n",
            "Epoch [18/100], Batch [58/240],  Cross Loss: 0.0083\n",
            "58/240, Train_loss: 0.0083 Train_dice: 0.0083\n",
            "Epoch [18/100], Batch [59/240],  Cross Loss: 0.0510\n",
            "59/240, Train_loss: 0.0510 Train_dice: 0.0510\n",
            "Epoch [18/100], Batch [60/240],  Cross Loss: 0.0163\n",
            "60/240, Train_loss: 0.0163 Train_dice: 0.0163\n",
            "Epoch [18/100], Batch [61/240],  Cross Loss: 0.1561\n",
            "61/240, Train_loss: 0.1561 Train_dice: 0.1561\n",
            "Epoch [18/100], Batch [62/240],  Cross Loss: 0.0080\n",
            "62/240, Train_loss: 0.0080 Train_dice: 0.0080\n",
            "Epoch [18/100], Batch [63/240],  Cross Loss: 0.0151\n",
            "63/240, Train_loss: 0.0151 Train_dice: 0.0151\n",
            "Epoch [18/100], Batch [64/240],  Cross Loss: 0.0075\n",
            "64/240, Train_loss: 0.0075 Train_dice: 0.0075\n",
            "Epoch [18/100], Batch [65/240],  Cross Loss: 0.0509\n",
            "65/240, Train_loss: 0.0509 Train_dice: 0.0509\n",
            "Epoch [18/100], Batch [66/240],  Cross Loss: 0.2884\n",
            "66/240, Train_loss: 0.2884 Train_dice: 0.2884\n",
            "Epoch [18/100], Batch [67/240],  Cross Loss: 0.1914\n",
            "67/240, Train_loss: 0.1914 Train_dice: 0.1914\n",
            "Epoch [18/100], Batch [68/240],  Cross Loss: 0.1057\n",
            "68/240, Train_loss: 0.1057 Train_dice: 0.1057\n",
            "Epoch [18/100], Batch [69/240],  Cross Loss: 0.1311\n",
            "69/240, Train_loss: 0.1311 Train_dice: 0.1311\n",
            "Epoch [18/100], Batch [70/240],  Cross Loss: 0.3182\n",
            "70/240, Train_loss: 0.3182 Train_dice: 0.3182\n",
            "Epoch [18/100], Batch [71/240],  Cross Loss: 0.4090\n",
            "71/240, Train_loss: 0.4090 Train_dice: 0.4090\n",
            "Epoch [18/100], Batch [72/240],  Cross Loss: 0.0652\n",
            "72/240, Train_loss: 0.0652 Train_dice: 0.0652\n",
            "Epoch [18/100], Batch [73/240],  Cross Loss: 0.0675\n",
            "73/240, Train_loss: 0.0675 Train_dice: 0.0675\n",
            "Epoch [18/100], Batch [74/240],  Cross Loss: 0.0856\n",
            "74/240, Train_loss: 0.0856 Train_dice: 0.0856\n",
            "Epoch [18/100], Batch [75/240],  Cross Loss: 0.3680\n",
            "75/240, Train_loss: 0.3680 Train_dice: 0.3680\n",
            "Epoch [18/100], Batch [76/240],  Cross Loss: 0.3958\n",
            "76/240, Train_loss: 0.3958 Train_dice: 0.3958\n",
            "Epoch [18/100], Batch [77/240],  Cross Loss: 0.0232\n",
            "77/240, Train_loss: 0.0232 Train_dice: 0.0232\n",
            "Epoch [18/100], Batch [78/240],  Cross Loss: 0.3077\n",
            "78/240, Train_loss: 0.3077 Train_dice: 0.3077\n",
            "Epoch [18/100], Batch [79/240],  Cross Loss: 0.0041\n",
            "79/240, Train_loss: 0.0041 Train_dice: 0.0041\n",
            "Epoch [18/100], Batch [80/240],  Cross Loss: 0.1659\n",
            "80/240, Train_loss: 0.1659 Train_dice: 0.1659\n",
            "Epoch [18/100], Batch [81/240],  Cross Loss: 0.0020\n",
            "81/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [18/100], Batch [82/240],  Cross Loss: 0.3110\n",
            "82/240, Train_loss: 0.3110 Train_dice: 0.3110\n",
            "Epoch [18/100], Batch [83/240],  Cross Loss: 0.2160\n",
            "83/240, Train_loss: 0.2160 Train_dice: 0.2160\n",
            "Epoch [18/100], Batch [84/240],  Cross Loss: 0.0049\n",
            "84/240, Train_loss: 0.0049 Train_dice: 0.0049\n",
            "Epoch [18/100], Batch [85/240],  Cross Loss: 0.0030\n",
            "85/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [18/100], Batch [86/240],  Cross Loss: 0.1426\n",
            "86/240, Train_loss: 0.1426 Train_dice: 0.1426\n",
            "Epoch [18/100], Batch [87/240],  Cross Loss: 0.0838\n",
            "87/240, Train_loss: 0.0838 Train_dice: 0.0838\n",
            "Epoch [18/100], Batch [88/240],  Cross Loss: 0.1511\n",
            "88/240, Train_loss: 0.1511 Train_dice: 0.1511\n",
            "Epoch [18/100], Batch [89/240],  Cross Loss: 0.0029\n",
            "89/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [18/100], Batch [90/240],  Cross Loss: 0.0071\n",
            "90/240, Train_loss: 0.0071 Train_dice: 0.0071\n",
            "Epoch [18/100], Batch [91/240],  Cross Loss: 0.2218\n",
            "91/240, Train_loss: 0.2218 Train_dice: 0.2218\n",
            "Epoch [18/100], Batch [92/240],  Cross Loss: 0.2042\n",
            "92/240, Train_loss: 0.2042 Train_dice: 0.2042\n",
            "Epoch [18/100], Batch [93/240],  Cross Loss: 0.4051\n",
            "93/240, Train_loss: 0.4051 Train_dice: 0.4051\n",
            "Epoch [18/100], Batch [94/240],  Cross Loss: 0.4331\n",
            "94/240, Train_loss: 0.4331 Train_dice: 0.4331\n",
            "Epoch [18/100], Batch [95/240],  Cross Loss: 0.0018\n",
            "95/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [18/100], Batch [96/240],  Cross Loss: 0.3339\n",
            "96/240, Train_loss: 0.3339 Train_dice: 0.3339\n",
            "Epoch [18/100], Batch [97/240],  Cross Loss: 0.0522\n",
            "97/240, Train_loss: 0.0522 Train_dice: 0.0522\n",
            "Epoch [18/100], Batch [98/240],  Cross Loss: 0.0014\n",
            "98/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [18/100], Batch [99/240],  Cross Loss: 0.1751\n",
            "99/240, Train_loss: 0.1751 Train_dice: 0.1751\n",
            "Epoch [18/100], Batch [100/240],  Cross Loss: 0.0428\n",
            "100/240, Train_loss: 0.0428 Train_dice: 0.0428\n",
            "Epoch [18/100], Batch [101/240],  Cross Loss: 0.5702\n",
            "101/240, Train_loss: 0.5702 Train_dice: 0.5702\n",
            "Epoch [18/100], Batch [102/240],  Cross Loss: 0.3440\n",
            "102/240, Train_loss: 0.3440 Train_dice: 0.3440\n",
            "Epoch [18/100], Batch [103/240],  Cross Loss: 0.0628\n",
            "103/240, Train_loss: 0.0628 Train_dice: 0.0628\n",
            "Epoch [18/100], Batch [104/240],  Cross Loss: 0.1871\n",
            "104/240, Train_loss: 0.1871 Train_dice: 0.1871\n",
            "Epoch [18/100], Batch [105/240],  Cross Loss: 0.0049\n",
            "105/240, Train_loss: 0.0049 Train_dice: 0.0049\n",
            "Epoch [18/100], Batch [106/240],  Cross Loss: 0.4280\n",
            "106/240, Train_loss: 0.4280 Train_dice: 0.4280\n",
            "Epoch [18/100], Batch [107/240],  Cross Loss: 0.4888\n",
            "107/240, Train_loss: 0.4888 Train_dice: 0.4888\n",
            "Epoch [18/100], Batch [108/240],  Cross Loss: 0.0367\n",
            "108/240, Train_loss: 0.0367 Train_dice: 0.0367\n",
            "Epoch [18/100], Batch [109/240],  Cross Loss: 0.0015\n",
            "109/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [18/100], Batch [110/240],  Cross Loss: 0.0108\n",
            "110/240, Train_loss: 0.0108 Train_dice: 0.0108\n",
            "Epoch [18/100], Batch [111/240],  Cross Loss: 0.0278\n",
            "111/240, Train_loss: 0.0278 Train_dice: 0.0278\n",
            "Epoch [18/100], Batch [112/240],  Cross Loss: 0.1447\n",
            "112/240, Train_loss: 0.1447 Train_dice: 0.1447\n",
            "Epoch [18/100], Batch [113/240],  Cross Loss: 0.2800\n",
            "113/240, Train_loss: 0.2800 Train_dice: 0.2800\n",
            "Epoch [18/100], Batch [114/240],  Cross Loss: 0.3838\n",
            "114/240, Train_loss: 0.3838 Train_dice: 0.3838\n",
            "Epoch [18/100], Batch [115/240],  Cross Loss: 0.6189\n",
            "115/240, Train_loss: 0.6189 Train_dice: 0.6189\n",
            "Epoch [18/100], Batch [116/240],  Cross Loss: 0.0085\n",
            "116/240, Train_loss: 0.0085 Train_dice: 0.0085\n",
            "Epoch [18/100], Batch [117/240],  Cross Loss: 0.0042\n",
            "117/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [18/100], Batch [118/240],  Cross Loss: 0.1666\n",
            "118/240, Train_loss: 0.1666 Train_dice: 0.1666\n",
            "Epoch [18/100], Batch [119/240],  Cross Loss: 0.0052\n",
            "119/240, Train_loss: 0.0052 Train_dice: 0.0052\n",
            "Epoch [18/100], Batch [120/240],  Cross Loss: 0.1664\n",
            "120/240, Train_loss: 0.1664 Train_dice: 0.1664\n",
            "Epoch [18/100], Batch [121/240],  Cross Loss: 0.5663\n",
            "121/240, Train_loss: 0.5663 Train_dice: 0.5663\n",
            "Epoch [18/100], Batch [122/240],  Cross Loss: 0.0254\n",
            "122/240, Train_loss: 0.0254 Train_dice: 0.0254\n",
            "Epoch [18/100], Batch [123/240],  Cross Loss: 0.0096\n",
            "123/240, Train_loss: 0.0096 Train_dice: 0.0096\n",
            "Epoch [18/100], Batch [124/240],  Cross Loss: 0.6220\n",
            "124/240, Train_loss: 0.6220 Train_dice: 0.6220\n",
            "Epoch [18/100], Batch [125/240],  Cross Loss: 0.1021\n",
            "125/240, Train_loss: 0.1021 Train_dice: 0.1021\n",
            "Epoch [18/100], Batch [126/240],  Cross Loss: 0.4764\n",
            "126/240, Train_loss: 0.4764 Train_dice: 0.4764\n",
            "Epoch [18/100], Batch [127/240],  Cross Loss: 0.1842\n",
            "127/240, Train_loss: 0.1842 Train_dice: 0.1842\n",
            "Epoch [18/100], Batch [128/240],  Cross Loss: 0.0128\n",
            "128/240, Train_loss: 0.0128 Train_dice: 0.0128\n",
            "Epoch [18/100], Batch [129/240],  Cross Loss: 0.0027\n",
            "129/240, Train_loss: 0.0027 Train_dice: 0.0027\n",
            "Epoch [18/100], Batch [130/240],  Cross Loss: 0.3735\n",
            "130/240, Train_loss: 0.3735 Train_dice: 0.3735\n",
            "Epoch [18/100], Batch [131/240],  Cross Loss: 0.0031\n",
            "131/240, Train_loss: 0.0031 Train_dice: 0.0031\n",
            "Epoch [18/100], Batch [132/240],  Cross Loss: 0.0208\n",
            "132/240, Train_loss: 0.0208 Train_dice: 0.0208\n",
            "Epoch [18/100], Batch [133/240],  Cross Loss: 0.0583\n",
            "133/240, Train_loss: 0.0583 Train_dice: 0.0583\n",
            "Epoch [18/100], Batch [134/240],  Cross Loss: 0.0810\n",
            "134/240, Train_loss: 0.0810 Train_dice: 0.0810\n",
            "Epoch [18/100], Batch [135/240],  Cross Loss: 0.0066\n",
            "135/240, Train_loss: 0.0066 Train_dice: 0.0066\n",
            "Epoch [18/100], Batch [136/240],  Cross Loss: 0.0479\n",
            "136/240, Train_loss: 0.0479 Train_dice: 0.0479\n",
            "Epoch [18/100], Batch [137/240],  Cross Loss: 0.6079\n",
            "137/240, Train_loss: 0.6079 Train_dice: 0.6079\n",
            "Epoch [18/100], Batch [138/240],  Cross Loss: 0.6008\n",
            "138/240, Train_loss: 0.6008 Train_dice: 0.6008\n",
            "Epoch [18/100], Batch [139/240],  Cross Loss: 0.0769\n",
            "139/240, Train_loss: 0.0769 Train_dice: 0.0769\n",
            "Epoch [18/100], Batch [140/240],  Cross Loss: 0.1869\n",
            "140/240, Train_loss: 0.1869 Train_dice: 0.1869\n",
            "Epoch [18/100], Batch [141/240],  Cross Loss: 0.4218\n",
            "141/240, Train_loss: 0.4218 Train_dice: 0.4218\n",
            "Epoch [18/100], Batch [142/240],  Cross Loss: 0.2188\n",
            "142/240, Train_loss: 0.2188 Train_dice: 0.2188\n",
            "Epoch [18/100], Batch [143/240],  Cross Loss: 0.0750\n",
            "143/240, Train_loss: 0.0750 Train_dice: 0.0750\n",
            "Epoch [18/100], Batch [144/240],  Cross Loss: 0.3846\n",
            "144/240, Train_loss: 0.3846 Train_dice: 0.3846\n",
            "Epoch [18/100], Batch [145/240],  Cross Loss: 0.2336\n",
            "145/240, Train_loss: 0.2336 Train_dice: 0.2336\n",
            "Epoch [18/100], Batch [146/240],  Cross Loss: 0.0008\n",
            "146/240, Train_loss: 0.0008 Train_dice: 0.0008\n",
            "Epoch [18/100], Batch [147/240],  Cross Loss: 0.0561\n",
            "147/240, Train_loss: 0.0561 Train_dice: 0.0561\n",
            "Epoch [18/100], Batch [148/240],  Cross Loss: 0.0009\n",
            "148/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [18/100], Batch [149/240],  Cross Loss: 0.0314\n",
            "149/240, Train_loss: 0.0314 Train_dice: 0.0314\n",
            "Epoch [18/100], Batch [150/240],  Cross Loss: 0.3446\n",
            "150/240, Train_loss: 0.3446 Train_dice: 0.3446\n",
            "Epoch [18/100], Batch [151/240],  Cross Loss: 0.0439\n",
            "151/240, Train_loss: 0.0439 Train_dice: 0.0439\n",
            "Epoch [18/100], Batch [152/240],  Cross Loss: 0.0911\n",
            "152/240, Train_loss: 0.0911 Train_dice: 0.0911\n",
            "Epoch [18/100], Batch [153/240],  Cross Loss: 0.0757\n",
            "153/240, Train_loss: 0.0757 Train_dice: 0.0757\n",
            "Epoch [18/100], Batch [154/240],  Cross Loss: 0.1686\n",
            "154/240, Train_loss: 0.1686 Train_dice: 0.1686\n",
            "Epoch [18/100], Batch [155/240],  Cross Loss: 0.2440\n",
            "155/240, Train_loss: 0.2440 Train_dice: 0.2440\n",
            "Epoch [18/100], Batch [156/240],  Cross Loss: 0.1643\n",
            "156/240, Train_loss: 0.1643 Train_dice: 0.1643\n",
            "Epoch [18/100], Batch [157/240],  Cross Loss: 0.0307\n",
            "157/240, Train_loss: 0.0307 Train_dice: 0.0307\n",
            "Epoch [18/100], Batch [158/240],  Cross Loss: 0.0026\n",
            "158/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [18/100], Batch [159/240],  Cross Loss: 0.0397\n",
            "159/240, Train_loss: 0.0397 Train_dice: 0.0397\n",
            "Epoch [18/100], Batch [160/240],  Cross Loss: 0.0082\n",
            "160/240, Train_loss: 0.0082 Train_dice: 0.0082\n",
            "Epoch [18/100], Batch [161/240],  Cross Loss: 0.0083\n",
            "161/240, Train_loss: 0.0083 Train_dice: 0.0083\n",
            "Epoch [18/100], Batch [162/240],  Cross Loss: 0.1016\n",
            "162/240, Train_loss: 0.1016 Train_dice: 0.1016\n",
            "Epoch [18/100], Batch [163/240],  Cross Loss: 0.2704\n",
            "163/240, Train_loss: 0.2704 Train_dice: 0.2704\n",
            "Epoch [18/100], Batch [164/240],  Cross Loss: 0.0435\n",
            "164/240, Train_loss: 0.0435 Train_dice: 0.0435\n",
            "Epoch [18/100], Batch [165/240],  Cross Loss: 0.2688\n",
            "165/240, Train_loss: 0.2688 Train_dice: 0.2688\n",
            "Epoch [18/100], Batch [166/240],  Cross Loss: 0.2357\n",
            "166/240, Train_loss: 0.2357 Train_dice: 0.2357\n",
            "Epoch [18/100], Batch [167/240],  Cross Loss: 0.2804\n",
            "167/240, Train_loss: 0.2804 Train_dice: 0.2804\n",
            "Epoch [18/100], Batch [168/240],  Cross Loss: 0.2113\n",
            "168/240, Train_loss: 0.2113 Train_dice: 0.2113\n",
            "Epoch [18/100], Batch [169/240],  Cross Loss: 0.1209\n",
            "169/240, Train_loss: 0.1209 Train_dice: 0.1209\n",
            "Epoch [18/100], Batch [170/240],  Cross Loss: 0.0029\n",
            "170/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [18/100], Batch [171/240],  Cross Loss: 0.0709\n",
            "171/240, Train_loss: 0.0709 Train_dice: 0.0709\n",
            "Epoch [18/100], Batch [172/240],  Cross Loss: 0.1009\n",
            "172/240, Train_loss: 0.1009 Train_dice: 0.1009\n",
            "Epoch [18/100], Batch [173/240],  Cross Loss: 0.0177\n",
            "173/240, Train_loss: 0.0177 Train_dice: 0.0177\n",
            "Epoch [18/100], Batch [174/240],  Cross Loss: 0.1419\n",
            "174/240, Train_loss: 0.1419 Train_dice: 0.1419\n",
            "Epoch [18/100], Batch [175/240],  Cross Loss: 0.0385\n",
            "175/240, Train_loss: 0.0385 Train_dice: 0.0385\n",
            "Epoch [18/100], Batch [176/240],  Cross Loss: 0.0037\n",
            "176/240, Train_loss: 0.0037 Train_dice: 0.0037\n",
            "Epoch [18/100], Batch [177/240],  Cross Loss: 0.0053\n",
            "177/240, Train_loss: 0.0053 Train_dice: 0.0053\n",
            "Epoch [18/100], Batch [178/240],  Cross Loss: 0.0020\n",
            "178/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [18/100], Batch [179/240],  Cross Loss: 0.1086\n",
            "179/240, Train_loss: 0.1086 Train_dice: 0.1086\n",
            "Epoch [18/100], Batch [180/240],  Cross Loss: 0.6343\n",
            "180/240, Train_loss: 0.6343 Train_dice: 0.6343\n",
            "Epoch [18/100], Batch [181/240],  Cross Loss: 0.0063\n",
            "181/240, Train_loss: 0.0063 Train_dice: 0.0063\n",
            "Epoch [18/100], Batch [182/240],  Cross Loss: 0.3421\n",
            "182/240, Train_loss: 0.3421 Train_dice: 0.3421\n",
            "Epoch [18/100], Batch [183/240],  Cross Loss: 0.0470\n",
            "183/240, Train_loss: 0.0470 Train_dice: 0.0470\n",
            "Epoch [18/100], Batch [184/240],  Cross Loss: 0.1975\n",
            "184/240, Train_loss: 0.1975 Train_dice: 0.1975\n",
            "Epoch [18/100], Batch [185/240],  Cross Loss: 0.3105\n",
            "185/240, Train_loss: 0.3105 Train_dice: 0.3105\n",
            "Epoch [18/100], Batch [186/240],  Cross Loss: 0.0058\n",
            "186/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [18/100], Batch [187/240],  Cross Loss: 0.2796\n",
            "187/240, Train_loss: 0.2796 Train_dice: 0.2796\n",
            "Epoch [18/100], Batch [188/240],  Cross Loss: 0.0238\n",
            "188/240, Train_loss: 0.0238 Train_dice: 0.0238\n",
            "Epoch [18/100], Batch [189/240],  Cross Loss: 0.0491\n",
            "189/240, Train_loss: 0.0491 Train_dice: 0.0491\n",
            "Epoch [18/100], Batch [190/240],  Cross Loss: 0.0036\n",
            "190/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "Epoch [18/100], Batch [191/240],  Cross Loss: 0.0172\n",
            "191/240, Train_loss: 0.0172 Train_dice: 0.0172\n",
            "Epoch [18/100], Batch [192/240],  Cross Loss: 0.3685\n",
            "192/240, Train_loss: 0.3685 Train_dice: 0.3685\n",
            "Epoch [18/100], Batch [193/240],  Cross Loss: 0.0040\n",
            "193/240, Train_loss: 0.0040 Train_dice: 0.0040\n",
            "Epoch [18/100], Batch [194/240],  Cross Loss: 0.0345\n",
            "194/240, Train_loss: 0.0345 Train_dice: 0.0345\n",
            "Epoch [18/100], Batch [195/240],  Cross Loss: 0.0378\n",
            "195/240, Train_loss: 0.0378 Train_dice: 0.0378\n",
            "Epoch [18/100], Batch [196/240],  Cross Loss: 0.0036\n",
            "196/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "Epoch [18/100], Batch [197/240],  Cross Loss: 0.0080\n",
            "197/240, Train_loss: 0.0080 Train_dice: 0.0080\n",
            "Epoch [18/100], Batch [198/240],  Cross Loss: 0.0410\n",
            "198/240, Train_loss: 0.0410 Train_dice: 0.0410\n",
            "Epoch [18/100], Batch [199/240],  Cross Loss: 0.0161\n",
            "199/240, Train_loss: 0.0161 Train_dice: 0.0161\n",
            "Epoch [18/100], Batch [200/240],  Cross Loss: 0.0250\n",
            "200/240, Train_loss: 0.0250 Train_dice: 0.0250\n",
            "Epoch [18/100], Batch [201/240],  Cross Loss: 0.0061\n",
            "201/240, Train_loss: 0.0061 Train_dice: 0.0061\n",
            "Epoch [18/100], Batch [202/240],  Cross Loss: 0.4330\n",
            "202/240, Train_loss: 0.4330 Train_dice: 0.4330\n",
            "Epoch [18/100], Batch [203/240],  Cross Loss: 0.3291\n",
            "203/240, Train_loss: 0.3291 Train_dice: 0.3291\n",
            "Epoch [18/100], Batch [204/240],  Cross Loss: 0.5088\n",
            "204/240, Train_loss: 0.5088 Train_dice: 0.5088\n",
            "Epoch [18/100], Batch [205/240],  Cross Loss: 0.0105\n",
            "205/240, Train_loss: 0.0105 Train_dice: 0.0105\n",
            "Epoch [18/100], Batch [206/240],  Cross Loss: 0.2371\n",
            "206/240, Train_loss: 0.2371 Train_dice: 0.2371\n",
            "Epoch [18/100], Batch [207/240],  Cross Loss: 0.2156\n",
            "207/240, Train_loss: 0.2156 Train_dice: 0.2156\n",
            "Epoch [18/100], Batch [208/240],  Cross Loss: 0.0572\n",
            "208/240, Train_loss: 0.0572 Train_dice: 0.0572\n",
            "Epoch [18/100], Batch [209/240],  Cross Loss: 0.1206\n",
            "209/240, Train_loss: 0.1206 Train_dice: 0.1206\n",
            "Epoch [18/100], Batch [210/240],  Cross Loss: 0.4548\n",
            "210/240, Train_loss: 0.4548 Train_dice: 0.4548\n",
            "Epoch [18/100], Batch [211/240],  Cross Loss: 0.0364\n",
            "211/240, Train_loss: 0.0364 Train_dice: 0.0364\n",
            "Epoch [18/100], Batch [212/240],  Cross Loss: 0.0035\n",
            "212/240, Train_loss: 0.0035 Train_dice: 0.0035\n",
            "Epoch [18/100], Batch [213/240],  Cross Loss: 0.0025\n",
            "213/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [18/100], Batch [214/240],  Cross Loss: 0.0643\n",
            "214/240, Train_loss: 0.0643 Train_dice: 0.0643\n",
            "Epoch [18/100], Batch [215/240],  Cross Loss: 0.3589\n",
            "215/240, Train_loss: 0.3589 Train_dice: 0.3589\n",
            "Epoch [18/100], Batch [216/240],  Cross Loss: 0.1685\n",
            "216/240, Train_loss: 0.1685 Train_dice: 0.1685\n",
            "Epoch [18/100], Batch [217/240],  Cross Loss: 0.0012\n",
            "217/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [18/100], Batch [218/240],  Cross Loss: 0.0767\n",
            "218/240, Train_loss: 0.0767 Train_dice: 0.0767\n",
            "Epoch [18/100], Batch [219/240],  Cross Loss: 0.0113\n",
            "219/240, Train_loss: 0.0113 Train_dice: 0.0113\n",
            "Epoch [18/100], Batch [220/240],  Cross Loss: 0.0374\n",
            "220/240, Train_loss: 0.0374 Train_dice: 0.0374\n",
            "Epoch [18/100], Batch [221/240],  Cross Loss: 0.0050\n",
            "221/240, Train_loss: 0.0050 Train_dice: 0.0050\n",
            "Epoch [18/100], Batch [222/240],  Cross Loss: 0.6388\n",
            "222/240, Train_loss: 0.6388 Train_dice: 0.6388\n",
            "Epoch [18/100], Batch [223/240],  Cross Loss: 0.1193\n",
            "223/240, Train_loss: 0.1193 Train_dice: 0.1193\n",
            "Epoch [18/100], Batch [224/240],  Cross Loss: 0.0553\n",
            "224/240, Train_loss: 0.0553 Train_dice: 0.0553\n",
            "Epoch [18/100], Batch [225/240],  Cross Loss: 0.0425\n",
            "225/240, Train_loss: 0.0425 Train_dice: 0.0425\n",
            "Epoch [18/100], Batch [226/240],  Cross Loss: 0.0042\n",
            "226/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [18/100], Batch [227/240],  Cross Loss: 0.6624\n",
            "227/240, Train_loss: 0.6624 Train_dice: 0.6624\n",
            "Epoch [18/100], Batch [228/240],  Cross Loss: 0.3364\n",
            "228/240, Train_loss: 0.3364 Train_dice: 0.3364\n",
            "Epoch [18/100], Batch [229/240],  Cross Loss: 0.0056\n",
            "229/240, Train_loss: 0.0056 Train_dice: 0.0056\n",
            "Epoch [18/100], Batch [230/240],  Cross Loss: 0.0069\n",
            "230/240, Train_loss: 0.0069 Train_dice: 0.0069\n",
            "Epoch [18/100], Batch [231/240],  Cross Loss: 0.0070\n",
            "231/240, Train_loss: 0.0070 Train_dice: 0.0070\n",
            "Epoch [18/100], Batch [232/240],  Cross Loss: 0.0074\n",
            "232/240, Train_loss: 0.0074 Train_dice: 0.0074\n",
            "Epoch [18/100], Batch [233/240],  Cross Loss: 0.0015\n",
            "233/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [18/100], Batch [234/240],  Cross Loss: 0.6859\n",
            "234/240, Train_loss: 0.6859 Train_dice: 0.6859\n",
            "Epoch [18/100], Batch [235/240],  Cross Loss: 0.2856\n",
            "235/240, Train_loss: 0.2856 Train_dice: 0.2856\n",
            "Epoch [18/100], Batch [236/240],  Cross Loss: 0.0635\n",
            "236/240, Train_loss: 0.0635 Train_dice: 0.0635\n",
            "Epoch [18/100], Batch [237/240],  Cross Loss: 0.0020\n",
            "237/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [18/100], Batch [238/240],  Cross Loss: 0.0013\n",
            "238/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [18/100], Batch [239/240],  Cross Loss: 0.0095\n",
            "239/240, Train_loss: 0.0095 Train_dice: 0.0095\n",
            "Epoch [18/100], Batch [240/240],  Cross Loss: 0.0283\n",
            "240/240, Train_loss: 0.0283 Train_dice: 0.0283\n",
            "--------------------\n",
            "Epoch_loss: 0.1503\n",
            "Epoch_metric: tensor(0.1503, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.4476\n",
            "test_dice_epoch: tensor(0.4476, device='cuda:0')\n",
            "current epoch: 18 current mean dice: tensor(0.4476, device='cuda:0')\n",
            "best mean dice: tensor(0.4476, device='cuda:0') at epoch: 18\n",
            "----------\n",
            "epoch 19/100\n",
            "Epoch [19/100], Batch [1/240],  Cross Loss: 0.0226\n",
            "1/240, Train_loss: 0.0226 Train_dice: 0.0226\n",
            "Epoch [19/100], Batch [2/240],  Cross Loss: 0.0509\n",
            "2/240, Train_loss: 0.0509 Train_dice: 0.0509\n",
            "Epoch [19/100], Batch [3/240],  Cross Loss: 0.2604\n",
            "3/240, Train_loss: 0.2604 Train_dice: 0.2604\n",
            "Epoch [19/100], Batch [4/240],  Cross Loss: 0.0932\n",
            "4/240, Train_loss: 0.0932 Train_dice: 0.0932\n",
            "Epoch [19/100], Batch [5/240],  Cross Loss: 0.0958\n",
            "5/240, Train_loss: 0.0958 Train_dice: 0.0958\n",
            "Epoch [19/100], Batch [6/240],  Cross Loss: 0.0227\n",
            "6/240, Train_loss: 0.0227 Train_dice: 0.0227\n",
            "Epoch [19/100], Batch [7/240],  Cross Loss: 0.0017\n",
            "7/240, Train_loss: 0.0017 Train_dice: 0.0017\n",
            "Epoch [19/100], Batch [8/240],  Cross Loss: 0.0072\n",
            "8/240, Train_loss: 0.0072 Train_dice: 0.0072\n",
            "Epoch [19/100], Batch [9/240],  Cross Loss: 0.0647\n",
            "9/240, Train_loss: 0.0647 Train_dice: 0.0647\n",
            "Epoch [19/100], Batch [10/240],  Cross Loss: 0.0103\n",
            "10/240, Train_loss: 0.0103 Train_dice: 0.0103\n",
            "Epoch [19/100], Batch [11/240],  Cross Loss: 0.0036\n",
            "11/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "Epoch [19/100], Batch [12/240],  Cross Loss: 0.0487\n",
            "12/240, Train_loss: 0.0487 Train_dice: 0.0487\n",
            "Epoch [19/100], Batch [13/240],  Cross Loss: 0.0614\n",
            "13/240, Train_loss: 0.0614 Train_dice: 0.0614\n",
            "Epoch [19/100], Batch [14/240],  Cross Loss: 0.1655\n",
            "14/240, Train_loss: 0.1655 Train_dice: 0.1655\n",
            "Epoch [19/100], Batch [15/240],  Cross Loss: 0.0240\n",
            "15/240, Train_loss: 0.0240 Train_dice: 0.0240\n",
            "Epoch [19/100], Batch [16/240],  Cross Loss: 0.1013\n",
            "16/240, Train_loss: 0.1013 Train_dice: 0.1013\n",
            "Epoch [19/100], Batch [17/240],  Cross Loss: 0.4078\n",
            "17/240, Train_loss: 0.4078 Train_dice: 0.4078\n",
            "Epoch [19/100], Batch [18/240],  Cross Loss: 0.0010\n",
            "18/240, Train_loss: 0.0010 Train_dice: 0.0010\n",
            "Epoch [19/100], Batch [19/240],  Cross Loss: 0.5129\n",
            "19/240, Train_loss: 0.5129 Train_dice: 0.5129\n",
            "Epoch [19/100], Batch [20/240],  Cross Loss: 0.0344\n",
            "20/240, Train_loss: 0.0344 Train_dice: 0.0344\n",
            "Epoch [19/100], Batch [21/240],  Cross Loss: 0.0081\n",
            "21/240, Train_loss: 0.0081 Train_dice: 0.0081\n",
            "Epoch [19/100], Batch [22/240],  Cross Loss: 0.0044\n",
            "22/240, Train_loss: 0.0044 Train_dice: 0.0044\n",
            "Epoch [19/100], Batch [23/240],  Cross Loss: 0.1406\n",
            "23/240, Train_loss: 0.1406 Train_dice: 0.1406\n",
            "Epoch [19/100], Batch [24/240],  Cross Loss: 0.1145\n",
            "24/240, Train_loss: 0.1145 Train_dice: 0.1145\n",
            "Epoch [19/100], Batch [25/240],  Cross Loss: 0.0064\n",
            "25/240, Train_loss: 0.0064 Train_dice: 0.0064\n",
            "Epoch [19/100], Batch [26/240],  Cross Loss: 0.0133\n",
            "26/240, Train_loss: 0.0133 Train_dice: 0.0133\n",
            "Epoch [19/100], Batch [27/240],  Cross Loss: 0.0092\n",
            "27/240, Train_loss: 0.0092 Train_dice: 0.0092\n",
            "Epoch [19/100], Batch [28/240],  Cross Loss: 0.0085\n",
            "28/240, Train_loss: 0.0085 Train_dice: 0.0085\n",
            "Epoch [19/100], Batch [29/240],  Cross Loss: 0.0129\n",
            "29/240, Train_loss: 0.0129 Train_dice: 0.0129\n",
            "Epoch [19/100], Batch [30/240],  Cross Loss: 0.4655\n",
            "30/240, Train_loss: 0.4655 Train_dice: 0.4655\n",
            "Epoch [19/100], Batch [31/240],  Cross Loss: 0.4056\n",
            "31/240, Train_loss: 0.4056 Train_dice: 0.4056\n",
            "Epoch [19/100], Batch [32/240],  Cross Loss: 0.5100\n",
            "32/240, Train_loss: 0.5100 Train_dice: 0.5100\n",
            "Epoch [19/100], Batch [33/240],  Cross Loss: 0.1285\n",
            "33/240, Train_loss: 0.1285 Train_dice: 0.1285\n",
            "Epoch [19/100], Batch [34/240],  Cross Loss: 0.3675\n",
            "34/240, Train_loss: 0.3675 Train_dice: 0.3675\n",
            "Epoch [19/100], Batch [35/240],  Cross Loss: 0.0012\n",
            "35/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [19/100], Batch [36/240],  Cross Loss: 0.0348\n",
            "36/240, Train_loss: 0.0348 Train_dice: 0.0348\n",
            "Epoch [19/100], Batch [37/240],  Cross Loss: 0.1009\n",
            "37/240, Train_loss: 0.1009 Train_dice: 0.1009\n",
            "Epoch [19/100], Batch [38/240],  Cross Loss: 0.1373\n",
            "38/240, Train_loss: 0.1373 Train_dice: 0.1373\n",
            "Epoch [19/100], Batch [39/240],  Cross Loss: 0.0047\n",
            "39/240, Train_loss: 0.0047 Train_dice: 0.0047\n",
            "Epoch [19/100], Batch [40/240],  Cross Loss: 0.0687\n",
            "40/240, Train_loss: 0.0687 Train_dice: 0.0687\n",
            "Epoch [19/100], Batch [41/240],  Cross Loss: 0.0129\n",
            "41/240, Train_loss: 0.0129 Train_dice: 0.0129\n",
            "Epoch [19/100], Batch [42/240],  Cross Loss: 0.7197\n",
            "42/240, Train_loss: 0.7197 Train_dice: 0.7197\n",
            "Epoch [19/100], Batch [43/240],  Cross Loss: 0.0034\n",
            "43/240, Train_loss: 0.0034 Train_dice: 0.0034\n",
            "Epoch [19/100], Batch [44/240],  Cross Loss: 0.0229\n",
            "44/240, Train_loss: 0.0229 Train_dice: 0.0229\n",
            "Epoch [19/100], Batch [45/240],  Cross Loss: 0.0418\n",
            "45/240, Train_loss: 0.0418 Train_dice: 0.0418\n",
            "Epoch [19/100], Batch [46/240],  Cross Loss: 0.0670\n",
            "46/240, Train_loss: 0.0670 Train_dice: 0.0670\n",
            "Epoch [19/100], Batch [47/240],  Cross Loss: 0.0465\n",
            "47/240, Train_loss: 0.0465 Train_dice: 0.0465\n",
            "Epoch [19/100], Batch [48/240],  Cross Loss: 0.0521\n",
            "48/240, Train_loss: 0.0521 Train_dice: 0.0521\n",
            "Epoch [19/100], Batch [49/240],  Cross Loss: 0.0199\n",
            "49/240, Train_loss: 0.0199 Train_dice: 0.0199\n",
            "Epoch [19/100], Batch [50/240],  Cross Loss: 0.0834\n",
            "50/240, Train_loss: 0.0834 Train_dice: 0.0834\n",
            "Epoch [19/100], Batch [51/240],  Cross Loss: 0.0076\n",
            "51/240, Train_loss: 0.0076 Train_dice: 0.0076\n",
            "Epoch [19/100], Batch [52/240],  Cross Loss: 0.1852\n",
            "52/240, Train_loss: 0.1852 Train_dice: 0.1852\n",
            "Epoch [19/100], Batch [53/240],  Cross Loss: 0.0099\n",
            "53/240, Train_loss: 0.0099 Train_dice: 0.0099\n",
            "Epoch [19/100], Batch [54/240],  Cross Loss: 0.0200\n",
            "54/240, Train_loss: 0.0200 Train_dice: 0.0200\n",
            "Epoch [19/100], Batch [55/240],  Cross Loss: 0.2658\n",
            "55/240, Train_loss: 0.2658 Train_dice: 0.2658\n",
            "Epoch [19/100], Batch [56/240],  Cross Loss: 0.4418\n",
            "56/240, Train_loss: 0.4418 Train_dice: 0.4418\n",
            "Epoch [19/100], Batch [57/240],  Cross Loss: 0.0029\n",
            "57/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [19/100], Batch [58/240],  Cross Loss: 0.0831\n",
            "58/240, Train_loss: 0.0831 Train_dice: 0.0831\n",
            "Epoch [19/100], Batch [59/240],  Cross Loss: 0.0024\n",
            "59/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [19/100], Batch [60/240],  Cross Loss: 0.5917\n",
            "60/240, Train_loss: 0.5917 Train_dice: 0.5917\n",
            "Epoch [19/100], Batch [61/240],  Cross Loss: 0.3469\n",
            "61/240, Train_loss: 0.3469 Train_dice: 0.3469\n",
            "Epoch [19/100], Batch [62/240],  Cross Loss: 0.0955\n",
            "62/240, Train_loss: 0.0955 Train_dice: 0.0955\n",
            "Epoch [19/100], Batch [63/240],  Cross Loss: 0.0578\n",
            "63/240, Train_loss: 0.0578 Train_dice: 0.0578\n",
            "Epoch [19/100], Batch [64/240],  Cross Loss: 0.5430\n",
            "64/240, Train_loss: 0.5430 Train_dice: 0.5430\n",
            "Epoch [19/100], Batch [65/240],  Cross Loss: 0.0013\n",
            "65/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [19/100], Batch [66/240],  Cross Loss: 0.0026\n",
            "66/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [19/100], Batch [67/240],  Cross Loss: 0.0340\n",
            "67/240, Train_loss: 0.0340 Train_dice: 0.0340\n",
            "Epoch [19/100], Batch [68/240],  Cross Loss: 0.0175\n",
            "68/240, Train_loss: 0.0175 Train_dice: 0.0175\n",
            "Epoch [19/100], Batch [69/240],  Cross Loss: 0.2038\n",
            "69/240, Train_loss: 0.2038 Train_dice: 0.2038\n",
            "Epoch [19/100], Batch [70/240],  Cross Loss: 0.2303\n",
            "70/240, Train_loss: 0.2303 Train_dice: 0.2303\n",
            "Epoch [19/100], Batch [71/240],  Cross Loss: 0.5803\n",
            "71/240, Train_loss: 0.5803 Train_dice: 0.5803\n",
            "Epoch [19/100], Batch [72/240],  Cross Loss: 0.0115\n",
            "72/240, Train_loss: 0.0115 Train_dice: 0.0115\n",
            "Epoch [19/100], Batch [73/240],  Cross Loss: 0.3038\n",
            "73/240, Train_loss: 0.3038 Train_dice: 0.3038\n",
            "Epoch [19/100], Batch [74/240],  Cross Loss: 0.0549\n",
            "74/240, Train_loss: 0.0549 Train_dice: 0.0549\n",
            "Epoch [19/100], Batch [75/240],  Cross Loss: 0.0736\n",
            "75/240, Train_loss: 0.0736 Train_dice: 0.0736\n",
            "Epoch [19/100], Batch [76/240],  Cross Loss: 0.0012\n",
            "76/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [19/100], Batch [77/240],  Cross Loss: 0.0605\n",
            "77/240, Train_loss: 0.0605 Train_dice: 0.0605\n",
            "Epoch [19/100], Batch [78/240],  Cross Loss: 0.0175\n",
            "78/240, Train_loss: 0.0175 Train_dice: 0.0175\n",
            "Epoch [19/100], Batch [79/240],  Cross Loss: 0.0721\n",
            "79/240, Train_loss: 0.0721 Train_dice: 0.0721\n",
            "Epoch [19/100], Batch [80/240],  Cross Loss: 0.0532\n",
            "80/240, Train_loss: 0.0532 Train_dice: 0.0532\n",
            "Epoch [19/100], Batch [81/240],  Cross Loss: 0.0029\n",
            "81/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [19/100], Batch [82/240],  Cross Loss: 0.4189\n",
            "82/240, Train_loss: 0.4189 Train_dice: 0.4189\n",
            "Epoch [19/100], Batch [83/240],  Cross Loss: 0.0831\n",
            "83/240, Train_loss: 0.0831 Train_dice: 0.0831\n",
            "Epoch [19/100], Batch [84/240],  Cross Loss: 0.1229\n",
            "84/240, Train_loss: 0.1229 Train_dice: 0.1229\n",
            "Epoch [19/100], Batch [85/240],  Cross Loss: 0.0028\n",
            "85/240, Train_loss: 0.0028 Train_dice: 0.0028\n",
            "Epoch [19/100], Batch [86/240],  Cross Loss: 0.1098\n",
            "86/240, Train_loss: 0.1098 Train_dice: 0.1098\n",
            "Epoch [19/100], Batch [87/240],  Cross Loss: 0.0045\n",
            "87/240, Train_loss: 0.0045 Train_dice: 0.0045\n",
            "Epoch [19/100], Batch [88/240],  Cross Loss: 0.1670\n",
            "88/240, Train_loss: 0.1670 Train_dice: 0.1670\n",
            "Epoch [19/100], Batch [89/240],  Cross Loss: 0.0200\n",
            "89/240, Train_loss: 0.0200 Train_dice: 0.0200\n",
            "Epoch [19/100], Batch [90/240],  Cross Loss: 0.0086\n",
            "90/240, Train_loss: 0.0086 Train_dice: 0.0086\n",
            "Epoch [19/100], Batch [91/240],  Cross Loss: 0.2545\n",
            "91/240, Train_loss: 0.2545 Train_dice: 0.2545\n",
            "Epoch [19/100], Batch [92/240],  Cross Loss: 0.3118\n",
            "92/240, Train_loss: 0.3118 Train_dice: 0.3118\n",
            "Epoch [19/100], Batch [93/240],  Cross Loss: 0.0094\n",
            "93/240, Train_loss: 0.0094 Train_dice: 0.0094\n",
            "Epoch [19/100], Batch [94/240],  Cross Loss: 0.3217\n",
            "94/240, Train_loss: 0.3217 Train_dice: 0.3217\n",
            "Epoch [19/100], Batch [95/240],  Cross Loss: 0.0296\n",
            "95/240, Train_loss: 0.0296 Train_dice: 0.0296\n",
            "Epoch [19/100], Batch [96/240],  Cross Loss: 0.0202\n",
            "96/240, Train_loss: 0.0202 Train_dice: 0.0202\n",
            "Epoch [19/100], Batch [97/240],  Cross Loss: 0.0111\n",
            "97/240, Train_loss: 0.0111 Train_dice: 0.0111\n",
            "Epoch [19/100], Batch [98/240],  Cross Loss: 0.0641\n",
            "98/240, Train_loss: 0.0641 Train_dice: 0.0641\n",
            "Epoch [19/100], Batch [99/240],  Cross Loss: 0.0063\n",
            "99/240, Train_loss: 0.0063 Train_dice: 0.0063\n",
            "Epoch [19/100], Batch [100/240],  Cross Loss: 0.0042\n",
            "100/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [19/100], Batch [101/240],  Cross Loss: 0.1752\n",
            "101/240, Train_loss: 0.1752 Train_dice: 0.1752\n",
            "Epoch [19/100], Batch [102/240],  Cross Loss: 0.2775\n",
            "102/240, Train_loss: 0.2775 Train_dice: 0.2775\n",
            "Epoch [19/100], Batch [103/240],  Cross Loss: 0.6114\n",
            "103/240, Train_loss: 0.6114 Train_dice: 0.6114\n",
            "Epoch [19/100], Batch [104/240],  Cross Loss: 0.0291\n",
            "104/240, Train_loss: 0.0291 Train_dice: 0.0291\n",
            "Epoch [19/100], Batch [105/240],  Cross Loss: 0.0282\n",
            "105/240, Train_loss: 0.0282 Train_dice: 0.0282\n",
            "Epoch [19/100], Batch [106/240],  Cross Loss: 0.2074\n",
            "106/240, Train_loss: 0.2074 Train_dice: 0.2074\n",
            "Epoch [19/100], Batch [107/240],  Cross Loss: 0.1461\n",
            "107/240, Train_loss: 0.1461 Train_dice: 0.1461\n",
            "Epoch [19/100], Batch [108/240],  Cross Loss: 0.0024\n",
            "108/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [19/100], Batch [109/240],  Cross Loss: 0.1931\n",
            "109/240, Train_loss: 0.1931 Train_dice: 0.1931\n",
            "Epoch [19/100], Batch [110/240],  Cross Loss: 0.0064\n",
            "110/240, Train_loss: 0.0064 Train_dice: 0.0064\n",
            "Epoch [19/100], Batch [111/240],  Cross Loss: 0.0534\n",
            "111/240, Train_loss: 0.0534 Train_dice: 0.0534\n",
            "Epoch [19/100], Batch [112/240],  Cross Loss: 0.0043\n",
            "112/240, Train_loss: 0.0043 Train_dice: 0.0043\n",
            "Epoch [19/100], Batch [113/240],  Cross Loss: 0.6439\n",
            "113/240, Train_loss: 0.6439 Train_dice: 0.6439\n",
            "Epoch [19/100], Batch [114/240],  Cross Loss: 0.0114\n",
            "114/240, Train_loss: 0.0114 Train_dice: 0.0114\n",
            "Epoch [19/100], Batch [115/240],  Cross Loss: 0.0055\n",
            "115/240, Train_loss: 0.0055 Train_dice: 0.0055\n",
            "Epoch [19/100], Batch [116/240],  Cross Loss: 0.0274\n",
            "116/240, Train_loss: 0.0274 Train_dice: 0.0274\n",
            "Epoch [19/100], Batch [117/240],  Cross Loss: 0.0034\n",
            "117/240, Train_loss: 0.0034 Train_dice: 0.0034\n",
            "Epoch [19/100], Batch [118/240],  Cross Loss: 0.0482\n",
            "118/240, Train_loss: 0.0482 Train_dice: 0.0482\n",
            "Epoch [19/100], Batch [119/240],  Cross Loss: 0.5699\n",
            "119/240, Train_loss: 0.5699 Train_dice: 0.5699\n",
            "Epoch [19/100], Batch [120/240],  Cross Loss: 0.0793\n",
            "120/240, Train_loss: 0.0793 Train_dice: 0.0793\n",
            "Epoch [19/100], Batch [121/240],  Cross Loss: 0.0062\n",
            "121/240, Train_loss: 0.0062 Train_dice: 0.0062\n",
            "Epoch [19/100], Batch [122/240],  Cross Loss: 0.0133\n",
            "122/240, Train_loss: 0.0133 Train_dice: 0.0133\n",
            "Epoch [19/100], Batch [123/240],  Cross Loss: 0.3144\n",
            "123/240, Train_loss: 0.3144 Train_dice: 0.3144\n",
            "Epoch [19/100], Batch [124/240],  Cross Loss: 0.0012\n",
            "124/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [19/100], Batch [125/240],  Cross Loss: 0.2089\n",
            "125/240, Train_loss: 0.2089 Train_dice: 0.2089\n",
            "Epoch [19/100], Batch [126/240],  Cross Loss: 0.0101\n",
            "126/240, Train_loss: 0.0101 Train_dice: 0.0101\n",
            "Epoch [19/100], Batch [127/240],  Cross Loss: 0.0101\n",
            "127/240, Train_loss: 0.0101 Train_dice: 0.0101\n",
            "Epoch [19/100], Batch [128/240],  Cross Loss: 0.0368\n",
            "128/240, Train_loss: 0.0368 Train_dice: 0.0368\n",
            "Epoch [19/100], Batch [129/240],  Cross Loss: 0.5266\n",
            "129/240, Train_loss: 0.5266 Train_dice: 0.5266\n",
            "Epoch [19/100], Batch [130/240],  Cross Loss: 0.0050\n",
            "130/240, Train_loss: 0.0050 Train_dice: 0.0050\n",
            "Epoch [19/100], Batch [131/240],  Cross Loss: 0.2538\n",
            "131/240, Train_loss: 0.2538 Train_dice: 0.2538\n",
            "Epoch [19/100], Batch [132/240],  Cross Loss: 0.2080\n",
            "132/240, Train_loss: 0.2080 Train_dice: 0.2080\n",
            "Epoch [19/100], Batch [133/240],  Cross Loss: 0.0426\n",
            "133/240, Train_loss: 0.0426 Train_dice: 0.0426\n",
            "Epoch [19/100], Batch [134/240],  Cross Loss: 0.0054\n",
            "134/240, Train_loss: 0.0054 Train_dice: 0.0054\n",
            "Epoch [19/100], Batch [135/240],  Cross Loss: 0.0099\n",
            "135/240, Train_loss: 0.0099 Train_dice: 0.0099\n",
            "Epoch [19/100], Batch [136/240],  Cross Loss: 0.1209\n",
            "136/240, Train_loss: 0.1209 Train_dice: 0.1209\n",
            "Epoch [19/100], Batch [137/240],  Cross Loss: 0.0067\n",
            "137/240, Train_loss: 0.0067 Train_dice: 0.0067\n",
            "Epoch [19/100], Batch [138/240],  Cross Loss: 0.6207\n",
            "138/240, Train_loss: 0.6207 Train_dice: 0.6207\n",
            "Epoch [19/100], Batch [139/240],  Cross Loss: 0.1073\n",
            "139/240, Train_loss: 0.1073 Train_dice: 0.1073\n",
            "Epoch [19/100], Batch [140/240],  Cross Loss: 0.0299\n",
            "140/240, Train_loss: 0.0299 Train_dice: 0.0299\n",
            "Epoch [19/100], Batch [141/240],  Cross Loss: 0.0145\n",
            "141/240, Train_loss: 0.0145 Train_dice: 0.0145\n",
            "Epoch [19/100], Batch [142/240],  Cross Loss: 0.0322\n",
            "142/240, Train_loss: 0.0322 Train_dice: 0.0322\n",
            "Epoch [19/100], Batch [143/240],  Cross Loss: 0.3241\n",
            "143/240, Train_loss: 0.3241 Train_dice: 0.3241\n",
            "Epoch [19/100], Batch [144/240],  Cross Loss: 0.0891\n",
            "144/240, Train_loss: 0.0891 Train_dice: 0.0891\n",
            "Epoch [19/100], Batch [145/240],  Cross Loss: 0.6151\n",
            "145/240, Train_loss: 0.6151 Train_dice: 0.6151\n",
            "Epoch [19/100], Batch [146/240],  Cross Loss: 0.0008\n",
            "146/240, Train_loss: 0.0008 Train_dice: 0.0008\n",
            "Epoch [19/100], Batch [147/240],  Cross Loss: 0.0025\n",
            "147/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [19/100], Batch [148/240],  Cross Loss: 0.6373\n",
            "148/240, Train_loss: 0.6373 Train_dice: 0.6373\n",
            "Epoch [19/100], Batch [149/240],  Cross Loss: 0.0502\n",
            "149/240, Train_loss: 0.0502 Train_dice: 0.0502\n",
            "Epoch [19/100], Batch [150/240],  Cross Loss: 0.1477\n",
            "150/240, Train_loss: 0.1477 Train_dice: 0.1477\n",
            "Epoch [19/100], Batch [151/240],  Cross Loss: 0.2461\n",
            "151/240, Train_loss: 0.2461 Train_dice: 0.2461\n",
            "Epoch [19/100], Batch [152/240],  Cross Loss: 0.1503\n",
            "152/240, Train_loss: 0.1503 Train_dice: 0.1503\n",
            "Epoch [19/100], Batch [153/240],  Cross Loss: 0.4198\n",
            "153/240, Train_loss: 0.4198 Train_dice: 0.4198\n",
            "Epoch [19/100], Batch [154/240],  Cross Loss: 0.2913\n",
            "154/240, Train_loss: 0.2913 Train_dice: 0.2913\n",
            "Epoch [19/100], Batch [155/240],  Cross Loss: 0.1334\n",
            "155/240, Train_loss: 0.1334 Train_dice: 0.1334\n",
            "Epoch [19/100], Batch [156/240],  Cross Loss: 0.0040\n",
            "156/240, Train_loss: 0.0040 Train_dice: 0.0040\n",
            "Epoch [19/100], Batch [157/240],  Cross Loss: 0.0014\n",
            "157/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [19/100], Batch [158/240],  Cross Loss: 0.0107\n",
            "158/240, Train_loss: 0.0107 Train_dice: 0.0107\n",
            "Epoch [19/100], Batch [159/240],  Cross Loss: 0.0019\n",
            "159/240, Train_loss: 0.0019 Train_dice: 0.0019\n",
            "Epoch [19/100], Batch [160/240],  Cross Loss: 0.2802\n",
            "160/240, Train_loss: 0.2802 Train_dice: 0.2802\n",
            "Epoch [19/100], Batch [161/240],  Cross Loss: 0.0026\n",
            "161/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [19/100], Batch [162/240],  Cross Loss: 0.1088\n",
            "162/240, Train_loss: 0.1088 Train_dice: 0.1088\n",
            "Epoch [19/100], Batch [163/240],  Cross Loss: 0.0012\n",
            "163/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [19/100], Batch [164/240],  Cross Loss: 0.1474\n",
            "164/240, Train_loss: 0.1474 Train_dice: 0.1474\n",
            "Epoch [19/100], Batch [165/240],  Cross Loss: 0.0038\n",
            "165/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [19/100], Batch [166/240],  Cross Loss: 0.0084\n",
            "166/240, Train_loss: 0.0084 Train_dice: 0.0084\n",
            "Epoch [19/100], Batch [167/240],  Cross Loss: 0.0037\n",
            "167/240, Train_loss: 0.0037 Train_dice: 0.0037\n",
            "Epoch [19/100], Batch [168/240],  Cross Loss: 0.4539\n",
            "168/240, Train_loss: 0.4539 Train_dice: 0.4539\n",
            "Epoch [19/100], Batch [169/240],  Cross Loss: 0.0362\n",
            "169/240, Train_loss: 0.0362 Train_dice: 0.0362\n",
            "Epoch [19/100], Batch [170/240],  Cross Loss: 0.0093\n",
            "170/240, Train_loss: 0.0093 Train_dice: 0.0093\n",
            "Epoch [19/100], Batch [171/240],  Cross Loss: 0.0172\n",
            "171/240, Train_loss: 0.0172 Train_dice: 0.0172\n",
            "Epoch [19/100], Batch [172/240],  Cross Loss: 0.0710\n",
            "172/240, Train_loss: 0.0710 Train_dice: 0.0710\n",
            "Epoch [19/100], Batch [173/240],  Cross Loss: 0.0885\n",
            "173/240, Train_loss: 0.0885 Train_dice: 0.0885\n",
            "Epoch [19/100], Batch [174/240],  Cross Loss: 0.1533\n",
            "174/240, Train_loss: 0.1533 Train_dice: 0.1533\n",
            "Epoch [19/100], Batch [175/240],  Cross Loss: 0.0022\n",
            "175/240, Train_loss: 0.0022 Train_dice: 0.0022\n",
            "Epoch [19/100], Batch [176/240],  Cross Loss: 0.0217\n",
            "176/240, Train_loss: 0.0217 Train_dice: 0.0217\n",
            "Epoch [19/100], Batch [177/240],  Cross Loss: 0.1224\n",
            "177/240, Train_loss: 0.1224 Train_dice: 0.1224\n",
            "Epoch [19/100], Batch [178/240],  Cross Loss: 0.4529\n",
            "178/240, Train_loss: 0.4529 Train_dice: 0.4529\n",
            "Epoch [19/100], Batch [179/240],  Cross Loss: 0.0052\n",
            "179/240, Train_loss: 0.0052 Train_dice: 0.0052\n",
            "Epoch [19/100], Batch [180/240],  Cross Loss: 0.1390\n",
            "180/240, Train_loss: 0.1390 Train_dice: 0.1390\n",
            "Epoch [19/100], Batch [181/240],  Cross Loss: 0.0026\n",
            "181/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [19/100], Batch [182/240],  Cross Loss: 0.0176\n",
            "182/240, Train_loss: 0.0176 Train_dice: 0.0176\n",
            "Epoch [19/100], Batch [183/240],  Cross Loss: 0.0006\n",
            "183/240, Train_loss: 0.0006 Train_dice: 0.0006\n",
            "Epoch [19/100], Batch [184/240],  Cross Loss: 0.0850\n",
            "184/240, Train_loss: 0.0850 Train_dice: 0.0850\n",
            "Epoch [19/100], Batch [185/240],  Cross Loss: 0.0851\n",
            "185/240, Train_loss: 0.0851 Train_dice: 0.0851\n",
            "Epoch [19/100], Batch [186/240],  Cross Loss: 0.3425\n",
            "186/240, Train_loss: 0.3425 Train_dice: 0.3425\n",
            "Epoch [19/100], Batch [187/240],  Cross Loss: 0.4247\n",
            "187/240, Train_loss: 0.4247 Train_dice: 0.4247\n",
            "Epoch [19/100], Batch [188/240],  Cross Loss: 0.0674\n",
            "188/240, Train_loss: 0.0674 Train_dice: 0.0674\n",
            "Epoch [19/100], Batch [189/240],  Cross Loss: 0.1036\n",
            "189/240, Train_loss: 0.1036 Train_dice: 0.1036\n",
            "Epoch [19/100], Batch [190/240],  Cross Loss: 0.1560\n",
            "190/240, Train_loss: 0.1560 Train_dice: 0.1560\n",
            "Epoch [19/100], Batch [191/240],  Cross Loss: 0.0094\n",
            "191/240, Train_loss: 0.0094 Train_dice: 0.0094\n",
            "Epoch [19/100], Batch [192/240],  Cross Loss: 0.0130\n",
            "192/240, Train_loss: 0.0130 Train_dice: 0.0130\n",
            "Epoch [19/100], Batch [193/240],  Cross Loss: 0.5747\n",
            "193/240, Train_loss: 0.5747 Train_dice: 0.5747\n",
            "Epoch [19/100], Batch [194/240],  Cross Loss: 0.0094\n",
            "194/240, Train_loss: 0.0094 Train_dice: 0.0094\n",
            "Epoch [19/100], Batch [195/240],  Cross Loss: 0.7229\n",
            "195/240, Train_loss: 0.7229 Train_dice: 0.7229\n",
            "Epoch [19/100], Batch [196/240],  Cross Loss: 0.2077\n",
            "196/240, Train_loss: 0.2077 Train_dice: 0.2077\n",
            "Epoch [19/100], Batch [197/240],  Cross Loss: 0.0969\n",
            "197/240, Train_loss: 0.0969 Train_dice: 0.0969\n",
            "Epoch [19/100], Batch [198/240],  Cross Loss: 0.0278\n",
            "198/240, Train_loss: 0.0278 Train_dice: 0.0278\n",
            "Epoch [19/100], Batch [199/240],  Cross Loss: 0.3996\n",
            "199/240, Train_loss: 0.3996 Train_dice: 0.3996\n",
            "Epoch [19/100], Batch [200/240],  Cross Loss: 0.1324\n",
            "200/240, Train_loss: 0.1324 Train_dice: 0.1324\n",
            "Epoch [19/100], Batch [201/240],  Cross Loss: 0.1899\n",
            "201/240, Train_loss: 0.1899 Train_dice: 0.1899\n",
            "Epoch [19/100], Batch [202/240],  Cross Loss: 0.0057\n",
            "202/240, Train_loss: 0.0057 Train_dice: 0.0057\n",
            "Epoch [19/100], Batch [203/240],  Cross Loss: 0.0392\n",
            "203/240, Train_loss: 0.0392 Train_dice: 0.0392\n",
            "Epoch [19/100], Batch [204/240],  Cross Loss: 0.3350\n",
            "204/240, Train_loss: 0.3350 Train_dice: 0.3350\n",
            "Epoch [19/100], Batch [205/240],  Cross Loss: 0.0385\n",
            "205/240, Train_loss: 0.0385 Train_dice: 0.0385\n",
            "Epoch [19/100], Batch [206/240],  Cross Loss: 0.0441\n",
            "206/240, Train_loss: 0.0441 Train_dice: 0.0441\n",
            "Epoch [19/100], Batch [207/240],  Cross Loss: 0.1875\n",
            "207/240, Train_loss: 0.1875 Train_dice: 0.1875\n",
            "Epoch [19/100], Batch [208/240],  Cross Loss: 0.3144\n",
            "208/240, Train_loss: 0.3144 Train_dice: 0.3144\n",
            "Epoch [19/100], Batch [209/240],  Cross Loss: 0.0099\n",
            "209/240, Train_loss: 0.0099 Train_dice: 0.0099\n",
            "Epoch [19/100], Batch [210/240],  Cross Loss: 0.0142\n",
            "210/240, Train_loss: 0.0142 Train_dice: 0.0142\n",
            "Epoch [19/100], Batch [211/240],  Cross Loss: 0.3602\n",
            "211/240, Train_loss: 0.3602 Train_dice: 0.3602\n",
            "Epoch [19/100], Batch [212/240],  Cross Loss: 0.0674\n",
            "212/240, Train_loss: 0.0674 Train_dice: 0.0674\n",
            "Epoch [19/100], Batch [213/240],  Cross Loss: 0.0166\n",
            "213/240, Train_loss: 0.0166 Train_dice: 0.0166\n",
            "Epoch [19/100], Batch [214/240],  Cross Loss: 0.1800\n",
            "214/240, Train_loss: 0.1800 Train_dice: 0.1800\n",
            "Epoch [19/100], Batch [215/240],  Cross Loss: 0.0029\n",
            "215/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [19/100], Batch [216/240],  Cross Loss: 0.0012\n",
            "216/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [19/100], Batch [217/240],  Cross Loss: 0.0876\n",
            "217/240, Train_loss: 0.0876 Train_dice: 0.0876\n",
            "Epoch [19/100], Batch [218/240],  Cross Loss: 0.5302\n",
            "218/240, Train_loss: 0.5302 Train_dice: 0.5302\n",
            "Epoch [19/100], Batch [219/240],  Cross Loss: 0.4278\n",
            "219/240, Train_loss: 0.4278 Train_dice: 0.4278\n",
            "Epoch [19/100], Batch [220/240],  Cross Loss: 0.5725\n",
            "220/240, Train_loss: 0.5725 Train_dice: 0.5725\n",
            "Epoch [19/100], Batch [221/240],  Cross Loss: 0.1776\n",
            "221/240, Train_loss: 0.1776 Train_dice: 0.1776\n",
            "Epoch [19/100], Batch [222/240],  Cross Loss: 0.1570\n",
            "222/240, Train_loss: 0.1570 Train_dice: 0.1570\n",
            "Epoch [19/100], Batch [223/240],  Cross Loss: 0.0915\n",
            "223/240, Train_loss: 0.0915 Train_dice: 0.0915\n",
            "Epoch [19/100], Batch [224/240],  Cross Loss: 0.0104\n",
            "224/240, Train_loss: 0.0104 Train_dice: 0.0104\n",
            "Epoch [19/100], Batch [225/240],  Cross Loss: 0.0115\n",
            "225/240, Train_loss: 0.0115 Train_dice: 0.0115\n",
            "Epoch [19/100], Batch [226/240],  Cross Loss: 0.2299\n",
            "226/240, Train_loss: 0.2299 Train_dice: 0.2299\n",
            "Epoch [19/100], Batch [227/240],  Cross Loss: 0.0164\n",
            "227/240, Train_loss: 0.0164 Train_dice: 0.0164\n",
            "Epoch [19/100], Batch [228/240],  Cross Loss: 0.0615\n",
            "228/240, Train_loss: 0.0615 Train_dice: 0.0615\n",
            "Epoch [19/100], Batch [229/240],  Cross Loss: 0.4109\n",
            "229/240, Train_loss: 0.4109 Train_dice: 0.4109\n",
            "Epoch [19/100], Batch [230/240],  Cross Loss: 0.3528\n",
            "230/240, Train_loss: 0.3528 Train_dice: 0.3528\n",
            "Epoch [19/100], Batch [231/240],  Cross Loss: 0.0599\n",
            "231/240, Train_loss: 0.0599 Train_dice: 0.0599\n",
            "Epoch [19/100], Batch [232/240],  Cross Loss: 0.2605\n",
            "232/240, Train_loss: 0.2605 Train_dice: 0.2605\n",
            "Epoch [19/100], Batch [233/240],  Cross Loss: 0.1603\n",
            "233/240, Train_loss: 0.1603 Train_dice: 0.1603\n",
            "Epoch [19/100], Batch [234/240],  Cross Loss: 0.0178\n",
            "234/240, Train_loss: 0.0178 Train_dice: 0.0178\n",
            "Epoch [19/100], Batch [235/240],  Cross Loss: 0.0010\n",
            "235/240, Train_loss: 0.0010 Train_dice: 0.0010\n",
            "Epoch [19/100], Batch [236/240],  Cross Loss: 0.0021\n",
            "236/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [19/100], Batch [237/240],  Cross Loss: 0.0009\n",
            "237/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [19/100], Batch [238/240],  Cross Loss: 0.0999\n",
            "238/240, Train_loss: 0.0999 Train_dice: 0.0999\n",
            "Epoch [19/100], Batch [239/240],  Cross Loss: 0.2016\n",
            "239/240, Train_loss: 0.2016 Train_dice: 0.2016\n",
            "Epoch [19/100], Batch [240/240],  Cross Loss: 0.0992\n",
            "240/240, Train_loss: 0.0992 Train_dice: 0.0992\n",
            "--------------------\n",
            "Epoch_loss: 0.1343\n",
            "Epoch_metric: tensor(0.1343, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 20/100\n",
            "Epoch [20/100], Batch [1/240],  Cross Loss: 0.0047\n",
            "1/240, Train_loss: 0.0047 Train_dice: 0.0047\n",
            "Epoch [20/100], Batch [2/240],  Cross Loss: 0.0503\n",
            "2/240, Train_loss: 0.0503 Train_dice: 0.0503\n",
            "Epoch [20/100], Batch [3/240],  Cross Loss: 0.0055\n",
            "3/240, Train_loss: 0.0055 Train_dice: 0.0055\n",
            "Epoch [20/100], Batch [4/240],  Cross Loss: 0.1225\n",
            "4/240, Train_loss: 0.1225 Train_dice: 0.1225\n",
            "Epoch [20/100], Batch [5/240],  Cross Loss: 0.0958\n",
            "5/240, Train_loss: 0.0958 Train_dice: 0.0958\n",
            "Epoch [20/100], Batch [6/240],  Cross Loss: 0.0746\n",
            "6/240, Train_loss: 0.0746 Train_dice: 0.0746\n",
            "Epoch [20/100], Batch [7/240],  Cross Loss: 0.0014\n",
            "7/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [20/100], Batch [8/240],  Cross Loss: 0.0890\n",
            "8/240, Train_loss: 0.0890 Train_dice: 0.0890\n",
            "Epoch [20/100], Batch [9/240],  Cross Loss: 0.0494\n",
            "9/240, Train_loss: 0.0494 Train_dice: 0.0494\n",
            "Epoch [20/100], Batch [10/240],  Cross Loss: 0.0166\n",
            "10/240, Train_loss: 0.0166 Train_dice: 0.0166\n",
            "Epoch [20/100], Batch [11/240],  Cross Loss: 0.1441\n",
            "11/240, Train_loss: 0.1441 Train_dice: 0.1441\n",
            "Epoch [20/100], Batch [12/240],  Cross Loss: 0.0244\n",
            "12/240, Train_loss: 0.0244 Train_dice: 0.0244\n",
            "Epoch [20/100], Batch [13/240],  Cross Loss: 0.0843\n",
            "13/240, Train_loss: 0.0843 Train_dice: 0.0843\n",
            "Epoch [20/100], Batch [14/240],  Cross Loss: 0.0253\n",
            "14/240, Train_loss: 0.0253 Train_dice: 0.0253\n",
            "Epoch [20/100], Batch [15/240],  Cross Loss: 0.1080\n",
            "15/240, Train_loss: 0.1080 Train_dice: 0.1080\n",
            "Epoch [20/100], Batch [16/240],  Cross Loss: 0.0100\n",
            "16/240, Train_loss: 0.0100 Train_dice: 0.0100\n",
            "Epoch [20/100], Batch [17/240],  Cross Loss: 0.0050\n",
            "17/240, Train_loss: 0.0050 Train_dice: 0.0050\n",
            "Epoch [20/100], Batch [18/240],  Cross Loss: 0.0062\n",
            "18/240, Train_loss: 0.0062 Train_dice: 0.0062\n",
            "Epoch [20/100], Batch [19/240],  Cross Loss: 0.0303\n",
            "19/240, Train_loss: 0.0303 Train_dice: 0.0303\n",
            "Epoch [20/100], Batch [20/240],  Cross Loss: 0.0139\n",
            "20/240, Train_loss: 0.0139 Train_dice: 0.0139\n",
            "Epoch [20/100], Batch [21/240],  Cross Loss: 0.0009\n",
            "21/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [20/100], Batch [22/240],  Cross Loss: 0.0033\n",
            "22/240, Train_loss: 0.0033 Train_dice: 0.0033\n",
            "Epoch [20/100], Batch [23/240],  Cross Loss: 0.6138\n",
            "23/240, Train_loss: 0.6138 Train_dice: 0.6138\n",
            "Epoch [20/100], Batch [24/240],  Cross Loss: 0.0013\n",
            "24/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [20/100], Batch [25/240],  Cross Loss: 0.0036\n",
            "25/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "Epoch [20/100], Batch [26/240],  Cross Loss: 0.3790\n",
            "26/240, Train_loss: 0.3790 Train_dice: 0.3790\n",
            "Epoch [20/100], Batch [27/240],  Cross Loss: 0.0086\n",
            "27/240, Train_loss: 0.0086 Train_dice: 0.0086\n",
            "Epoch [20/100], Batch [28/240],  Cross Loss: 0.0024\n",
            "28/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [20/100], Batch [29/240],  Cross Loss: 0.0718\n",
            "29/240, Train_loss: 0.0718 Train_dice: 0.0718\n",
            "Epoch [20/100], Batch [30/240],  Cross Loss: 0.6748\n",
            "30/240, Train_loss: 0.6748 Train_dice: 0.6748\n",
            "Epoch [20/100], Batch [31/240],  Cross Loss: 0.1047\n",
            "31/240, Train_loss: 0.1047 Train_dice: 0.1047\n",
            "Epoch [20/100], Batch [32/240],  Cross Loss: 0.1857\n",
            "32/240, Train_loss: 0.1857 Train_dice: 0.1857\n",
            "Epoch [20/100], Batch [33/240],  Cross Loss: 0.3002\n",
            "33/240, Train_loss: 0.3002 Train_dice: 0.3002\n",
            "Epoch [20/100], Batch [34/240],  Cross Loss: 0.0300\n",
            "34/240, Train_loss: 0.0300 Train_dice: 0.0300\n",
            "Epoch [20/100], Batch [35/240],  Cross Loss: 0.0017\n",
            "35/240, Train_loss: 0.0017 Train_dice: 0.0017\n",
            "Epoch [20/100], Batch [36/240],  Cross Loss: 0.5281\n",
            "36/240, Train_loss: 0.5281 Train_dice: 0.5281\n",
            "Epoch [20/100], Batch [37/240],  Cross Loss: 0.0016\n",
            "37/240, Train_loss: 0.0016 Train_dice: 0.0016\n",
            "Epoch [20/100], Batch [38/240],  Cross Loss: 0.0203\n",
            "38/240, Train_loss: 0.0203 Train_dice: 0.0203\n",
            "Epoch [20/100], Batch [39/240],  Cross Loss: 0.0387\n",
            "39/240, Train_loss: 0.0387 Train_dice: 0.0387\n",
            "Epoch [20/100], Batch [40/240],  Cross Loss: 0.0469\n",
            "40/240, Train_loss: 0.0469 Train_dice: 0.0469\n",
            "Epoch [20/100], Batch [41/240],  Cross Loss: 0.2073\n",
            "41/240, Train_loss: 0.2073 Train_dice: 0.2073\n",
            "Epoch [20/100], Batch [42/240],  Cross Loss: 0.0071\n",
            "42/240, Train_loss: 0.0071 Train_dice: 0.0071\n",
            "Epoch [20/100], Batch [43/240],  Cross Loss: 0.0072\n",
            "43/240, Train_loss: 0.0072 Train_dice: 0.0072\n",
            "Epoch [20/100], Batch [44/240],  Cross Loss: 0.3059\n",
            "44/240, Train_loss: 0.3059 Train_dice: 0.3059\n",
            "Epoch [20/100], Batch [45/240],  Cross Loss: 0.0135\n",
            "45/240, Train_loss: 0.0135 Train_dice: 0.0135\n",
            "Epoch [20/100], Batch [46/240],  Cross Loss: 0.1905\n",
            "46/240, Train_loss: 0.1905 Train_dice: 0.1905\n",
            "Epoch [20/100], Batch [47/240],  Cross Loss: 0.0932\n",
            "47/240, Train_loss: 0.0932 Train_dice: 0.0932\n",
            "Epoch [20/100], Batch [48/240],  Cross Loss: 0.0024\n",
            "48/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [20/100], Batch [49/240],  Cross Loss: 0.0066\n",
            "49/240, Train_loss: 0.0066 Train_dice: 0.0066\n",
            "Epoch [20/100], Batch [50/240],  Cross Loss: 0.1249\n",
            "50/240, Train_loss: 0.1249 Train_dice: 0.1249\n",
            "Epoch [20/100], Batch [51/240],  Cross Loss: 0.4384\n",
            "51/240, Train_loss: 0.4384 Train_dice: 0.4384\n",
            "Epoch [20/100], Batch [52/240],  Cross Loss: 0.0026\n",
            "52/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [20/100], Batch [53/240],  Cross Loss: 0.1636\n",
            "53/240, Train_loss: 0.1636 Train_dice: 0.1636\n",
            "Epoch [20/100], Batch [54/240],  Cross Loss: 0.0022\n",
            "54/240, Train_loss: 0.0022 Train_dice: 0.0022\n",
            "Epoch [20/100], Batch [55/240],  Cross Loss: 0.1063\n",
            "55/240, Train_loss: 0.1063 Train_dice: 0.1063\n",
            "Epoch [20/100], Batch [56/240],  Cross Loss: 0.0494\n",
            "56/240, Train_loss: 0.0494 Train_dice: 0.0494\n",
            "Epoch [20/100], Batch [57/240],  Cross Loss: 0.0116\n",
            "57/240, Train_loss: 0.0116 Train_dice: 0.0116\n",
            "Epoch [20/100], Batch [58/240],  Cross Loss: 0.0425\n",
            "58/240, Train_loss: 0.0425 Train_dice: 0.0425\n",
            "Epoch [20/100], Batch [59/240],  Cross Loss: 0.1030\n",
            "59/240, Train_loss: 0.1030 Train_dice: 0.1030\n",
            "Epoch [20/100], Batch [60/240],  Cross Loss: 0.5827\n",
            "60/240, Train_loss: 0.5827 Train_dice: 0.5827\n",
            "Epoch [20/100], Batch [61/240],  Cross Loss: 0.0246\n",
            "61/240, Train_loss: 0.0246 Train_dice: 0.0246\n",
            "Epoch [20/100], Batch [62/240],  Cross Loss: 0.4461\n",
            "62/240, Train_loss: 0.4461 Train_dice: 0.4461\n",
            "Epoch [20/100], Batch [63/240],  Cross Loss: 0.0034\n",
            "63/240, Train_loss: 0.0034 Train_dice: 0.0034\n",
            "Epoch [20/100], Batch [64/240],  Cross Loss: 0.0008\n",
            "64/240, Train_loss: 0.0008 Train_dice: 0.0008\n",
            "Epoch [20/100], Batch [65/240],  Cross Loss: 0.0168\n",
            "65/240, Train_loss: 0.0168 Train_dice: 0.0168\n",
            "Epoch [20/100], Batch [66/240],  Cross Loss: 0.5222\n",
            "66/240, Train_loss: 0.5222 Train_dice: 0.5222\n",
            "Epoch [20/100], Batch [67/240],  Cross Loss: 0.0544\n",
            "67/240, Train_loss: 0.0544 Train_dice: 0.0544\n",
            "Epoch [20/100], Batch [68/240],  Cross Loss: 0.2287\n",
            "68/240, Train_loss: 0.2287 Train_dice: 0.2287\n",
            "Epoch [20/100], Batch [69/240],  Cross Loss: 0.0391\n",
            "69/240, Train_loss: 0.0391 Train_dice: 0.0391\n",
            "Epoch [20/100], Batch [70/240],  Cross Loss: 0.0024\n",
            "70/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [20/100], Batch [71/240],  Cross Loss: 0.5634\n",
            "71/240, Train_loss: 0.5634 Train_dice: 0.5634\n",
            "Epoch [20/100], Batch [72/240],  Cross Loss: 0.0900\n",
            "72/240, Train_loss: 0.0900 Train_dice: 0.0900\n",
            "Epoch [20/100], Batch [73/240],  Cross Loss: 0.0079\n",
            "73/240, Train_loss: 0.0079 Train_dice: 0.0079\n",
            "Epoch [20/100], Batch [74/240],  Cross Loss: 0.1042\n",
            "74/240, Train_loss: 0.1042 Train_dice: 0.1042\n",
            "Epoch [20/100], Batch [75/240],  Cross Loss: 0.0065\n",
            "75/240, Train_loss: 0.0065 Train_dice: 0.0065\n",
            "Epoch [20/100], Batch [76/240],  Cross Loss: 0.0017\n",
            "76/240, Train_loss: 0.0017 Train_dice: 0.0017\n",
            "Epoch [20/100], Batch [77/240],  Cross Loss: 0.3821\n",
            "77/240, Train_loss: 0.3821 Train_dice: 0.3821\n",
            "Epoch [20/100], Batch [78/240],  Cross Loss: 0.0010\n",
            "78/240, Train_loss: 0.0010 Train_dice: 0.0010\n",
            "Epoch [20/100], Batch [79/240],  Cross Loss: 0.0275\n",
            "79/240, Train_loss: 0.0275 Train_dice: 0.0275\n",
            "Epoch [20/100], Batch [80/240],  Cross Loss: 0.0293\n",
            "80/240, Train_loss: 0.0293 Train_dice: 0.0293\n",
            "Epoch [20/100], Batch [81/240],  Cross Loss: 0.0267\n",
            "81/240, Train_loss: 0.0267 Train_dice: 0.0267\n",
            "Epoch [20/100], Batch [82/240],  Cross Loss: 0.0388\n",
            "82/240, Train_loss: 0.0388 Train_dice: 0.0388\n",
            "Epoch [20/100], Batch [83/240],  Cross Loss: 0.5907\n",
            "83/240, Train_loss: 0.5907 Train_dice: 0.5907\n",
            "Epoch [20/100], Batch [84/240],  Cross Loss: 0.5245\n",
            "84/240, Train_loss: 0.5245 Train_dice: 0.5245\n",
            "Epoch [20/100], Batch [85/240],  Cross Loss: 0.0013\n",
            "85/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [20/100], Batch [86/240],  Cross Loss: 0.0022\n",
            "86/240, Train_loss: 0.0022 Train_dice: 0.0022\n",
            "Epoch [20/100], Batch [87/240],  Cross Loss: 0.0064\n",
            "87/240, Train_loss: 0.0064 Train_dice: 0.0064\n",
            "Epoch [20/100], Batch [88/240],  Cross Loss: 0.0065\n",
            "88/240, Train_loss: 0.0065 Train_dice: 0.0065\n",
            "Epoch [20/100], Batch [89/240],  Cross Loss: 0.0044\n",
            "89/240, Train_loss: 0.0044 Train_dice: 0.0044\n",
            "Epoch [20/100], Batch [90/240],  Cross Loss: 0.1067\n",
            "90/240, Train_loss: 0.1067 Train_dice: 0.1067\n",
            "Epoch [20/100], Batch [91/240],  Cross Loss: 0.0469\n",
            "91/240, Train_loss: 0.0469 Train_dice: 0.0469\n",
            "Epoch [20/100], Batch [92/240],  Cross Loss: 0.0018\n",
            "92/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [20/100], Batch [93/240],  Cross Loss: 0.0222\n",
            "93/240, Train_loss: 0.0222 Train_dice: 0.0222\n",
            "Epoch [20/100], Batch [94/240],  Cross Loss: 0.0057\n",
            "94/240, Train_loss: 0.0057 Train_dice: 0.0057\n",
            "Epoch [20/100], Batch [95/240],  Cross Loss: 0.0264\n",
            "95/240, Train_loss: 0.0264 Train_dice: 0.0264\n",
            "Epoch [20/100], Batch [96/240],  Cross Loss: 0.0331\n",
            "96/240, Train_loss: 0.0331 Train_dice: 0.0331\n",
            "Epoch [20/100], Batch [97/240],  Cross Loss: 0.1445\n",
            "97/240, Train_loss: 0.1445 Train_dice: 0.1445\n",
            "Epoch [20/100], Batch [98/240],  Cross Loss: 0.0124\n",
            "98/240, Train_loss: 0.0124 Train_dice: 0.0124\n",
            "Epoch [20/100], Batch [99/240],  Cross Loss: 0.0445\n",
            "99/240, Train_loss: 0.0445 Train_dice: 0.0445\n",
            "Epoch [20/100], Batch [100/240],  Cross Loss: 0.4336\n",
            "100/240, Train_loss: 0.4336 Train_dice: 0.4336\n",
            "Epoch [20/100], Batch [101/240],  Cross Loss: 0.0283\n",
            "101/240, Train_loss: 0.0283 Train_dice: 0.0283\n",
            "Epoch [20/100], Batch [102/240],  Cross Loss: 0.5609\n",
            "102/240, Train_loss: 0.5609 Train_dice: 0.5609\n",
            "Epoch [20/100], Batch [103/240],  Cross Loss: 0.0057\n",
            "103/240, Train_loss: 0.0057 Train_dice: 0.0057\n",
            "Epoch [20/100], Batch [104/240],  Cross Loss: 0.1895\n",
            "104/240, Train_loss: 0.1895 Train_dice: 0.1895\n",
            "Epoch [20/100], Batch [105/240],  Cross Loss: 0.0150\n",
            "105/240, Train_loss: 0.0150 Train_dice: 0.0150\n",
            "Epoch [20/100], Batch [106/240],  Cross Loss: 0.1202\n",
            "106/240, Train_loss: 0.1202 Train_dice: 0.1202\n",
            "Epoch [20/100], Batch [107/240],  Cross Loss: 0.0139\n",
            "107/240, Train_loss: 0.0139 Train_dice: 0.0139\n",
            "Epoch [20/100], Batch [108/240],  Cross Loss: 0.0015\n",
            "108/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [20/100], Batch [109/240],  Cross Loss: 0.0182\n",
            "109/240, Train_loss: 0.0182 Train_dice: 0.0182\n",
            "Epoch [20/100], Batch [110/240],  Cross Loss: 0.0055\n",
            "110/240, Train_loss: 0.0055 Train_dice: 0.0055\n",
            "Epoch [20/100], Batch [111/240],  Cross Loss: 0.0396\n",
            "111/240, Train_loss: 0.0396 Train_dice: 0.0396\n",
            "Epoch [20/100], Batch [112/240],  Cross Loss: 0.0194\n",
            "112/240, Train_loss: 0.0194 Train_dice: 0.0194\n",
            "Epoch [20/100], Batch [113/240],  Cross Loss: 0.0493\n",
            "113/240, Train_loss: 0.0493 Train_dice: 0.0493\n",
            "Epoch [20/100], Batch [114/240],  Cross Loss: 0.3301\n",
            "114/240, Train_loss: 0.3301 Train_dice: 0.3301\n",
            "Epoch [20/100], Batch [115/240],  Cross Loss: 0.0176\n",
            "115/240, Train_loss: 0.0176 Train_dice: 0.0176\n",
            "Epoch [20/100], Batch [116/240],  Cross Loss: 0.2282\n",
            "116/240, Train_loss: 0.2282 Train_dice: 0.2282\n",
            "Epoch [20/100], Batch [117/240],  Cross Loss: 0.3690\n",
            "117/240, Train_loss: 0.3690 Train_dice: 0.3690\n",
            "Epoch [20/100], Batch [118/240],  Cross Loss: 0.0418\n",
            "118/240, Train_loss: 0.0418 Train_dice: 0.0418\n",
            "Epoch [20/100], Batch [119/240],  Cross Loss: 0.0187\n",
            "119/240, Train_loss: 0.0187 Train_dice: 0.0187\n",
            "Epoch [20/100], Batch [120/240],  Cross Loss: 0.0030\n",
            "120/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [20/100], Batch [121/240],  Cross Loss: 0.1324\n",
            "121/240, Train_loss: 0.1324 Train_dice: 0.1324\n",
            "Epoch [20/100], Batch [122/240],  Cross Loss: 0.1740\n",
            "122/240, Train_loss: 0.1740 Train_dice: 0.1740\n",
            "Epoch [20/100], Batch [123/240],  Cross Loss: 0.5806\n",
            "123/240, Train_loss: 0.5806 Train_dice: 0.5806\n",
            "Epoch [20/100], Batch [124/240],  Cross Loss: 0.1607\n",
            "124/240, Train_loss: 0.1607 Train_dice: 0.1607\n",
            "Epoch [20/100], Batch [125/240],  Cross Loss: 0.0348\n",
            "125/240, Train_loss: 0.0348 Train_dice: 0.0348\n",
            "Epoch [20/100], Batch [126/240],  Cross Loss: 0.0801\n",
            "126/240, Train_loss: 0.0801 Train_dice: 0.0801\n",
            "Epoch [20/100], Batch [127/240],  Cross Loss: 0.2021\n",
            "127/240, Train_loss: 0.2021 Train_dice: 0.2021\n",
            "Epoch [20/100], Batch [128/240],  Cross Loss: 0.0936\n",
            "128/240, Train_loss: 0.0936 Train_dice: 0.0936\n",
            "Epoch [20/100], Batch [129/240],  Cross Loss: 0.3138\n",
            "129/240, Train_loss: 0.3138 Train_dice: 0.3138\n",
            "Epoch [20/100], Batch [130/240],  Cross Loss: 0.0108\n",
            "130/240, Train_loss: 0.0108 Train_dice: 0.0108\n",
            "Epoch [20/100], Batch [131/240],  Cross Loss: 0.0993\n",
            "131/240, Train_loss: 0.0993 Train_dice: 0.0993\n",
            "Epoch [20/100], Batch [132/240],  Cross Loss: 0.0594\n",
            "132/240, Train_loss: 0.0594 Train_dice: 0.0594\n",
            "Epoch [20/100], Batch [133/240],  Cross Loss: 0.1796\n",
            "133/240, Train_loss: 0.1796 Train_dice: 0.1796\n",
            "Epoch [20/100], Batch [134/240],  Cross Loss: 0.0021\n",
            "134/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [20/100], Batch [135/240],  Cross Loss: 0.0042\n",
            "135/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [20/100], Batch [136/240],  Cross Loss: 0.1181\n",
            "136/240, Train_loss: 0.1181 Train_dice: 0.1181\n",
            "Epoch [20/100], Batch [137/240],  Cross Loss: 0.2099\n",
            "137/240, Train_loss: 0.2099 Train_dice: 0.2099\n",
            "Epoch [20/100], Batch [138/240],  Cross Loss: 0.4859\n",
            "138/240, Train_loss: 0.4859 Train_dice: 0.4859\n",
            "Epoch [20/100], Batch [139/240],  Cross Loss: 0.3751\n",
            "139/240, Train_loss: 0.3751 Train_dice: 0.3751\n",
            "Epoch [20/100], Batch [140/240],  Cross Loss: 0.5463\n",
            "140/240, Train_loss: 0.5463 Train_dice: 0.5463\n",
            "Epoch [20/100], Batch [141/240],  Cross Loss: 0.0174\n",
            "141/240, Train_loss: 0.0174 Train_dice: 0.0174\n",
            "Epoch [20/100], Batch [142/240],  Cross Loss: 0.1460\n",
            "142/240, Train_loss: 0.1460 Train_dice: 0.1460\n",
            "Epoch [20/100], Batch [143/240],  Cross Loss: 0.0143\n",
            "143/240, Train_loss: 0.0143 Train_dice: 0.0143\n",
            "Epoch [20/100], Batch [144/240],  Cross Loss: 0.3828\n",
            "144/240, Train_loss: 0.3828 Train_dice: 0.3828\n",
            "Epoch [20/100], Batch [145/240],  Cross Loss: 0.1221\n",
            "145/240, Train_loss: 0.1221 Train_dice: 0.1221\n",
            "Epoch [20/100], Batch [146/240],  Cross Loss: 0.0239\n",
            "146/240, Train_loss: 0.0239 Train_dice: 0.0239\n",
            "Epoch [20/100], Batch [147/240],  Cross Loss: 0.1468\n",
            "147/240, Train_loss: 0.1468 Train_dice: 0.1468\n",
            "Epoch [20/100], Batch [148/240],  Cross Loss: 0.0931\n",
            "148/240, Train_loss: 0.0931 Train_dice: 0.0931\n",
            "Epoch [20/100], Batch [149/240],  Cross Loss: 0.0045\n",
            "149/240, Train_loss: 0.0045 Train_dice: 0.0045\n",
            "Epoch [20/100], Batch [150/240],  Cross Loss: 0.0013\n",
            "150/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [20/100], Batch [151/240],  Cross Loss: 0.0132\n",
            "151/240, Train_loss: 0.0132 Train_dice: 0.0132\n",
            "Epoch [20/100], Batch [152/240],  Cross Loss: 0.0152\n",
            "152/240, Train_loss: 0.0152 Train_dice: 0.0152\n",
            "Epoch [20/100], Batch [153/240],  Cross Loss: 0.3292\n",
            "153/240, Train_loss: 0.3292 Train_dice: 0.3292\n",
            "Epoch [20/100], Batch [154/240],  Cross Loss: 0.2399\n",
            "154/240, Train_loss: 0.2399 Train_dice: 0.2399\n",
            "Epoch [20/100], Batch [155/240],  Cross Loss: 0.0537\n",
            "155/240, Train_loss: 0.0537 Train_dice: 0.0537\n",
            "Epoch [20/100], Batch [156/240],  Cross Loss: 0.0376\n",
            "156/240, Train_loss: 0.0376 Train_dice: 0.0376\n",
            "Epoch [20/100], Batch [157/240],  Cross Loss: 0.1125\n",
            "157/240, Train_loss: 0.1125 Train_dice: 0.1125\n",
            "Epoch [20/100], Batch [158/240],  Cross Loss: 0.0080\n",
            "158/240, Train_loss: 0.0080 Train_dice: 0.0080\n",
            "Epoch [20/100], Batch [159/240],  Cross Loss: 0.4301\n",
            "159/240, Train_loss: 0.4301 Train_dice: 0.4301\n",
            "Epoch [20/100], Batch [160/240],  Cross Loss: 0.0025\n",
            "160/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [20/100], Batch [161/240],  Cross Loss: 0.0008\n",
            "161/240, Train_loss: 0.0008 Train_dice: 0.0008\n",
            "Epoch [20/100], Batch [162/240],  Cross Loss: 0.2423\n",
            "162/240, Train_loss: 0.2423 Train_dice: 0.2423\n",
            "Epoch [20/100], Batch [163/240],  Cross Loss: 0.1694\n",
            "163/240, Train_loss: 0.1694 Train_dice: 0.1694\n",
            "Epoch [20/100], Batch [164/240],  Cross Loss: 0.0326\n",
            "164/240, Train_loss: 0.0326 Train_dice: 0.0326\n",
            "Epoch [20/100], Batch [165/240],  Cross Loss: 0.3124\n",
            "165/240, Train_loss: 0.3124 Train_dice: 0.3124\n",
            "Epoch [20/100], Batch [166/240],  Cross Loss: 0.0345\n",
            "166/240, Train_loss: 0.0345 Train_dice: 0.0345\n",
            "Epoch [20/100], Batch [167/240],  Cross Loss: 0.1080\n",
            "167/240, Train_loss: 0.1080 Train_dice: 0.1080\n",
            "Epoch [20/100], Batch [168/240],  Cross Loss: 0.0053\n",
            "168/240, Train_loss: 0.0053 Train_dice: 0.0053\n",
            "Epoch [20/100], Batch [169/240],  Cross Loss: 0.0223\n",
            "169/240, Train_loss: 0.0223 Train_dice: 0.0223\n",
            "Epoch [20/100], Batch [170/240],  Cross Loss: 0.6722\n",
            "170/240, Train_loss: 0.6722 Train_dice: 0.6722\n",
            "Epoch [20/100], Batch [171/240],  Cross Loss: 0.3885\n",
            "171/240, Train_loss: 0.3885 Train_dice: 0.3885\n",
            "Epoch [20/100], Batch [172/240],  Cross Loss: 0.0075\n",
            "172/240, Train_loss: 0.0075 Train_dice: 0.0075\n",
            "Epoch [20/100], Batch [173/240],  Cross Loss: 0.0089\n",
            "173/240, Train_loss: 0.0089 Train_dice: 0.0089\n",
            "Epoch [20/100], Batch [174/240],  Cross Loss: 0.0011\n",
            "174/240, Train_loss: 0.0011 Train_dice: 0.0011\n",
            "Epoch [20/100], Batch [175/240],  Cross Loss: 0.0016\n",
            "175/240, Train_loss: 0.0016 Train_dice: 0.0016\n",
            "Epoch [20/100], Batch [176/240],  Cross Loss: 0.0091\n",
            "176/240, Train_loss: 0.0091 Train_dice: 0.0091\n",
            "Epoch [20/100], Batch [177/240],  Cross Loss: 0.0906\n",
            "177/240, Train_loss: 0.0906 Train_dice: 0.0906\n",
            "Epoch [20/100], Batch [178/240],  Cross Loss: 0.0020\n",
            "178/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [20/100], Batch [179/240],  Cross Loss: 0.5086\n",
            "179/240, Train_loss: 0.5086 Train_dice: 0.5086\n",
            "Epoch [20/100], Batch [180/240],  Cross Loss: 0.0190\n",
            "180/240, Train_loss: 0.0190 Train_dice: 0.0190\n",
            "Epoch [20/100], Batch [181/240],  Cross Loss: 0.0088\n",
            "181/240, Train_loss: 0.0088 Train_dice: 0.0088\n",
            "Epoch [20/100], Batch [182/240],  Cross Loss: 0.0373\n",
            "182/240, Train_loss: 0.0373 Train_dice: 0.0373\n",
            "Epoch [20/100], Batch [183/240],  Cross Loss: 0.4821\n",
            "183/240, Train_loss: 0.4821 Train_dice: 0.4821\n",
            "Epoch [20/100], Batch [184/240],  Cross Loss: 0.0060\n",
            "184/240, Train_loss: 0.0060 Train_dice: 0.0060\n",
            "Epoch [20/100], Batch [185/240],  Cross Loss: 0.0322\n",
            "185/240, Train_loss: 0.0322 Train_dice: 0.0322\n",
            "Epoch [20/100], Batch [186/240],  Cross Loss: 0.3817\n",
            "186/240, Train_loss: 0.3817 Train_dice: 0.3817\n",
            "Epoch [20/100], Batch [187/240],  Cross Loss: 0.4200\n",
            "187/240, Train_loss: 0.4200 Train_dice: 0.4200\n",
            "Epoch [20/100], Batch [188/240],  Cross Loss: 0.0428\n",
            "188/240, Train_loss: 0.0428 Train_dice: 0.0428\n",
            "Epoch [20/100], Batch [189/240],  Cross Loss: 0.0090\n",
            "189/240, Train_loss: 0.0090 Train_dice: 0.0090\n",
            "Epoch [20/100], Batch [190/240],  Cross Loss: 0.0072\n",
            "190/240, Train_loss: 0.0072 Train_dice: 0.0072\n",
            "Epoch [20/100], Batch [191/240],  Cross Loss: 0.0267\n",
            "191/240, Train_loss: 0.0267 Train_dice: 0.0267\n",
            "Epoch [20/100], Batch [192/240],  Cross Loss: 0.2446\n",
            "192/240, Train_loss: 0.2446 Train_dice: 0.2446\n",
            "Epoch [20/100], Batch [193/240],  Cross Loss: 0.0303\n",
            "193/240, Train_loss: 0.0303 Train_dice: 0.0303\n",
            "Epoch [20/100], Batch [194/240],  Cross Loss: 0.0036\n",
            "194/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "Epoch [20/100], Batch [195/240],  Cross Loss: 0.1151\n",
            "195/240, Train_loss: 0.1151 Train_dice: 0.1151\n",
            "Epoch [20/100], Batch [196/240],  Cross Loss: 0.0014\n",
            "196/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [20/100], Batch [197/240],  Cross Loss: 0.0405\n",
            "197/240, Train_loss: 0.0405 Train_dice: 0.0405\n",
            "Epoch [20/100], Batch [198/240],  Cross Loss: 0.5761\n",
            "198/240, Train_loss: 0.5761 Train_dice: 0.5761\n",
            "Epoch [20/100], Batch [199/240],  Cross Loss: 0.1118\n",
            "199/240, Train_loss: 0.1118 Train_dice: 0.1118\n",
            "Epoch [20/100], Batch [200/240],  Cross Loss: 0.2260\n",
            "200/240, Train_loss: 0.2260 Train_dice: 0.2260\n",
            "Epoch [20/100], Batch [201/240],  Cross Loss: 0.0336\n",
            "201/240, Train_loss: 0.0336 Train_dice: 0.0336\n",
            "Epoch [20/100], Batch [202/240],  Cross Loss: 0.1114\n",
            "202/240, Train_loss: 0.1114 Train_dice: 0.1114\n",
            "Epoch [20/100], Batch [203/240],  Cross Loss: 0.0018\n",
            "203/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [20/100], Batch [204/240],  Cross Loss: 0.2255\n",
            "204/240, Train_loss: 0.2255 Train_dice: 0.2255\n",
            "Epoch [20/100], Batch [205/240],  Cross Loss: 0.0286\n",
            "205/240, Train_loss: 0.0286 Train_dice: 0.0286\n",
            "Epoch [20/100], Batch [206/240],  Cross Loss: 0.0408\n",
            "206/240, Train_loss: 0.0408 Train_dice: 0.0408\n",
            "Epoch [20/100], Batch [207/240],  Cross Loss: 0.0671\n",
            "207/240, Train_loss: 0.0671 Train_dice: 0.0671\n",
            "Epoch [20/100], Batch [208/240],  Cross Loss: 0.1230\n",
            "208/240, Train_loss: 0.1230 Train_dice: 0.1230\n",
            "Epoch [20/100], Batch [209/240],  Cross Loss: 0.1431\n",
            "209/240, Train_loss: 0.1431 Train_dice: 0.1431\n",
            "Epoch [20/100], Batch [210/240],  Cross Loss: 0.2386\n",
            "210/240, Train_loss: 0.2386 Train_dice: 0.2386\n",
            "Epoch [20/100], Batch [211/240],  Cross Loss: 0.2527\n",
            "211/240, Train_loss: 0.2527 Train_dice: 0.2527\n",
            "Epoch [20/100], Batch [212/240],  Cross Loss: 0.5072\n",
            "212/240, Train_loss: 0.5072 Train_dice: 0.5072\n",
            "Epoch [20/100], Batch [213/240],  Cross Loss: 0.0015\n",
            "213/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [20/100], Batch [214/240],  Cross Loss: 0.0523\n",
            "214/240, Train_loss: 0.0523 Train_dice: 0.0523\n",
            "Epoch [20/100], Batch [215/240],  Cross Loss: 0.2583\n",
            "215/240, Train_loss: 0.2583 Train_dice: 0.2583\n",
            "Epoch [20/100], Batch [216/240],  Cross Loss: 0.0185\n",
            "216/240, Train_loss: 0.0185 Train_dice: 0.0185\n",
            "Epoch [20/100], Batch [217/240],  Cross Loss: 0.0135\n",
            "217/240, Train_loss: 0.0135 Train_dice: 0.0135\n",
            "Epoch [20/100], Batch [218/240],  Cross Loss: 0.0042\n",
            "218/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [20/100], Batch [219/240],  Cross Loss: 0.0331\n",
            "219/240, Train_loss: 0.0331 Train_dice: 0.0331\n",
            "Epoch [20/100], Batch [220/240],  Cross Loss: 0.0626\n",
            "220/240, Train_loss: 0.0626 Train_dice: 0.0626\n",
            "Epoch [20/100], Batch [221/240],  Cross Loss: 0.0015\n",
            "221/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [20/100], Batch [222/240],  Cross Loss: 0.0054\n",
            "222/240, Train_loss: 0.0054 Train_dice: 0.0054\n",
            "Epoch [20/100], Batch [223/240],  Cross Loss: 0.0015\n",
            "223/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [20/100], Batch [224/240],  Cross Loss: 0.0353\n",
            "224/240, Train_loss: 0.0353 Train_dice: 0.0353\n",
            "Epoch [20/100], Batch [225/240],  Cross Loss: 0.0472\n",
            "225/240, Train_loss: 0.0472 Train_dice: 0.0472\n",
            "Epoch [20/100], Batch [226/240],  Cross Loss: 0.2010\n",
            "226/240, Train_loss: 0.2010 Train_dice: 0.2010\n",
            "Epoch [20/100], Batch [227/240],  Cross Loss: 0.0116\n",
            "227/240, Train_loss: 0.0116 Train_dice: 0.0116\n",
            "Epoch [20/100], Batch [228/240],  Cross Loss: 0.0276\n",
            "228/240, Train_loss: 0.0276 Train_dice: 0.0276\n",
            "Epoch [20/100], Batch [229/240],  Cross Loss: 0.0581\n",
            "229/240, Train_loss: 0.0581 Train_dice: 0.0581\n",
            "Epoch [20/100], Batch [230/240],  Cross Loss: 0.5624\n",
            "230/240, Train_loss: 0.5624 Train_dice: 0.5624\n",
            "Epoch [20/100], Batch [231/240],  Cross Loss: 0.0110\n",
            "231/240, Train_loss: 0.0110 Train_dice: 0.0110\n",
            "Epoch [20/100], Batch [232/240],  Cross Loss: 0.1603\n",
            "232/240, Train_loss: 0.1603 Train_dice: 0.1603\n",
            "Epoch [20/100], Batch [233/240],  Cross Loss: 0.0858\n",
            "233/240, Train_loss: 0.0858 Train_dice: 0.0858\n",
            "Epoch [20/100], Batch [234/240],  Cross Loss: 0.0035\n",
            "234/240, Train_loss: 0.0035 Train_dice: 0.0035\n",
            "Epoch [20/100], Batch [235/240],  Cross Loss: 0.0264\n",
            "235/240, Train_loss: 0.0264 Train_dice: 0.0264\n",
            "Epoch [20/100], Batch [236/240],  Cross Loss: 0.0020\n",
            "236/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [20/100], Batch [237/240],  Cross Loss: 0.5101\n",
            "237/240, Train_loss: 0.5101 Train_dice: 0.5101\n",
            "Epoch [20/100], Batch [238/240],  Cross Loss: 0.1521\n",
            "238/240, Train_loss: 0.1521 Train_dice: 0.1521\n",
            "Epoch [20/100], Batch [239/240],  Cross Loss: 0.1191\n",
            "239/240, Train_loss: 0.1191 Train_dice: 0.1191\n",
            "Epoch [20/100], Batch [240/240],  Cross Loss: 0.0092\n",
            "240/240, Train_loss: 0.0092 Train_dice: 0.0092\n",
            "--------------------\n",
            "Epoch_loss: 0.1204\n",
            "Epoch_metric: tensor(0.1204, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.4701\n",
            "test_dice_epoch: tensor(0.4701, device='cuda:0')\n",
            "current epoch: 20 current mean dice: tensor(0.4701, device='cuda:0')\n",
            "best mean dice: tensor(0.4701, device='cuda:0') at epoch: 20\n",
            "----------\n",
            "epoch 21/100\n",
            "Epoch [21/100], Batch [1/240],  Cross Loss: 0.0578\n",
            "1/240, Train_loss: 0.0578 Train_dice: 0.0578\n",
            "Epoch [21/100], Batch [2/240],  Cross Loss: 0.0171\n",
            "2/240, Train_loss: 0.0171 Train_dice: 0.0171\n",
            "Epoch [21/100], Batch [3/240],  Cross Loss: 0.0070\n",
            "3/240, Train_loss: 0.0070 Train_dice: 0.0070\n",
            "Epoch [21/100], Batch [4/240],  Cross Loss: 0.0051\n",
            "4/240, Train_loss: 0.0051 Train_dice: 0.0051\n",
            "Epoch [21/100], Batch [5/240],  Cross Loss: 0.0570\n",
            "5/240, Train_loss: 0.0570 Train_dice: 0.0570\n",
            "Epoch [21/100], Batch [6/240],  Cross Loss: 0.0045\n",
            "6/240, Train_loss: 0.0045 Train_dice: 0.0045\n",
            "Epoch [21/100], Batch [7/240],  Cross Loss: 0.0026\n",
            "7/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [21/100], Batch [8/240],  Cross Loss: 0.1123\n",
            "8/240, Train_loss: 0.1123 Train_dice: 0.1123\n",
            "Epoch [21/100], Batch [9/240],  Cross Loss: 0.0015\n",
            "9/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [21/100], Batch [10/240],  Cross Loss: 0.0034\n",
            "10/240, Train_loss: 0.0034 Train_dice: 0.0034\n",
            "Epoch [21/100], Batch [11/240],  Cross Loss: 0.3690\n",
            "11/240, Train_loss: 0.3690 Train_dice: 0.3690\n",
            "Epoch [21/100], Batch [12/240],  Cross Loss: 0.3364\n",
            "12/240, Train_loss: 0.3364 Train_dice: 0.3364\n",
            "Epoch [21/100], Batch [13/240],  Cross Loss: 0.0042\n",
            "13/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [21/100], Batch [14/240],  Cross Loss: 0.0273\n",
            "14/240, Train_loss: 0.0273 Train_dice: 0.0273\n",
            "Epoch [21/100], Batch [15/240],  Cross Loss: 0.0075\n",
            "15/240, Train_loss: 0.0075 Train_dice: 0.0075\n",
            "Epoch [21/100], Batch [16/240],  Cross Loss: 0.0678\n",
            "16/240, Train_loss: 0.0678 Train_dice: 0.0678\n",
            "Epoch [21/100], Batch [17/240],  Cross Loss: 0.0139\n",
            "17/240, Train_loss: 0.0139 Train_dice: 0.0139\n",
            "Epoch [21/100], Batch [18/240],  Cross Loss: 0.2832\n",
            "18/240, Train_loss: 0.2832 Train_dice: 0.2832\n",
            "Epoch [21/100], Batch [19/240],  Cross Loss: 0.1867\n",
            "19/240, Train_loss: 0.1867 Train_dice: 0.1867\n",
            "Epoch [21/100], Batch [20/240],  Cross Loss: 0.1380\n",
            "20/240, Train_loss: 0.1380 Train_dice: 0.1380\n",
            "Epoch [21/100], Batch [21/240],  Cross Loss: 0.0079\n",
            "21/240, Train_loss: 0.0079 Train_dice: 0.0079\n",
            "Epoch [21/100], Batch [22/240],  Cross Loss: 0.0063\n",
            "22/240, Train_loss: 0.0063 Train_dice: 0.0063\n",
            "Epoch [21/100], Batch [23/240],  Cross Loss: 0.4773\n",
            "23/240, Train_loss: 0.4773 Train_dice: 0.4773\n",
            "Epoch [21/100], Batch [24/240],  Cross Loss: 0.0343\n",
            "24/240, Train_loss: 0.0343 Train_dice: 0.0343\n",
            "Epoch [21/100], Batch [25/240],  Cross Loss: 0.0605\n",
            "25/240, Train_loss: 0.0605 Train_dice: 0.0605\n",
            "Epoch [21/100], Batch [26/240],  Cross Loss: 0.0006\n",
            "26/240, Train_loss: 0.0006 Train_dice: 0.0006\n",
            "Epoch [21/100], Batch [27/240],  Cross Loss: 0.6385\n",
            "27/240, Train_loss: 0.6385 Train_dice: 0.6385\n",
            "Epoch [21/100], Batch [28/240],  Cross Loss: 0.1720\n",
            "28/240, Train_loss: 0.1720 Train_dice: 0.1720\n",
            "Epoch [21/100], Batch [29/240],  Cross Loss: 0.1827\n",
            "29/240, Train_loss: 0.1827 Train_dice: 0.1827\n",
            "Epoch [21/100], Batch [30/240],  Cross Loss: 0.2887\n",
            "30/240, Train_loss: 0.2887 Train_dice: 0.2887\n",
            "Epoch [21/100], Batch [31/240],  Cross Loss: 0.2395\n",
            "31/240, Train_loss: 0.2395 Train_dice: 0.2395\n",
            "Epoch [21/100], Batch [32/240],  Cross Loss: 0.3627\n",
            "32/240, Train_loss: 0.3627 Train_dice: 0.3627\n",
            "Epoch [21/100], Batch [33/240],  Cross Loss: 0.0110\n",
            "33/240, Train_loss: 0.0110 Train_dice: 0.0110\n",
            "Epoch [21/100], Batch [34/240],  Cross Loss: 0.0739\n",
            "34/240, Train_loss: 0.0739 Train_dice: 0.0739\n",
            "Epoch [21/100], Batch [35/240],  Cross Loss: 0.0600\n",
            "35/240, Train_loss: 0.0600 Train_dice: 0.0600\n",
            "Epoch [21/100], Batch [36/240],  Cross Loss: 0.0046\n",
            "36/240, Train_loss: 0.0046 Train_dice: 0.0046\n",
            "Epoch [21/100], Batch [37/240],  Cross Loss: 0.0036\n",
            "37/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "Epoch [21/100], Batch [38/240],  Cross Loss: 0.0202\n",
            "38/240, Train_loss: 0.0202 Train_dice: 0.0202\n",
            "Epoch [21/100], Batch [39/240],  Cross Loss: 0.1245\n",
            "39/240, Train_loss: 0.1245 Train_dice: 0.1245\n",
            "Epoch [21/100], Batch [40/240],  Cross Loss: 0.0578\n",
            "40/240, Train_loss: 0.0578 Train_dice: 0.0578\n",
            "Epoch [21/100], Batch [41/240],  Cross Loss: 0.0017\n",
            "41/240, Train_loss: 0.0017 Train_dice: 0.0017\n",
            "Epoch [21/100], Batch [42/240],  Cross Loss: 0.1009\n",
            "42/240, Train_loss: 0.1009 Train_dice: 0.1009\n",
            "Epoch [21/100], Batch [43/240],  Cross Loss: 0.0043\n",
            "43/240, Train_loss: 0.0043 Train_dice: 0.0043\n",
            "Epoch [21/100], Batch [44/240],  Cross Loss: 0.0008\n",
            "44/240, Train_loss: 0.0008 Train_dice: 0.0008\n",
            "Epoch [21/100], Batch [45/240],  Cross Loss: 0.0442\n",
            "45/240, Train_loss: 0.0442 Train_dice: 0.0442\n",
            "Epoch [21/100], Batch [46/240],  Cross Loss: 0.1621\n",
            "46/240, Train_loss: 0.1621 Train_dice: 0.1621\n",
            "Epoch [21/100], Batch [47/240],  Cross Loss: 0.2389\n",
            "47/240, Train_loss: 0.2389 Train_dice: 0.2389\n",
            "Epoch [21/100], Batch [48/240],  Cross Loss: 0.0458\n",
            "48/240, Train_loss: 0.0458 Train_dice: 0.0458\n",
            "Epoch [21/100], Batch [49/240],  Cross Loss: 0.5216\n",
            "49/240, Train_loss: 0.5216 Train_dice: 0.5216\n",
            "Epoch [21/100], Batch [50/240],  Cross Loss: 0.3203\n",
            "50/240, Train_loss: 0.3203 Train_dice: 0.3203\n",
            "Epoch [21/100], Batch [51/240],  Cross Loss: 0.0047\n",
            "51/240, Train_loss: 0.0047 Train_dice: 0.0047\n",
            "Epoch [21/100], Batch [52/240],  Cross Loss: 0.0049\n",
            "52/240, Train_loss: 0.0049 Train_dice: 0.0049\n",
            "Epoch [21/100], Batch [53/240],  Cross Loss: 0.0792\n",
            "53/240, Train_loss: 0.0792 Train_dice: 0.0792\n",
            "Epoch [21/100], Batch [54/240],  Cross Loss: 0.0033\n",
            "54/240, Train_loss: 0.0033 Train_dice: 0.0033\n",
            "Epoch [21/100], Batch [55/240],  Cross Loss: 0.0025\n",
            "55/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [21/100], Batch [56/240],  Cross Loss: 0.0036\n",
            "56/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "Epoch [21/100], Batch [57/240],  Cross Loss: 0.2298\n",
            "57/240, Train_loss: 0.2298 Train_dice: 0.2298\n",
            "Epoch [21/100], Batch [58/240],  Cross Loss: 0.0173\n",
            "58/240, Train_loss: 0.0173 Train_dice: 0.0173\n",
            "Epoch [21/100], Batch [59/240],  Cross Loss: 0.1316\n",
            "59/240, Train_loss: 0.1316 Train_dice: 0.1316\n",
            "Epoch [21/100], Batch [60/240],  Cross Loss: 0.0469\n",
            "60/240, Train_loss: 0.0469 Train_dice: 0.0469\n",
            "Epoch [21/100], Batch [61/240],  Cross Loss: 0.0652\n",
            "61/240, Train_loss: 0.0652 Train_dice: 0.0652\n",
            "Epoch [21/100], Batch [62/240],  Cross Loss: 0.0009\n",
            "62/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [21/100], Batch [63/240],  Cross Loss: 0.0273\n",
            "63/240, Train_loss: 0.0273 Train_dice: 0.0273\n",
            "Epoch [21/100], Batch [64/240],  Cross Loss: 0.0724\n",
            "64/240, Train_loss: 0.0724 Train_dice: 0.0724\n",
            "Epoch [21/100], Batch [65/240],  Cross Loss: 0.0027\n",
            "65/240, Train_loss: 0.0027 Train_dice: 0.0027\n",
            "Epoch [21/100], Batch [66/240],  Cross Loss: 0.0097\n",
            "66/240, Train_loss: 0.0097 Train_dice: 0.0097\n",
            "Epoch [21/100], Batch [67/240],  Cross Loss: 0.3291\n",
            "67/240, Train_loss: 0.3291 Train_dice: 0.3291\n",
            "Epoch [21/100], Batch [68/240],  Cross Loss: 0.1094\n",
            "68/240, Train_loss: 0.1094 Train_dice: 0.1094\n",
            "Epoch [21/100], Batch [69/240],  Cross Loss: 0.0071\n",
            "69/240, Train_loss: 0.0071 Train_dice: 0.0071\n",
            "Epoch [21/100], Batch [70/240],  Cross Loss: 0.0644\n",
            "70/240, Train_loss: 0.0644 Train_dice: 0.0644\n",
            "Epoch [21/100], Batch [71/240],  Cross Loss: 0.5484\n",
            "71/240, Train_loss: 0.5484 Train_dice: 0.5484\n",
            "Epoch [21/100], Batch [72/240],  Cross Loss: 0.0012\n",
            "72/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [21/100], Batch [73/240],  Cross Loss: 0.0372\n",
            "73/240, Train_loss: 0.0372 Train_dice: 0.0372\n",
            "Epoch [21/100], Batch [74/240],  Cross Loss: 0.5628\n",
            "74/240, Train_loss: 0.5628 Train_dice: 0.5628\n",
            "Epoch [21/100], Batch [75/240],  Cross Loss: 0.0369\n",
            "75/240, Train_loss: 0.0369 Train_dice: 0.0369\n",
            "Epoch [21/100], Batch [76/240],  Cross Loss: 0.0535\n",
            "76/240, Train_loss: 0.0535 Train_dice: 0.0535\n",
            "Epoch [21/100], Batch [77/240],  Cross Loss: 0.4417\n",
            "77/240, Train_loss: 0.4417 Train_dice: 0.4417\n",
            "Epoch [21/100], Batch [78/240],  Cross Loss: 0.0055\n",
            "78/240, Train_loss: 0.0055 Train_dice: 0.0055\n",
            "Epoch [21/100], Batch [79/240],  Cross Loss: 0.6223\n",
            "79/240, Train_loss: 0.6223 Train_dice: 0.6223\n",
            "Epoch [21/100], Batch [80/240],  Cross Loss: 0.0240\n",
            "80/240, Train_loss: 0.0240 Train_dice: 0.0240\n",
            "Epoch [21/100], Batch [81/240],  Cross Loss: 0.1144\n",
            "81/240, Train_loss: 0.1144 Train_dice: 0.1144\n",
            "Epoch [21/100], Batch [82/240],  Cross Loss: 0.0762\n",
            "82/240, Train_loss: 0.0762 Train_dice: 0.0762\n",
            "Epoch [21/100], Batch [83/240],  Cross Loss: 0.0025\n",
            "83/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [21/100], Batch [84/240],  Cross Loss: 0.0240\n",
            "84/240, Train_loss: 0.0240 Train_dice: 0.0240\n",
            "Epoch [21/100], Batch [85/240],  Cross Loss: 0.0127\n",
            "85/240, Train_loss: 0.0127 Train_dice: 0.0127\n",
            "Epoch [21/100], Batch [86/240],  Cross Loss: 0.0304\n",
            "86/240, Train_loss: 0.0304 Train_dice: 0.0304\n",
            "Epoch [21/100], Batch [87/240],  Cross Loss: 0.0014\n",
            "87/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [21/100], Batch [88/240],  Cross Loss: 0.0408\n",
            "88/240, Train_loss: 0.0408 Train_dice: 0.0408\n",
            "Epoch [21/100], Batch [89/240],  Cross Loss: 0.0398\n",
            "89/240, Train_loss: 0.0398 Train_dice: 0.0398\n",
            "Epoch [21/100], Batch [90/240],  Cross Loss: 0.0046\n",
            "90/240, Train_loss: 0.0046 Train_dice: 0.0046\n",
            "Epoch [21/100], Batch [91/240],  Cross Loss: 0.2579\n",
            "91/240, Train_loss: 0.2579 Train_dice: 0.2579\n",
            "Epoch [21/100], Batch [92/240],  Cross Loss: 0.0280\n",
            "92/240, Train_loss: 0.0280 Train_dice: 0.0280\n",
            "Epoch [21/100], Batch [93/240],  Cross Loss: 0.2602\n",
            "93/240, Train_loss: 0.2602 Train_dice: 0.2602\n",
            "Epoch [21/100], Batch [94/240],  Cross Loss: 0.0169\n",
            "94/240, Train_loss: 0.0169 Train_dice: 0.0169\n",
            "Epoch [21/100], Batch [95/240],  Cross Loss: 0.0030\n",
            "95/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [21/100], Batch [96/240],  Cross Loss: 0.3143\n",
            "96/240, Train_loss: 0.3143 Train_dice: 0.3143\n",
            "Epoch [21/100], Batch [97/240],  Cross Loss: 0.1286\n",
            "97/240, Train_loss: 0.1286 Train_dice: 0.1286\n",
            "Epoch [21/100], Batch [98/240],  Cross Loss: 0.0195\n",
            "98/240, Train_loss: 0.0195 Train_dice: 0.0195\n",
            "Epoch [21/100], Batch [99/240],  Cross Loss: 0.0280\n",
            "99/240, Train_loss: 0.0280 Train_dice: 0.0280\n",
            "Epoch [21/100], Batch [100/240],  Cross Loss: 0.0213\n",
            "100/240, Train_loss: 0.0213 Train_dice: 0.0213\n",
            "Epoch [21/100], Batch [101/240],  Cross Loss: 0.1582\n",
            "101/240, Train_loss: 0.1582 Train_dice: 0.1582\n",
            "Epoch [21/100], Batch [102/240],  Cross Loss: 0.0021\n",
            "102/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [21/100], Batch [103/240],  Cross Loss: 0.0006\n",
            "103/240, Train_loss: 0.0006 Train_dice: 0.0006\n",
            "Epoch [21/100], Batch [104/240],  Cross Loss: 0.1810\n",
            "104/240, Train_loss: 0.1810 Train_dice: 0.1810\n",
            "Epoch [21/100], Batch [105/240],  Cross Loss: 0.0097\n",
            "105/240, Train_loss: 0.0097 Train_dice: 0.0097\n",
            "Epoch [21/100], Batch [106/240],  Cross Loss: 0.0181\n",
            "106/240, Train_loss: 0.0181 Train_dice: 0.0181\n",
            "Epoch [21/100], Batch [107/240],  Cross Loss: 0.0044\n",
            "107/240, Train_loss: 0.0044 Train_dice: 0.0044\n",
            "Epoch [21/100], Batch [108/240],  Cross Loss: 0.0202\n",
            "108/240, Train_loss: 0.0202 Train_dice: 0.0202\n",
            "Epoch [21/100], Batch [109/240],  Cross Loss: 0.0504\n",
            "109/240, Train_loss: 0.0504 Train_dice: 0.0504\n",
            "Epoch [21/100], Batch [110/240],  Cross Loss: 0.0241\n",
            "110/240, Train_loss: 0.0241 Train_dice: 0.0241\n",
            "Epoch [21/100], Batch [111/240],  Cross Loss: 0.1401\n",
            "111/240, Train_loss: 0.1401 Train_dice: 0.1401\n",
            "Epoch [21/100], Batch [112/240],  Cross Loss: 0.0166\n",
            "112/240, Train_loss: 0.0166 Train_dice: 0.0166\n",
            "Epoch [21/100], Batch [113/240],  Cross Loss: 0.1066\n",
            "113/240, Train_loss: 0.1066 Train_dice: 0.1066\n",
            "Epoch [21/100], Batch [114/240],  Cross Loss: 0.6641\n",
            "114/240, Train_loss: 0.6641 Train_dice: 0.6641\n",
            "Epoch [21/100], Batch [115/240],  Cross Loss: 0.4273\n",
            "115/240, Train_loss: 0.4273 Train_dice: 0.4273\n",
            "Epoch [21/100], Batch [116/240],  Cross Loss: 0.0042\n",
            "116/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [21/100], Batch [117/240],  Cross Loss: 0.0320\n",
            "117/240, Train_loss: 0.0320 Train_dice: 0.0320\n",
            "Epoch [21/100], Batch [118/240],  Cross Loss: 0.0055\n",
            "118/240, Train_loss: 0.0055 Train_dice: 0.0055\n",
            "Epoch [21/100], Batch [119/240],  Cross Loss: 0.0086\n",
            "119/240, Train_loss: 0.0086 Train_dice: 0.0086\n",
            "Epoch [21/100], Batch [120/240],  Cross Loss: 0.1875\n",
            "120/240, Train_loss: 0.1875 Train_dice: 0.1875\n",
            "Epoch [21/100], Batch [121/240],  Cross Loss: 0.0869\n",
            "121/240, Train_loss: 0.0869 Train_dice: 0.0869\n",
            "Epoch [21/100], Batch [122/240],  Cross Loss: 0.0584\n",
            "122/240, Train_loss: 0.0584 Train_dice: 0.0584\n",
            "Epoch [21/100], Batch [123/240],  Cross Loss: 0.0229\n",
            "123/240, Train_loss: 0.0229 Train_dice: 0.0229\n",
            "Epoch [21/100], Batch [124/240],  Cross Loss: 0.2138\n",
            "124/240, Train_loss: 0.2138 Train_dice: 0.2138\n",
            "Epoch [21/100], Batch [125/240],  Cross Loss: 0.0311\n",
            "125/240, Train_loss: 0.0311 Train_dice: 0.0311\n",
            "Epoch [21/100], Batch [126/240],  Cross Loss: 0.0004\n",
            "126/240, Train_loss: 0.0004 Train_dice: 0.0004\n",
            "Epoch [21/100], Batch [127/240],  Cross Loss: 0.0810\n",
            "127/240, Train_loss: 0.0810 Train_dice: 0.0810\n",
            "Epoch [21/100], Batch [128/240],  Cross Loss: 0.0029\n",
            "128/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [21/100], Batch [129/240],  Cross Loss: 0.0138\n",
            "129/240, Train_loss: 0.0138 Train_dice: 0.0138\n",
            "Epoch [21/100], Batch [130/240],  Cross Loss: 0.0620\n",
            "130/240, Train_loss: 0.0620 Train_dice: 0.0620\n",
            "Epoch [21/100], Batch [131/240],  Cross Loss: 0.8454\n",
            "131/240, Train_loss: 0.8454 Train_dice: 0.8454\n",
            "Epoch [21/100], Batch [132/240],  Cross Loss: 0.0306\n",
            "132/240, Train_loss: 0.0306 Train_dice: 0.0306\n",
            "Epoch [21/100], Batch [133/240],  Cross Loss: 0.4101\n",
            "133/240, Train_loss: 0.4101 Train_dice: 0.4101\n",
            "Epoch [21/100], Batch [134/240],  Cross Loss: 0.0163\n",
            "134/240, Train_loss: 0.0163 Train_dice: 0.0163\n",
            "Epoch [21/100], Batch [135/240],  Cross Loss: 0.0004\n",
            "135/240, Train_loss: 0.0004 Train_dice: 0.0004\n",
            "Epoch [21/100], Batch [136/240],  Cross Loss: 0.0037\n",
            "136/240, Train_loss: 0.0037 Train_dice: 0.0037\n",
            "Epoch [21/100], Batch [137/240],  Cross Loss: 0.0451\n",
            "137/240, Train_loss: 0.0451 Train_dice: 0.0451\n",
            "Epoch [21/100], Batch [138/240],  Cross Loss: 0.0019\n",
            "138/240, Train_loss: 0.0019 Train_dice: 0.0019\n",
            "Epoch [21/100], Batch [139/240],  Cross Loss: 0.0096\n",
            "139/240, Train_loss: 0.0096 Train_dice: 0.0096\n",
            "Epoch [21/100], Batch [140/240],  Cross Loss: 0.0023\n",
            "140/240, Train_loss: 0.0023 Train_dice: 0.0023\n",
            "Epoch [21/100], Batch [141/240],  Cross Loss: 0.0301\n",
            "141/240, Train_loss: 0.0301 Train_dice: 0.0301\n",
            "Epoch [21/100], Batch [142/240],  Cross Loss: 0.0510\n",
            "142/240, Train_loss: 0.0510 Train_dice: 0.0510\n",
            "Epoch [21/100], Batch [143/240],  Cross Loss: 0.0365\n",
            "143/240, Train_loss: 0.0365 Train_dice: 0.0365\n",
            "Epoch [21/100], Batch [144/240],  Cross Loss: 0.0429\n",
            "144/240, Train_loss: 0.0429 Train_dice: 0.0429\n",
            "Epoch [21/100], Batch [145/240],  Cross Loss: 0.0549\n",
            "145/240, Train_loss: 0.0549 Train_dice: 0.0549\n",
            "Epoch [21/100], Batch [146/240],  Cross Loss: 0.0103\n",
            "146/240, Train_loss: 0.0103 Train_dice: 0.0103\n",
            "Epoch [21/100], Batch [147/240],  Cross Loss: 0.1272\n",
            "147/240, Train_loss: 0.1272 Train_dice: 0.1272\n",
            "Epoch [21/100], Batch [148/240],  Cross Loss: 0.0103\n",
            "148/240, Train_loss: 0.0103 Train_dice: 0.0103\n",
            "Epoch [21/100], Batch [149/240],  Cross Loss: 0.1914\n",
            "149/240, Train_loss: 0.1914 Train_dice: 0.1914\n",
            "Epoch [21/100], Batch [150/240],  Cross Loss: 0.0030\n",
            "150/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [21/100], Batch [151/240],  Cross Loss: 0.3143\n",
            "151/240, Train_loss: 0.3143 Train_dice: 0.3143\n",
            "Epoch [21/100], Batch [152/240],  Cross Loss: 0.2305\n",
            "152/240, Train_loss: 0.2305 Train_dice: 0.2305\n",
            "Epoch [21/100], Batch [153/240],  Cross Loss: 0.0545\n",
            "153/240, Train_loss: 0.0545 Train_dice: 0.0545\n",
            "Epoch [21/100], Batch [154/240],  Cross Loss: 0.0323\n",
            "154/240, Train_loss: 0.0323 Train_dice: 0.0323\n",
            "Epoch [21/100], Batch [155/240],  Cross Loss: 0.0057\n",
            "155/240, Train_loss: 0.0057 Train_dice: 0.0057\n",
            "Epoch [21/100], Batch [156/240],  Cross Loss: 0.1060\n",
            "156/240, Train_loss: 0.1060 Train_dice: 0.1060\n",
            "Epoch [21/100], Batch [157/240],  Cross Loss: 0.0138\n",
            "157/240, Train_loss: 0.0138 Train_dice: 0.0138\n",
            "Epoch [21/100], Batch [158/240],  Cross Loss: 0.0033\n",
            "158/240, Train_loss: 0.0033 Train_dice: 0.0033\n",
            "Epoch [21/100], Batch [159/240],  Cross Loss: 0.3584\n",
            "159/240, Train_loss: 0.3584 Train_dice: 0.3584\n",
            "Epoch [21/100], Batch [160/240],  Cross Loss: 0.0883\n",
            "160/240, Train_loss: 0.0883 Train_dice: 0.0883\n",
            "Epoch [21/100], Batch [161/240],  Cross Loss: 0.0212\n",
            "161/240, Train_loss: 0.0212 Train_dice: 0.0212\n",
            "Epoch [21/100], Batch [162/240],  Cross Loss: 0.0389\n",
            "162/240, Train_loss: 0.0389 Train_dice: 0.0389\n",
            "Epoch [21/100], Batch [163/240],  Cross Loss: 0.0054\n",
            "163/240, Train_loss: 0.0054 Train_dice: 0.0054\n",
            "Epoch [21/100], Batch [164/240],  Cross Loss: 0.5575\n",
            "164/240, Train_loss: 0.5575 Train_dice: 0.5575\n",
            "Epoch [21/100], Batch [165/240],  Cross Loss: 0.3126\n",
            "165/240, Train_loss: 0.3126 Train_dice: 0.3126\n",
            "Epoch [21/100], Batch [166/240],  Cross Loss: 0.5460\n",
            "166/240, Train_loss: 0.5460 Train_dice: 0.5460\n",
            "Epoch [21/100], Batch [167/240],  Cross Loss: 0.0744\n",
            "167/240, Train_loss: 0.0744 Train_dice: 0.0744\n",
            "Epoch [21/100], Batch [168/240],  Cross Loss: 0.0102\n",
            "168/240, Train_loss: 0.0102 Train_dice: 0.0102\n",
            "Epoch [21/100], Batch [169/240],  Cross Loss: 0.0148\n",
            "169/240, Train_loss: 0.0148 Train_dice: 0.0148\n",
            "Epoch [21/100], Batch [170/240],  Cross Loss: 0.0163\n",
            "170/240, Train_loss: 0.0163 Train_dice: 0.0163\n",
            "Epoch [21/100], Batch [171/240],  Cross Loss: 0.0151\n",
            "171/240, Train_loss: 0.0151 Train_dice: 0.0151\n",
            "Epoch [21/100], Batch [172/240],  Cross Loss: 0.0041\n",
            "172/240, Train_loss: 0.0041 Train_dice: 0.0041\n",
            "Epoch [21/100], Batch [173/240],  Cross Loss: 0.0014\n",
            "173/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [21/100], Batch [174/240],  Cross Loss: 0.0284\n",
            "174/240, Train_loss: 0.0284 Train_dice: 0.0284\n",
            "Epoch [21/100], Batch [175/240],  Cross Loss: 0.4394\n",
            "175/240, Train_loss: 0.4394 Train_dice: 0.4394\n",
            "Epoch [21/100], Batch [176/240],  Cross Loss: 0.0118\n",
            "176/240, Train_loss: 0.0118 Train_dice: 0.0118\n",
            "Epoch [21/100], Batch [177/240],  Cross Loss: 0.5639\n",
            "177/240, Train_loss: 0.5639 Train_dice: 0.5639\n",
            "Epoch [21/100], Batch [178/240],  Cross Loss: 0.1406\n",
            "178/240, Train_loss: 0.1406 Train_dice: 0.1406\n",
            "Epoch [21/100], Batch [179/240],  Cross Loss: 0.0871\n",
            "179/240, Train_loss: 0.0871 Train_dice: 0.0871\n",
            "Epoch [21/100], Batch [180/240],  Cross Loss: 0.0024\n",
            "180/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [21/100], Batch [181/240],  Cross Loss: 0.1598\n",
            "181/240, Train_loss: 0.1598 Train_dice: 0.1598\n",
            "Epoch [21/100], Batch [182/240],  Cross Loss: 0.1083\n",
            "182/240, Train_loss: 0.1083 Train_dice: 0.1083\n",
            "Epoch [21/100], Batch [183/240],  Cross Loss: 0.0014\n",
            "183/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [21/100], Batch [184/240],  Cross Loss: 0.0341\n",
            "184/240, Train_loss: 0.0341 Train_dice: 0.0341\n",
            "Epoch [21/100], Batch [185/240],  Cross Loss: 0.0664\n",
            "185/240, Train_loss: 0.0664 Train_dice: 0.0664\n",
            "Epoch [21/100], Batch [186/240],  Cross Loss: 0.1607\n",
            "186/240, Train_loss: 0.1607 Train_dice: 0.1607\n",
            "Epoch [21/100], Batch [187/240],  Cross Loss: 0.4603\n",
            "187/240, Train_loss: 0.4603 Train_dice: 0.4603\n",
            "Epoch [21/100], Batch [188/240],  Cross Loss: 0.4501\n",
            "188/240, Train_loss: 0.4501 Train_dice: 0.4501\n",
            "Epoch [21/100], Batch [189/240],  Cross Loss: 0.0063\n",
            "189/240, Train_loss: 0.0063 Train_dice: 0.0063\n",
            "Epoch [21/100], Batch [190/240],  Cross Loss: 0.0850\n",
            "190/240, Train_loss: 0.0850 Train_dice: 0.0850\n",
            "Epoch [21/100], Batch [191/240],  Cross Loss: 0.0945\n",
            "191/240, Train_loss: 0.0945 Train_dice: 0.0945\n",
            "Epoch [21/100], Batch [192/240],  Cross Loss: 0.0121\n",
            "192/240, Train_loss: 0.0121 Train_dice: 0.0121\n",
            "Epoch [21/100], Batch [193/240],  Cross Loss: 0.3820\n",
            "193/240, Train_loss: 0.3820 Train_dice: 0.3820\n",
            "Epoch [21/100], Batch [194/240],  Cross Loss: 0.0102\n",
            "194/240, Train_loss: 0.0102 Train_dice: 0.0102\n",
            "Epoch [21/100], Batch [195/240],  Cross Loss: 0.0178\n",
            "195/240, Train_loss: 0.0178 Train_dice: 0.0178\n",
            "Epoch [21/100], Batch [196/240],  Cross Loss: 0.1712\n",
            "196/240, Train_loss: 0.1712 Train_dice: 0.1712\n",
            "Epoch [21/100], Batch [197/240],  Cross Loss: 0.6211\n",
            "197/240, Train_loss: 0.6211 Train_dice: 0.6211\n",
            "Epoch [21/100], Batch [198/240],  Cross Loss: 0.0716\n",
            "198/240, Train_loss: 0.0716 Train_dice: 0.0716\n",
            "Epoch [21/100], Batch [199/240],  Cross Loss: 0.3337\n",
            "199/240, Train_loss: 0.3337 Train_dice: 0.3337\n",
            "Epoch [21/100], Batch [200/240],  Cross Loss: 0.1548\n",
            "200/240, Train_loss: 0.1548 Train_dice: 0.1548\n",
            "Epoch [21/100], Batch [201/240],  Cross Loss: 0.0449\n",
            "201/240, Train_loss: 0.0449 Train_dice: 0.0449\n",
            "Epoch [21/100], Batch [202/240],  Cross Loss: 0.2251\n",
            "202/240, Train_loss: 0.2251 Train_dice: 0.2251\n",
            "Epoch [21/100], Batch [203/240],  Cross Loss: 0.0624\n",
            "203/240, Train_loss: 0.0624 Train_dice: 0.0624\n",
            "Epoch [21/100], Batch [204/240],  Cross Loss: 0.0050\n",
            "204/240, Train_loss: 0.0050 Train_dice: 0.0050\n",
            "Epoch [21/100], Batch [205/240],  Cross Loss: 0.0102\n",
            "205/240, Train_loss: 0.0102 Train_dice: 0.0102\n",
            "Epoch [21/100], Batch [206/240],  Cross Loss: 0.0205\n",
            "206/240, Train_loss: 0.0205 Train_dice: 0.0205\n",
            "Epoch [21/100], Batch [207/240],  Cross Loss: 0.0503\n",
            "207/240, Train_loss: 0.0503 Train_dice: 0.0503\n",
            "Epoch [21/100], Batch [208/240],  Cross Loss: 0.0005\n",
            "208/240, Train_loss: 0.0005 Train_dice: 0.0005\n",
            "Epoch [21/100], Batch [209/240],  Cross Loss: 0.0087\n",
            "209/240, Train_loss: 0.0087 Train_dice: 0.0087\n",
            "Epoch [21/100], Batch [210/240],  Cross Loss: 0.4864\n",
            "210/240, Train_loss: 0.4864 Train_dice: 0.4864\n",
            "Epoch [21/100], Batch [211/240],  Cross Loss: 0.0111\n",
            "211/240, Train_loss: 0.0111 Train_dice: 0.0111\n",
            "Epoch [21/100], Batch [212/240],  Cross Loss: 0.5812\n",
            "212/240, Train_loss: 0.5812 Train_dice: 0.5812\n",
            "Epoch [21/100], Batch [213/240],  Cross Loss: 0.0068\n",
            "213/240, Train_loss: 0.0068 Train_dice: 0.0068\n",
            "Epoch [21/100], Batch [214/240],  Cross Loss: 0.1241\n",
            "214/240, Train_loss: 0.1241 Train_dice: 0.1241\n",
            "Epoch [21/100], Batch [215/240],  Cross Loss: 0.0031\n",
            "215/240, Train_loss: 0.0031 Train_dice: 0.0031\n",
            "Epoch [21/100], Batch [216/240],  Cross Loss: 0.0914\n",
            "216/240, Train_loss: 0.0914 Train_dice: 0.0914\n",
            "Epoch [21/100], Batch [217/240],  Cross Loss: 0.0037\n",
            "217/240, Train_loss: 0.0037 Train_dice: 0.0037\n",
            "Epoch [21/100], Batch [218/240],  Cross Loss: 0.0013\n",
            "218/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [21/100], Batch [219/240],  Cross Loss: 0.4820\n",
            "219/240, Train_loss: 0.4820 Train_dice: 0.4820\n",
            "Epoch [21/100], Batch [220/240],  Cross Loss: 0.2016\n",
            "220/240, Train_loss: 0.2016 Train_dice: 0.2016\n",
            "Epoch [21/100], Batch [221/240],  Cross Loss: 0.0040\n",
            "221/240, Train_loss: 0.0040 Train_dice: 0.0040\n",
            "Epoch [21/100], Batch [222/240],  Cross Loss: 0.0008\n",
            "222/240, Train_loss: 0.0008 Train_dice: 0.0008\n",
            "Epoch [21/100], Batch [223/240],  Cross Loss: 0.0011\n",
            "223/240, Train_loss: 0.0011 Train_dice: 0.0011\n",
            "Epoch [21/100], Batch [224/240],  Cross Loss: 0.0021\n",
            "224/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [21/100], Batch [225/240],  Cross Loss: 0.0149\n",
            "225/240, Train_loss: 0.0149 Train_dice: 0.0149\n",
            "Epoch [21/100], Batch [226/240],  Cross Loss: 0.1412\n",
            "226/240, Train_loss: 0.1412 Train_dice: 0.1412\n",
            "Epoch [21/100], Batch [227/240],  Cross Loss: 0.1678\n",
            "227/240, Train_loss: 0.1678 Train_dice: 0.1678\n",
            "Epoch [21/100], Batch [228/240],  Cross Loss: 0.0066\n",
            "228/240, Train_loss: 0.0066 Train_dice: 0.0066\n",
            "Epoch [21/100], Batch [229/240],  Cross Loss: 0.0009\n",
            "229/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [21/100], Batch [230/240],  Cross Loss: 0.0331\n",
            "230/240, Train_loss: 0.0331 Train_dice: 0.0331\n",
            "Epoch [21/100], Batch [231/240],  Cross Loss: 0.0172\n",
            "231/240, Train_loss: 0.0172 Train_dice: 0.0172\n",
            "Epoch [21/100], Batch [232/240],  Cross Loss: 0.0436\n",
            "232/240, Train_loss: 0.0436 Train_dice: 0.0436\n",
            "Epoch [21/100], Batch [233/240],  Cross Loss: 0.0258\n",
            "233/240, Train_loss: 0.0258 Train_dice: 0.0258\n",
            "Epoch [21/100], Batch [234/240],  Cross Loss: 0.1115\n",
            "234/240, Train_loss: 0.1115 Train_dice: 0.1115\n",
            "Epoch [21/100], Batch [235/240],  Cross Loss: 0.0010\n",
            "235/240, Train_loss: 0.0010 Train_dice: 0.0010\n",
            "Epoch [21/100], Batch [236/240],  Cross Loss: 0.0075\n",
            "236/240, Train_loss: 0.0075 Train_dice: 0.0075\n",
            "Epoch [21/100], Batch [237/240],  Cross Loss: 0.2031\n",
            "237/240, Train_loss: 0.2031 Train_dice: 0.2031\n",
            "Epoch [21/100], Batch [238/240],  Cross Loss: 0.0050\n",
            "238/240, Train_loss: 0.0050 Train_dice: 0.0050\n",
            "Epoch [21/100], Batch [239/240],  Cross Loss: 0.0016\n",
            "239/240, Train_loss: 0.0016 Train_dice: 0.0016\n",
            "Epoch [21/100], Batch [240/240],  Cross Loss: 0.0018\n",
            "240/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "--------------------\n",
            "Epoch_loss: 0.1092\n",
            "Epoch_metric: tensor(0.1092, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 22/100\n",
            "Epoch [22/100], Batch [1/240],  Cross Loss: 0.0105\n",
            "1/240, Train_loss: 0.0105 Train_dice: 0.0105\n",
            "Epoch [22/100], Batch [2/240],  Cross Loss: 0.0041\n",
            "2/240, Train_loss: 0.0041 Train_dice: 0.0041\n",
            "Epoch [22/100], Batch [3/240],  Cross Loss: 0.2535\n",
            "3/240, Train_loss: 0.2535 Train_dice: 0.2535\n",
            "Epoch [22/100], Batch [4/240],  Cross Loss: 0.0274\n",
            "4/240, Train_loss: 0.0274 Train_dice: 0.0274\n",
            "Epoch [22/100], Batch [5/240],  Cross Loss: 0.0010\n",
            "5/240, Train_loss: 0.0010 Train_dice: 0.0010\n",
            "Epoch [22/100], Batch [6/240],  Cross Loss: 0.0018\n",
            "6/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [22/100], Batch [7/240],  Cross Loss: 0.0196\n",
            "7/240, Train_loss: 0.0196 Train_dice: 0.0196\n",
            "Epoch [22/100], Batch [8/240],  Cross Loss: 0.0314\n",
            "8/240, Train_loss: 0.0314 Train_dice: 0.0314\n",
            "Epoch [22/100], Batch [9/240],  Cross Loss: 0.1095\n",
            "9/240, Train_loss: 0.1095 Train_dice: 0.1095\n",
            "Epoch [22/100], Batch [10/240],  Cross Loss: 0.3901\n",
            "10/240, Train_loss: 0.3901 Train_dice: 0.3901\n",
            "Epoch [22/100], Batch [11/240],  Cross Loss: 0.3273\n",
            "11/240, Train_loss: 0.3273 Train_dice: 0.3273\n",
            "Epoch [22/100], Batch [12/240],  Cross Loss: 0.0007\n",
            "12/240, Train_loss: 0.0007 Train_dice: 0.0007\n",
            "Epoch [22/100], Batch [13/240],  Cross Loss: 0.0112\n",
            "13/240, Train_loss: 0.0112 Train_dice: 0.0112\n",
            "Epoch [22/100], Batch [14/240],  Cross Loss: 0.0502\n",
            "14/240, Train_loss: 0.0502 Train_dice: 0.0502\n",
            "Epoch [22/100], Batch [15/240],  Cross Loss: 0.0010\n",
            "15/240, Train_loss: 0.0010 Train_dice: 0.0010\n",
            "Epoch [22/100], Batch [16/240],  Cross Loss: 0.0525\n",
            "16/240, Train_loss: 0.0525 Train_dice: 0.0525\n",
            "Epoch [22/100], Batch [17/240],  Cross Loss: 0.0038\n",
            "17/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [22/100], Batch [18/240],  Cross Loss: 0.0171\n",
            "18/240, Train_loss: 0.0171 Train_dice: 0.0171\n",
            "Epoch [22/100], Batch [19/240],  Cross Loss: 0.0736\n",
            "19/240, Train_loss: 0.0736 Train_dice: 0.0736\n",
            "Epoch [22/100], Batch [20/240],  Cross Loss: 0.0153\n",
            "20/240, Train_loss: 0.0153 Train_dice: 0.0153\n",
            "Epoch [22/100], Batch [21/240],  Cross Loss: 0.1399\n",
            "21/240, Train_loss: 0.1399 Train_dice: 0.1399\n",
            "Epoch [22/100], Batch [22/240],  Cross Loss: 0.4858\n",
            "22/240, Train_loss: 0.4858 Train_dice: 0.4858\n",
            "Epoch [22/100], Batch [23/240],  Cross Loss: 0.0053\n",
            "23/240, Train_loss: 0.0053 Train_dice: 0.0053\n",
            "Epoch [22/100], Batch [24/240],  Cross Loss: 0.0597\n",
            "24/240, Train_loss: 0.0597 Train_dice: 0.0597\n",
            "Epoch [22/100], Batch [25/240],  Cross Loss: 0.1594\n",
            "25/240, Train_loss: 0.1594 Train_dice: 0.1594\n",
            "Epoch [22/100], Batch [26/240],  Cross Loss: 0.1012\n",
            "26/240, Train_loss: 0.1012 Train_dice: 0.1012\n",
            "Epoch [22/100], Batch [27/240],  Cross Loss: 0.2841\n",
            "27/240, Train_loss: 0.2841 Train_dice: 0.2841\n",
            "Epoch [22/100], Batch [28/240],  Cross Loss: 0.0162\n",
            "28/240, Train_loss: 0.0162 Train_dice: 0.0162\n",
            "Epoch [22/100], Batch [29/240],  Cross Loss: 0.0057\n",
            "29/240, Train_loss: 0.0057 Train_dice: 0.0057\n",
            "Epoch [22/100], Batch [30/240],  Cross Loss: 0.1139\n",
            "30/240, Train_loss: 0.1139 Train_dice: 0.1139\n",
            "Epoch [22/100], Batch [31/240],  Cross Loss: 0.0106\n",
            "31/240, Train_loss: 0.0106 Train_dice: 0.0106\n",
            "Epoch [22/100], Batch [32/240],  Cross Loss: 0.0122\n",
            "32/240, Train_loss: 0.0122 Train_dice: 0.0122\n",
            "Epoch [22/100], Batch [33/240],  Cross Loss: 0.0331\n",
            "33/240, Train_loss: 0.0331 Train_dice: 0.0331\n",
            "Epoch [22/100], Batch [34/240],  Cross Loss: 0.5942\n",
            "34/240, Train_loss: 0.5942 Train_dice: 0.5942\n",
            "Epoch [22/100], Batch [35/240],  Cross Loss: 0.0635\n",
            "35/240, Train_loss: 0.0635 Train_dice: 0.0635\n",
            "Epoch [22/100], Batch [36/240],  Cross Loss: 0.0167\n",
            "36/240, Train_loss: 0.0167 Train_dice: 0.0167\n",
            "Epoch [22/100], Batch [37/240],  Cross Loss: 0.0019\n",
            "37/240, Train_loss: 0.0019 Train_dice: 0.0019\n",
            "Epoch [22/100], Batch [38/240],  Cross Loss: 0.6405\n",
            "38/240, Train_loss: 0.6405 Train_dice: 0.6405\n",
            "Epoch [22/100], Batch [39/240],  Cross Loss: 0.0034\n",
            "39/240, Train_loss: 0.0034 Train_dice: 0.0034\n",
            "Epoch [22/100], Batch [40/240],  Cross Loss: 0.0120\n",
            "40/240, Train_loss: 0.0120 Train_dice: 0.0120\n",
            "Epoch [22/100], Batch [41/240],  Cross Loss: 0.0160\n",
            "41/240, Train_loss: 0.0160 Train_dice: 0.0160\n",
            "Epoch [22/100], Batch [42/240],  Cross Loss: 0.0081\n",
            "42/240, Train_loss: 0.0081 Train_dice: 0.0081\n",
            "Epoch [22/100], Batch [43/240],  Cross Loss: 0.1384\n",
            "43/240, Train_loss: 0.1384 Train_dice: 0.1384\n",
            "Epoch [22/100], Batch [44/240],  Cross Loss: 0.6324\n",
            "44/240, Train_loss: 0.6324 Train_dice: 0.6324\n",
            "Epoch [22/100], Batch [45/240],  Cross Loss: 0.6788\n",
            "45/240, Train_loss: 0.6788 Train_dice: 0.6788\n",
            "Epoch [22/100], Batch [46/240],  Cross Loss: 0.0488\n",
            "46/240, Train_loss: 0.0488 Train_dice: 0.0488\n",
            "Epoch [22/100], Batch [47/240],  Cross Loss: 0.0435\n",
            "47/240, Train_loss: 0.0435 Train_dice: 0.0435\n",
            "Epoch [22/100], Batch [48/240],  Cross Loss: 0.0143\n",
            "48/240, Train_loss: 0.0143 Train_dice: 0.0143\n",
            "Epoch [22/100], Batch [49/240],  Cross Loss: 0.0008\n",
            "49/240, Train_loss: 0.0008 Train_dice: 0.0008\n",
            "Epoch [22/100], Batch [50/240],  Cross Loss: 0.0151\n",
            "50/240, Train_loss: 0.0151 Train_dice: 0.0151\n",
            "Epoch [22/100], Batch [51/240],  Cross Loss: 0.0052\n",
            "51/240, Train_loss: 0.0052 Train_dice: 0.0052\n",
            "Epoch [22/100], Batch [52/240],  Cross Loss: 0.0009\n",
            "52/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [22/100], Batch [53/240],  Cross Loss: 0.0018\n",
            "53/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [22/100], Batch [54/240],  Cross Loss: 0.0040\n",
            "54/240, Train_loss: 0.0040 Train_dice: 0.0040\n",
            "Epoch [22/100], Batch [55/240],  Cross Loss: 0.0447\n",
            "55/240, Train_loss: 0.0447 Train_dice: 0.0447\n",
            "Epoch [22/100], Batch [56/240],  Cross Loss: 0.0040\n",
            "56/240, Train_loss: 0.0040 Train_dice: 0.0040\n",
            "Epoch [22/100], Batch [57/240],  Cross Loss: 0.0034\n",
            "57/240, Train_loss: 0.0034 Train_dice: 0.0034\n",
            "Epoch [22/100], Batch [58/240],  Cross Loss: 0.1184\n",
            "58/240, Train_loss: 0.1184 Train_dice: 0.1184\n",
            "Epoch [22/100], Batch [59/240],  Cross Loss: 0.0017\n",
            "59/240, Train_loss: 0.0017 Train_dice: 0.0017\n",
            "Epoch [22/100], Batch [60/240],  Cross Loss: 0.0390\n",
            "60/240, Train_loss: 0.0390 Train_dice: 0.0390\n",
            "Epoch [22/100], Batch [61/240],  Cross Loss: 0.0031\n",
            "61/240, Train_loss: 0.0031 Train_dice: 0.0031\n",
            "Epoch [22/100], Batch [62/240],  Cross Loss: 0.0136\n",
            "62/240, Train_loss: 0.0136 Train_dice: 0.0136\n",
            "Epoch [22/100], Batch [63/240],  Cross Loss: 0.0860\n",
            "63/240, Train_loss: 0.0860 Train_dice: 0.0860\n",
            "Epoch [22/100], Batch [64/240],  Cross Loss: 0.0480\n",
            "64/240, Train_loss: 0.0480 Train_dice: 0.0480\n",
            "Epoch [22/100], Batch [65/240],  Cross Loss: 0.3957\n",
            "65/240, Train_loss: 0.3957 Train_dice: 0.3957\n",
            "Epoch [22/100], Batch [66/240],  Cross Loss: 0.0088\n",
            "66/240, Train_loss: 0.0088 Train_dice: 0.0088\n",
            "Epoch [22/100], Batch [67/240],  Cross Loss: 0.0030\n",
            "67/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [22/100], Batch [68/240],  Cross Loss: 0.0005\n",
            "68/240, Train_loss: 0.0005 Train_dice: 0.0005\n",
            "Epoch [22/100], Batch [69/240],  Cross Loss: 0.1329\n",
            "69/240, Train_loss: 0.1329 Train_dice: 0.1329\n",
            "Epoch [22/100], Batch [70/240],  Cross Loss: 0.0256\n",
            "70/240, Train_loss: 0.0256 Train_dice: 0.0256\n",
            "Epoch [22/100], Batch [71/240],  Cross Loss: 0.0470\n",
            "71/240, Train_loss: 0.0470 Train_dice: 0.0470\n",
            "Epoch [22/100], Batch [72/240],  Cross Loss: 0.0116\n",
            "72/240, Train_loss: 0.0116 Train_dice: 0.0116\n",
            "Epoch [22/100], Batch [73/240],  Cross Loss: 0.0029\n",
            "73/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [22/100], Batch [74/240],  Cross Loss: 0.6236\n",
            "74/240, Train_loss: 0.6236 Train_dice: 0.6236\n",
            "Epoch [22/100], Batch [75/240],  Cross Loss: 0.3518\n",
            "75/240, Train_loss: 0.3518 Train_dice: 0.3518\n",
            "Epoch [22/100], Batch [76/240],  Cross Loss: 0.0003\n",
            "76/240, Train_loss: 0.0003 Train_dice: 0.0003\n",
            "Epoch [22/100], Batch [77/240],  Cross Loss: 0.0519\n",
            "77/240, Train_loss: 0.0519 Train_dice: 0.0519\n",
            "Epoch [22/100], Batch [78/240],  Cross Loss: 0.0015\n",
            "78/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [22/100], Batch [79/240],  Cross Loss: 0.0036\n",
            "79/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "Epoch [22/100], Batch [80/240],  Cross Loss: 0.0026\n",
            "80/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [22/100], Batch [81/240],  Cross Loss: 0.0087\n",
            "81/240, Train_loss: 0.0087 Train_dice: 0.0087\n",
            "Epoch [22/100], Batch [82/240],  Cross Loss: 0.0028\n",
            "82/240, Train_loss: 0.0028 Train_dice: 0.0028\n",
            "Epoch [22/100], Batch [83/240],  Cross Loss: 0.0004\n",
            "83/240, Train_loss: 0.0004 Train_dice: 0.0004\n",
            "Epoch [22/100], Batch [84/240],  Cross Loss: 0.0125\n",
            "84/240, Train_loss: 0.0125 Train_dice: 0.0125\n",
            "Epoch [22/100], Batch [85/240],  Cross Loss: 0.0025\n",
            "85/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [22/100], Batch [86/240],  Cross Loss: 0.0113\n",
            "86/240, Train_loss: 0.0113 Train_dice: 0.0113\n",
            "Epoch [22/100], Batch [87/240],  Cross Loss: 0.0013\n",
            "87/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [22/100], Batch [88/240],  Cross Loss: 0.0687\n",
            "88/240, Train_loss: 0.0687 Train_dice: 0.0687\n",
            "Epoch [22/100], Batch [89/240],  Cross Loss: 0.6062\n",
            "89/240, Train_loss: 0.6062 Train_dice: 0.6062\n",
            "Epoch [22/100], Batch [90/240],  Cross Loss: 0.0013\n",
            "90/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [22/100], Batch [91/240],  Cross Loss: 0.6506\n",
            "91/240, Train_loss: 0.6506 Train_dice: 0.6506\n",
            "Epoch [22/100], Batch [92/240],  Cross Loss: 0.0677\n",
            "92/240, Train_loss: 0.0677 Train_dice: 0.0677\n",
            "Epoch [22/100], Batch [93/240],  Cross Loss: 0.0497\n",
            "93/240, Train_loss: 0.0497 Train_dice: 0.0497\n",
            "Epoch [22/100], Batch [94/240],  Cross Loss: 0.0727\n",
            "94/240, Train_loss: 0.0727 Train_dice: 0.0727\n",
            "Epoch [22/100], Batch [95/240],  Cross Loss: 0.0162\n",
            "95/240, Train_loss: 0.0162 Train_dice: 0.0162\n",
            "Epoch [22/100], Batch [96/240],  Cross Loss: 0.0047\n",
            "96/240, Train_loss: 0.0047 Train_dice: 0.0047\n",
            "Epoch [22/100], Batch [97/240],  Cross Loss: 0.0728\n",
            "97/240, Train_loss: 0.0728 Train_dice: 0.0728\n",
            "Epoch [22/100], Batch [98/240],  Cross Loss: 0.0051\n",
            "98/240, Train_loss: 0.0051 Train_dice: 0.0051\n",
            "Epoch [22/100], Batch [99/240],  Cross Loss: 0.0922\n",
            "99/240, Train_loss: 0.0922 Train_dice: 0.0922\n",
            "Epoch [22/100], Batch [100/240],  Cross Loss: 0.1205\n",
            "100/240, Train_loss: 0.1205 Train_dice: 0.1205\n",
            "Epoch [22/100], Batch [101/240],  Cross Loss: 0.2187\n",
            "101/240, Train_loss: 0.2187 Train_dice: 0.2187\n",
            "Epoch [22/100], Batch [102/240],  Cross Loss: 0.0410\n",
            "102/240, Train_loss: 0.0410 Train_dice: 0.0410\n",
            "Epoch [22/100], Batch [103/240],  Cross Loss: 0.0716\n",
            "103/240, Train_loss: 0.0716 Train_dice: 0.0716\n",
            "Epoch [22/100], Batch [104/240],  Cross Loss: 0.0276\n",
            "104/240, Train_loss: 0.0276 Train_dice: 0.0276\n",
            "Epoch [22/100], Batch [105/240],  Cross Loss: 0.0003\n",
            "105/240, Train_loss: 0.0003 Train_dice: 0.0003\n",
            "Epoch [22/100], Batch [106/240],  Cross Loss: 0.4825\n",
            "106/240, Train_loss: 0.4825 Train_dice: 0.4825\n",
            "Epoch [22/100], Batch [107/240],  Cross Loss: 0.1118\n",
            "107/240, Train_loss: 0.1118 Train_dice: 0.1118\n",
            "Epoch [22/100], Batch [108/240],  Cross Loss: 0.0009\n",
            "108/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [22/100], Batch [109/240],  Cross Loss: 0.0050\n",
            "109/240, Train_loss: 0.0050 Train_dice: 0.0050\n",
            "Epoch [22/100], Batch [110/240],  Cross Loss: 0.1053\n",
            "110/240, Train_loss: 0.1053 Train_dice: 0.1053\n",
            "Epoch [22/100], Batch [111/240],  Cross Loss: 0.0140\n",
            "111/240, Train_loss: 0.0140 Train_dice: 0.0140\n",
            "Epoch [22/100], Batch [112/240],  Cross Loss: 0.0006\n",
            "112/240, Train_loss: 0.0006 Train_dice: 0.0006\n",
            "Epoch [22/100], Batch [113/240],  Cross Loss: 0.0248\n",
            "113/240, Train_loss: 0.0248 Train_dice: 0.0248\n",
            "Epoch [22/100], Batch [114/240],  Cross Loss: 0.0022\n",
            "114/240, Train_loss: 0.0022 Train_dice: 0.0022\n",
            "Epoch [22/100], Batch [115/240],  Cross Loss: 0.0008\n",
            "115/240, Train_loss: 0.0008 Train_dice: 0.0008\n",
            "Epoch [22/100], Batch [116/240],  Cross Loss: 0.0005\n",
            "116/240, Train_loss: 0.0005 Train_dice: 0.0005\n",
            "Epoch [22/100], Batch [117/240],  Cross Loss: 0.0013\n",
            "117/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [22/100], Batch [118/240],  Cross Loss: 0.0013\n",
            "118/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [22/100], Batch [119/240],  Cross Loss: 0.0004\n",
            "119/240, Train_loss: 0.0004 Train_dice: 0.0004\n",
            "Epoch [22/100], Batch [120/240],  Cross Loss: 0.0581\n",
            "120/240, Train_loss: 0.0581 Train_dice: 0.0581\n",
            "Epoch [22/100], Batch [121/240],  Cross Loss: 0.5267\n",
            "121/240, Train_loss: 0.5267 Train_dice: 0.5267\n",
            "Epoch [22/100], Batch [122/240],  Cross Loss: 0.2707\n",
            "122/240, Train_loss: 0.2707 Train_dice: 0.2707\n",
            "Epoch [22/100], Batch [123/240],  Cross Loss: 0.5659\n",
            "123/240, Train_loss: 0.5659 Train_dice: 0.5659\n",
            "Epoch [22/100], Batch [124/240],  Cross Loss: 0.1681\n",
            "124/240, Train_loss: 0.1681 Train_dice: 0.1681\n",
            "Epoch [22/100], Batch [125/240],  Cross Loss: 0.0015\n",
            "125/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [22/100], Batch [126/240],  Cross Loss: 0.0016\n",
            "126/240, Train_loss: 0.0016 Train_dice: 0.0016\n",
            "Epoch [22/100], Batch [127/240],  Cross Loss: 0.0373\n",
            "127/240, Train_loss: 0.0373 Train_dice: 0.0373\n",
            "Epoch [22/100], Batch [128/240],  Cross Loss: 0.1290\n",
            "128/240, Train_loss: 0.1290 Train_dice: 0.1290\n",
            "Epoch [22/100], Batch [129/240],  Cross Loss: 0.0608\n",
            "129/240, Train_loss: 0.0608 Train_dice: 0.0608\n",
            "Epoch [22/100], Batch [130/240],  Cross Loss: 0.6288\n",
            "130/240, Train_loss: 0.6288 Train_dice: 0.6288\n",
            "Epoch [22/100], Batch [131/240],  Cross Loss: 0.0291\n",
            "131/240, Train_loss: 0.0291 Train_dice: 0.0291\n",
            "Epoch [22/100], Batch [132/240],  Cross Loss: 0.3824\n",
            "132/240, Train_loss: 0.3824 Train_dice: 0.3824\n",
            "Epoch [22/100], Batch [133/240],  Cross Loss: 0.0034\n",
            "133/240, Train_loss: 0.0034 Train_dice: 0.0034\n",
            "Epoch [22/100], Batch [134/240],  Cross Loss: 0.0381\n",
            "134/240, Train_loss: 0.0381 Train_dice: 0.0381\n",
            "Epoch [22/100], Batch [135/240],  Cross Loss: 0.1403\n",
            "135/240, Train_loss: 0.1403 Train_dice: 0.1403\n",
            "Epoch [22/100], Batch [136/240],  Cross Loss: 0.0017\n",
            "136/240, Train_loss: 0.0017 Train_dice: 0.0017\n",
            "Epoch [22/100], Batch [137/240],  Cross Loss: 0.0405\n",
            "137/240, Train_loss: 0.0405 Train_dice: 0.0405\n",
            "Epoch [22/100], Batch [138/240],  Cross Loss: 0.0097\n",
            "138/240, Train_loss: 0.0097 Train_dice: 0.0097\n",
            "Epoch [22/100], Batch [139/240],  Cross Loss: 0.2269\n",
            "139/240, Train_loss: 0.2269 Train_dice: 0.2269\n",
            "Epoch [22/100], Batch [140/240],  Cross Loss: 0.0013\n",
            "140/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [22/100], Batch [141/240],  Cross Loss: 0.0025\n",
            "141/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [22/100], Batch [142/240],  Cross Loss: 0.0418\n",
            "142/240, Train_loss: 0.0418 Train_dice: 0.0418\n",
            "Epoch [22/100], Batch [143/240],  Cross Loss: 0.0061\n",
            "143/240, Train_loss: 0.0061 Train_dice: 0.0061\n",
            "Epoch [22/100], Batch [144/240],  Cross Loss: 0.1918\n",
            "144/240, Train_loss: 0.1918 Train_dice: 0.1918\n",
            "Epoch [22/100], Batch [145/240],  Cross Loss: 0.0170\n",
            "145/240, Train_loss: 0.0170 Train_dice: 0.0170\n",
            "Epoch [22/100], Batch [146/240],  Cross Loss: 0.0179\n",
            "146/240, Train_loss: 0.0179 Train_dice: 0.0179\n",
            "Epoch [22/100], Batch [147/240],  Cross Loss: 0.0235\n",
            "147/240, Train_loss: 0.0235 Train_dice: 0.0235\n",
            "Epoch [22/100], Batch [148/240],  Cross Loss: 0.0030\n",
            "148/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [22/100], Batch [149/240],  Cross Loss: 0.0040\n",
            "149/240, Train_loss: 0.0040 Train_dice: 0.0040\n",
            "Epoch [22/100], Batch [150/240],  Cross Loss: 0.3438\n",
            "150/240, Train_loss: 0.3438 Train_dice: 0.3438\n",
            "Epoch [22/100], Batch [151/240],  Cross Loss: 0.0120\n",
            "151/240, Train_loss: 0.0120 Train_dice: 0.0120\n",
            "Epoch [22/100], Batch [152/240],  Cross Loss: 0.1348\n",
            "152/240, Train_loss: 0.1348 Train_dice: 0.1348\n",
            "Epoch [22/100], Batch [153/240],  Cross Loss: 0.4339\n",
            "153/240, Train_loss: 0.4339 Train_dice: 0.4339\n",
            "Epoch [22/100], Batch [154/240],  Cross Loss: 0.0143\n",
            "154/240, Train_loss: 0.0143 Train_dice: 0.0143\n",
            "Epoch [22/100], Batch [155/240],  Cross Loss: 0.4349\n",
            "155/240, Train_loss: 0.4349 Train_dice: 0.4349\n",
            "Epoch [22/100], Batch [156/240],  Cross Loss: 0.0047\n",
            "156/240, Train_loss: 0.0047 Train_dice: 0.0047\n",
            "Epoch [22/100], Batch [157/240],  Cross Loss: 0.0012\n",
            "157/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [22/100], Batch [158/240],  Cross Loss: 0.0072\n",
            "158/240, Train_loss: 0.0072 Train_dice: 0.0072\n",
            "Epoch [22/100], Batch [159/240],  Cross Loss: 0.0289\n",
            "159/240, Train_loss: 0.0289 Train_dice: 0.0289\n",
            "Epoch [22/100], Batch [160/240],  Cross Loss: 0.0166\n",
            "160/240, Train_loss: 0.0166 Train_dice: 0.0166\n",
            "Epoch [22/100], Batch [161/240],  Cross Loss: 0.0190\n",
            "161/240, Train_loss: 0.0190 Train_dice: 0.0190\n",
            "Epoch [22/100], Batch [162/240],  Cross Loss: 0.0115\n",
            "162/240, Train_loss: 0.0115 Train_dice: 0.0115\n",
            "Epoch [22/100], Batch [163/240],  Cross Loss: 0.0142\n",
            "163/240, Train_loss: 0.0142 Train_dice: 0.0142\n",
            "Epoch [22/100], Batch [164/240],  Cross Loss: 0.0975\n",
            "164/240, Train_loss: 0.0975 Train_dice: 0.0975\n",
            "Epoch [22/100], Batch [165/240],  Cross Loss: 0.0766\n",
            "165/240, Train_loss: 0.0766 Train_dice: 0.0766\n",
            "Epoch [22/100], Batch [166/240],  Cross Loss: 0.0201\n",
            "166/240, Train_loss: 0.0201 Train_dice: 0.0201\n",
            "Epoch [22/100], Batch [167/240],  Cross Loss: 0.0228\n",
            "167/240, Train_loss: 0.0228 Train_dice: 0.0228\n",
            "Epoch [22/100], Batch [168/240],  Cross Loss: 0.5592\n",
            "168/240, Train_loss: 0.5592 Train_dice: 0.5592\n",
            "Epoch [22/100], Batch [169/240],  Cross Loss: 0.0075\n",
            "169/240, Train_loss: 0.0075 Train_dice: 0.0075\n",
            "Epoch [22/100], Batch [170/240],  Cross Loss: 0.0436\n",
            "170/240, Train_loss: 0.0436 Train_dice: 0.0436\n",
            "Epoch [22/100], Batch [171/240],  Cross Loss: 0.0556\n",
            "171/240, Train_loss: 0.0556 Train_dice: 0.0556\n",
            "Epoch [22/100], Batch [172/240],  Cross Loss: 0.0060\n",
            "172/240, Train_loss: 0.0060 Train_dice: 0.0060\n",
            "Epoch [22/100], Batch [173/240],  Cross Loss: 0.0358\n",
            "173/240, Train_loss: 0.0358 Train_dice: 0.0358\n",
            "Epoch [22/100], Batch [174/240],  Cross Loss: 0.0045\n",
            "174/240, Train_loss: 0.0045 Train_dice: 0.0045\n",
            "Epoch [22/100], Batch [175/240],  Cross Loss: 0.1941\n",
            "175/240, Train_loss: 0.1941 Train_dice: 0.1941\n",
            "Epoch [22/100], Batch [176/240],  Cross Loss: 0.1540\n",
            "176/240, Train_loss: 0.1540 Train_dice: 0.1540\n",
            "Epoch [22/100], Batch [177/240],  Cross Loss: 0.0079\n",
            "177/240, Train_loss: 0.0079 Train_dice: 0.0079\n",
            "Epoch [22/100], Batch [178/240],  Cross Loss: 0.2795\n",
            "178/240, Train_loss: 0.2795 Train_dice: 0.2795\n",
            "Epoch [22/100], Batch [179/240],  Cross Loss: 0.0027\n",
            "179/240, Train_loss: 0.0027 Train_dice: 0.0027\n",
            "Epoch [22/100], Batch [180/240],  Cross Loss: 0.0052\n",
            "180/240, Train_loss: 0.0052 Train_dice: 0.0052\n",
            "Epoch [22/100], Batch [181/240],  Cross Loss: 0.1523\n",
            "181/240, Train_loss: 0.1523 Train_dice: 0.1523\n",
            "Epoch [22/100], Batch [182/240],  Cross Loss: 0.0018\n",
            "182/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [22/100], Batch [183/240],  Cross Loss: 0.0122\n",
            "183/240, Train_loss: 0.0122 Train_dice: 0.0122\n",
            "Epoch [22/100], Batch [184/240],  Cross Loss: 0.0058\n",
            "184/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [22/100], Batch [185/240],  Cross Loss: 0.0277\n",
            "185/240, Train_loss: 0.0277 Train_dice: 0.0277\n",
            "Epoch [22/100], Batch [186/240],  Cross Loss: 0.0031\n",
            "186/240, Train_loss: 0.0031 Train_dice: 0.0031\n",
            "Epoch [22/100], Batch [187/240],  Cross Loss: 0.0625\n",
            "187/240, Train_loss: 0.0625 Train_dice: 0.0625\n",
            "Epoch [22/100], Batch [188/240],  Cross Loss: 0.0214\n",
            "188/240, Train_loss: 0.0214 Train_dice: 0.0214\n",
            "Epoch [22/100], Batch [189/240],  Cross Loss: 0.0072\n",
            "189/240, Train_loss: 0.0072 Train_dice: 0.0072\n",
            "Epoch [22/100], Batch [190/240],  Cross Loss: 0.0023\n",
            "190/240, Train_loss: 0.0023 Train_dice: 0.0023\n",
            "Epoch [22/100], Batch [191/240],  Cross Loss: 0.0103\n",
            "191/240, Train_loss: 0.0103 Train_dice: 0.0103\n",
            "Epoch [22/100], Batch [192/240],  Cross Loss: 0.0126\n",
            "192/240, Train_loss: 0.0126 Train_dice: 0.0126\n",
            "Epoch [22/100], Batch [193/240],  Cross Loss: 0.0052\n",
            "193/240, Train_loss: 0.0052 Train_dice: 0.0052\n",
            "Epoch [22/100], Batch [194/240],  Cross Loss: 0.0087\n",
            "194/240, Train_loss: 0.0087 Train_dice: 0.0087\n",
            "Epoch [22/100], Batch [195/240],  Cross Loss: 0.0807\n",
            "195/240, Train_loss: 0.0807 Train_dice: 0.0807\n",
            "Epoch [22/100], Batch [196/240],  Cross Loss: 0.0022\n",
            "196/240, Train_loss: 0.0022 Train_dice: 0.0022\n",
            "Epoch [22/100], Batch [197/240],  Cross Loss: 0.0403\n",
            "197/240, Train_loss: 0.0403 Train_dice: 0.0403\n",
            "Epoch [22/100], Batch [198/240],  Cross Loss: 0.0087\n",
            "198/240, Train_loss: 0.0087 Train_dice: 0.0087\n",
            "Epoch [22/100], Batch [199/240],  Cross Loss: 0.0195\n",
            "199/240, Train_loss: 0.0195 Train_dice: 0.0195\n",
            "Epoch [22/100], Batch [200/240],  Cross Loss: 0.0472\n",
            "200/240, Train_loss: 0.0472 Train_dice: 0.0472\n",
            "Epoch [22/100], Batch [201/240],  Cross Loss: 0.0119\n",
            "201/240, Train_loss: 0.0119 Train_dice: 0.0119\n",
            "Epoch [22/100], Batch [202/240],  Cross Loss: 0.0258\n",
            "202/240, Train_loss: 0.0258 Train_dice: 0.0258\n",
            "Epoch [22/100], Batch [203/240],  Cross Loss: 0.0004\n",
            "203/240, Train_loss: 0.0004 Train_dice: 0.0004\n",
            "Epoch [22/100], Batch [204/240],  Cross Loss: 0.0653\n",
            "204/240, Train_loss: 0.0653 Train_dice: 0.0653\n",
            "Epoch [22/100], Batch [205/240],  Cross Loss: 0.3790\n",
            "205/240, Train_loss: 0.3790 Train_dice: 0.3790\n",
            "Epoch [22/100], Batch [206/240],  Cross Loss: 0.0153\n",
            "206/240, Train_loss: 0.0153 Train_dice: 0.0153\n",
            "Epoch [22/100], Batch [207/240],  Cross Loss: 0.0171\n",
            "207/240, Train_loss: 0.0171 Train_dice: 0.0171\n",
            "Epoch [22/100], Batch [208/240],  Cross Loss: 0.0101\n",
            "208/240, Train_loss: 0.0101 Train_dice: 0.0101\n",
            "Epoch [22/100], Batch [209/240],  Cross Loss: 0.0497\n",
            "209/240, Train_loss: 0.0497 Train_dice: 0.0497\n",
            "Epoch [22/100], Batch [210/240],  Cross Loss: 0.1422\n",
            "210/240, Train_loss: 0.1422 Train_dice: 0.1422\n",
            "Epoch [22/100], Batch [211/240],  Cross Loss: 0.0312\n",
            "211/240, Train_loss: 0.0312 Train_dice: 0.0312\n",
            "Epoch [22/100], Batch [212/240],  Cross Loss: 0.0010\n",
            "212/240, Train_loss: 0.0010 Train_dice: 0.0010\n",
            "Epoch [22/100], Batch [213/240],  Cross Loss: 0.0122\n",
            "213/240, Train_loss: 0.0122 Train_dice: 0.0122\n",
            "Epoch [22/100], Batch [214/240],  Cross Loss: 0.2651\n",
            "214/240, Train_loss: 0.2651 Train_dice: 0.2651\n",
            "Epoch [22/100], Batch [215/240],  Cross Loss: 0.0064\n",
            "215/240, Train_loss: 0.0064 Train_dice: 0.0064\n",
            "Epoch [22/100], Batch [216/240],  Cross Loss: 0.0106\n",
            "216/240, Train_loss: 0.0106 Train_dice: 0.0106\n",
            "Epoch [22/100], Batch [217/240],  Cross Loss: 0.3225\n",
            "217/240, Train_loss: 0.3225 Train_dice: 0.3225\n",
            "Epoch [22/100], Batch [218/240],  Cross Loss: 0.0024\n",
            "218/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [22/100], Batch [219/240],  Cross Loss: 0.0084\n",
            "219/240, Train_loss: 0.0084 Train_dice: 0.0084\n",
            "Epoch [22/100], Batch [220/240],  Cross Loss: 0.0188\n",
            "220/240, Train_loss: 0.0188 Train_dice: 0.0188\n",
            "Epoch [22/100], Batch [221/240],  Cross Loss: 0.0034\n",
            "221/240, Train_loss: 0.0034 Train_dice: 0.0034\n",
            "Epoch [22/100], Batch [222/240],  Cross Loss: 0.1622\n",
            "222/240, Train_loss: 0.1622 Train_dice: 0.1622\n",
            "Epoch [22/100], Batch [223/240],  Cross Loss: 0.0288\n",
            "223/240, Train_loss: 0.0288 Train_dice: 0.0288\n",
            "Epoch [22/100], Batch [224/240],  Cross Loss: 0.0925\n",
            "224/240, Train_loss: 0.0925 Train_dice: 0.0925\n",
            "Epoch [22/100], Batch [225/240],  Cross Loss: 0.0031\n",
            "225/240, Train_loss: 0.0031 Train_dice: 0.0031\n",
            "Epoch [22/100], Batch [226/240],  Cross Loss: 0.0034\n",
            "226/240, Train_loss: 0.0034 Train_dice: 0.0034\n",
            "Epoch [22/100], Batch [227/240],  Cross Loss: 0.1714\n",
            "227/240, Train_loss: 0.1714 Train_dice: 0.1714\n",
            "Epoch [22/100], Batch [228/240],  Cross Loss: 0.0119\n",
            "228/240, Train_loss: 0.0119 Train_dice: 0.0119\n",
            "Epoch [22/100], Batch [229/240],  Cross Loss: 0.4389\n",
            "229/240, Train_loss: 0.4389 Train_dice: 0.4389\n",
            "Epoch [22/100], Batch [230/240],  Cross Loss: 0.0190\n",
            "230/240, Train_loss: 0.0190 Train_dice: 0.0190\n",
            "Epoch [22/100], Batch [231/240],  Cross Loss: 0.1542\n",
            "231/240, Train_loss: 0.1542 Train_dice: 0.1542\n",
            "Epoch [22/100], Batch [232/240],  Cross Loss: 0.0012\n",
            "232/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [22/100], Batch [233/240],  Cross Loss: 0.6094\n",
            "233/240, Train_loss: 0.6094 Train_dice: 0.6094\n",
            "Epoch [22/100], Batch [234/240],  Cross Loss: 0.0133\n",
            "234/240, Train_loss: 0.0133 Train_dice: 0.0133\n",
            "Epoch [22/100], Batch [235/240],  Cross Loss: 0.0191\n",
            "235/240, Train_loss: 0.0191 Train_dice: 0.0191\n",
            "Epoch [22/100], Batch [236/240],  Cross Loss: 0.0028\n",
            "236/240, Train_loss: 0.0028 Train_dice: 0.0028\n",
            "Epoch [22/100], Batch [237/240],  Cross Loss: 0.2848\n",
            "237/240, Train_loss: 0.2848 Train_dice: 0.2848\n",
            "Epoch [22/100], Batch [238/240],  Cross Loss: 0.0632\n",
            "238/240, Train_loss: 0.0632 Train_dice: 0.0632\n",
            "Epoch [22/100], Batch [239/240],  Cross Loss: 0.0386\n",
            "239/240, Train_loss: 0.0386 Train_dice: 0.0386\n",
            "Epoch [22/100], Batch [240/240],  Cross Loss: 0.0049\n",
            "240/240, Train_loss: 0.0049 Train_dice: 0.0049\n",
            "--------------------\n",
            "Epoch_loss: 0.0896\n",
            "Epoch_metric: tensor(0.0896, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.4947\n",
            "test_dice_epoch: tensor(0.4947, device='cuda:0')\n",
            "current epoch: 22 current mean dice: tensor(0.4947, device='cuda:0')\n",
            "best mean dice: tensor(0.4947, device='cuda:0') at epoch: 22\n",
            "----------\n",
            "epoch 23/100\n",
            "Epoch [23/100], Batch [1/240],  Cross Loss: 0.0058\n",
            "1/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [23/100], Batch [2/240],  Cross Loss: 0.4296\n",
            "2/240, Train_loss: 0.4296 Train_dice: 0.4296\n",
            "Epoch [23/100], Batch [3/240],  Cross Loss: 0.0021\n",
            "3/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [23/100], Batch [4/240],  Cross Loss: 0.0608\n",
            "4/240, Train_loss: 0.0608 Train_dice: 0.0608\n",
            "Epoch [23/100], Batch [5/240],  Cross Loss: 0.7442\n",
            "5/240, Train_loss: 0.7442 Train_dice: 0.7442\n",
            "Epoch [23/100], Batch [6/240],  Cross Loss: 0.3567\n",
            "6/240, Train_loss: 0.3567 Train_dice: 0.3567\n",
            "Epoch [23/100], Batch [7/240],  Cross Loss: 0.5014\n",
            "7/240, Train_loss: 0.5014 Train_dice: 0.5014\n",
            "Epoch [23/100], Batch [8/240],  Cross Loss: 0.0075\n",
            "8/240, Train_loss: 0.0075 Train_dice: 0.0075\n",
            "Epoch [23/100], Batch [9/240],  Cross Loss: 0.0100\n",
            "9/240, Train_loss: 0.0100 Train_dice: 0.0100\n",
            "Epoch [23/100], Batch [10/240],  Cross Loss: 0.1783\n",
            "10/240, Train_loss: 0.1783 Train_dice: 0.1783\n",
            "Epoch [23/100], Batch [11/240],  Cross Loss: 0.0148\n",
            "11/240, Train_loss: 0.0148 Train_dice: 0.0148\n",
            "Epoch [23/100], Batch [12/240],  Cross Loss: 0.0095\n",
            "12/240, Train_loss: 0.0095 Train_dice: 0.0095\n",
            "Epoch [23/100], Batch [13/240],  Cross Loss: 0.0037\n",
            "13/240, Train_loss: 0.0037 Train_dice: 0.0037\n",
            "Epoch [23/100], Batch [14/240],  Cross Loss: 0.5598\n",
            "14/240, Train_loss: 0.5598 Train_dice: 0.5598\n",
            "Epoch [23/100], Batch [15/240],  Cross Loss: 0.0093\n",
            "15/240, Train_loss: 0.0093 Train_dice: 0.0093\n",
            "Epoch [23/100], Batch [16/240],  Cross Loss: 0.0054\n",
            "16/240, Train_loss: 0.0054 Train_dice: 0.0054\n",
            "Epoch [23/100], Batch [17/240],  Cross Loss: 0.0026\n",
            "17/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [23/100], Batch [18/240],  Cross Loss: 0.0234\n",
            "18/240, Train_loss: 0.0234 Train_dice: 0.0234\n",
            "Epoch [23/100], Batch [19/240],  Cross Loss: 0.0264\n",
            "19/240, Train_loss: 0.0264 Train_dice: 0.0264\n",
            "Epoch [23/100], Batch [20/240],  Cross Loss: 0.0074\n",
            "20/240, Train_loss: 0.0074 Train_dice: 0.0074\n",
            "Epoch [23/100], Batch [21/240],  Cross Loss: 0.0010\n",
            "21/240, Train_loss: 0.0010 Train_dice: 0.0010\n",
            "Epoch [23/100], Batch [22/240],  Cross Loss: 0.1457\n",
            "22/240, Train_loss: 0.1457 Train_dice: 0.1457\n",
            "Epoch [23/100], Batch [23/240],  Cross Loss: 0.0103\n",
            "23/240, Train_loss: 0.0103 Train_dice: 0.0103\n",
            "Epoch [23/100], Batch [24/240],  Cross Loss: 0.1030\n",
            "24/240, Train_loss: 0.1030 Train_dice: 0.1030\n",
            "Epoch [23/100], Batch [25/240],  Cross Loss: 0.0238\n",
            "25/240, Train_loss: 0.0238 Train_dice: 0.0238\n",
            "Epoch [23/100], Batch [26/240],  Cross Loss: 0.1086\n",
            "26/240, Train_loss: 0.1086 Train_dice: 0.1086\n",
            "Epoch [23/100], Batch [27/240],  Cross Loss: 0.1114\n",
            "27/240, Train_loss: 0.1114 Train_dice: 0.1114\n",
            "Epoch [23/100], Batch [28/240],  Cross Loss: 0.0063\n",
            "28/240, Train_loss: 0.0063 Train_dice: 0.0063\n",
            "Epoch [23/100], Batch [29/240],  Cross Loss: 0.0632\n",
            "29/240, Train_loss: 0.0632 Train_dice: 0.0632\n",
            "Epoch [23/100], Batch [30/240],  Cross Loss: 0.0234\n",
            "30/240, Train_loss: 0.0234 Train_dice: 0.0234\n",
            "Epoch [23/100], Batch [31/240],  Cross Loss: 0.2497\n",
            "31/240, Train_loss: 0.2497 Train_dice: 0.2497\n",
            "Epoch [23/100], Batch [32/240],  Cross Loss: 0.5402\n",
            "32/240, Train_loss: 0.5402 Train_dice: 0.5402\n",
            "Epoch [23/100], Batch [33/240],  Cross Loss: 0.0066\n",
            "33/240, Train_loss: 0.0066 Train_dice: 0.0066\n",
            "Epoch [23/100], Batch [34/240],  Cross Loss: 0.0007\n",
            "34/240, Train_loss: 0.0007 Train_dice: 0.0007\n",
            "Epoch [23/100], Batch [35/240],  Cross Loss: 0.0421\n",
            "35/240, Train_loss: 0.0421 Train_dice: 0.0421\n",
            "Epoch [23/100], Batch [36/240],  Cross Loss: 0.0023\n",
            "36/240, Train_loss: 0.0023 Train_dice: 0.0023\n",
            "Epoch [23/100], Batch [37/240],  Cross Loss: 0.3382\n",
            "37/240, Train_loss: 0.3382 Train_dice: 0.3382\n",
            "Epoch [23/100], Batch [38/240],  Cross Loss: 0.0009\n",
            "38/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [23/100], Batch [39/240],  Cross Loss: 0.0042\n",
            "39/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [23/100], Batch [40/240],  Cross Loss: 0.6871\n",
            "40/240, Train_loss: 0.6871 Train_dice: 0.6871\n",
            "Epoch [23/100], Batch [41/240],  Cross Loss: 0.0397\n",
            "41/240, Train_loss: 0.0397 Train_dice: 0.0397\n",
            "Epoch [23/100], Batch [42/240],  Cross Loss: 0.2617\n",
            "42/240, Train_loss: 0.2617 Train_dice: 0.2617\n",
            "Epoch [23/100], Batch [43/240],  Cross Loss: 0.0139\n",
            "43/240, Train_loss: 0.0139 Train_dice: 0.0139\n",
            "Epoch [23/100], Batch [44/240],  Cross Loss: 0.1376\n",
            "44/240, Train_loss: 0.1376 Train_dice: 0.1376\n",
            "Epoch [23/100], Batch [45/240],  Cross Loss: 0.3521\n",
            "45/240, Train_loss: 0.3521 Train_dice: 0.3521\n",
            "Epoch [23/100], Batch [46/240],  Cross Loss: 0.0415\n",
            "46/240, Train_loss: 0.0415 Train_dice: 0.0415\n",
            "Epoch [23/100], Batch [47/240],  Cross Loss: 0.0029\n",
            "47/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [23/100], Batch [48/240],  Cross Loss: 0.5993\n",
            "48/240, Train_loss: 0.5993 Train_dice: 0.5993\n",
            "Epoch [23/100], Batch [49/240],  Cross Loss: 0.0260\n",
            "49/240, Train_loss: 0.0260 Train_dice: 0.0260\n",
            "Epoch [23/100], Batch [50/240],  Cross Loss: 0.3376\n",
            "50/240, Train_loss: 0.3376 Train_dice: 0.3376\n",
            "Epoch [23/100], Batch [51/240],  Cross Loss: 0.0038\n",
            "51/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [23/100], Batch [52/240],  Cross Loss: 0.0023\n",
            "52/240, Train_loss: 0.0023 Train_dice: 0.0023\n",
            "Epoch [23/100], Batch [53/240],  Cross Loss: 0.0841\n",
            "53/240, Train_loss: 0.0841 Train_dice: 0.0841\n",
            "Epoch [23/100], Batch [54/240],  Cross Loss: 0.0054\n",
            "54/240, Train_loss: 0.0054 Train_dice: 0.0054\n",
            "Epoch [23/100], Batch [55/240],  Cross Loss: 0.2319\n",
            "55/240, Train_loss: 0.2319 Train_dice: 0.2319\n",
            "Epoch [23/100], Batch [56/240],  Cross Loss: 0.0586\n",
            "56/240, Train_loss: 0.0586 Train_dice: 0.0586\n",
            "Epoch [23/100], Batch [57/240],  Cross Loss: 0.0737\n",
            "57/240, Train_loss: 0.0737 Train_dice: 0.0737\n",
            "Epoch [23/100], Batch [58/240],  Cross Loss: 0.5963\n",
            "58/240, Train_loss: 0.5963 Train_dice: 0.5963\n",
            "Epoch [23/100], Batch [59/240],  Cross Loss: 0.0036\n",
            "59/240, Train_loss: 0.0036 Train_dice: 0.0036\n",
            "Epoch [23/100], Batch [60/240],  Cross Loss: 0.0170\n",
            "60/240, Train_loss: 0.0170 Train_dice: 0.0170\n",
            "Epoch [23/100], Batch [61/240],  Cross Loss: 0.6452\n",
            "61/240, Train_loss: 0.6452 Train_dice: 0.6452\n",
            "Epoch [23/100], Batch [62/240],  Cross Loss: 0.0058\n",
            "62/240, Train_loss: 0.0058 Train_dice: 0.0058\n",
            "Epoch [23/100], Batch [63/240],  Cross Loss: 0.0770\n",
            "63/240, Train_loss: 0.0770 Train_dice: 0.0770\n",
            "Epoch [23/100], Batch [64/240],  Cross Loss: 0.0021\n",
            "64/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [23/100], Batch [65/240],  Cross Loss: 0.0374\n",
            "65/240, Train_loss: 0.0374 Train_dice: 0.0374\n",
            "Epoch [23/100], Batch [66/240],  Cross Loss: 0.0686\n",
            "66/240, Train_loss: 0.0686 Train_dice: 0.0686\n",
            "Epoch [23/100], Batch [67/240],  Cross Loss: 0.0009\n",
            "67/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [23/100], Batch [68/240],  Cross Loss: 0.2323\n",
            "68/240, Train_loss: 0.2323 Train_dice: 0.2323\n",
            "Epoch [23/100], Batch [69/240],  Cross Loss: 0.0021\n",
            "69/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [23/100], Batch [70/240],  Cross Loss: 0.0430\n",
            "70/240, Train_loss: 0.0430 Train_dice: 0.0430\n",
            "Epoch [23/100], Batch [71/240],  Cross Loss: 0.3083\n",
            "71/240, Train_loss: 0.3083 Train_dice: 0.3083\n",
            "Epoch [23/100], Batch [72/240],  Cross Loss: 0.0099\n",
            "72/240, Train_loss: 0.0099 Train_dice: 0.0099\n",
            "Epoch [23/100], Batch [73/240],  Cross Loss: 0.0047\n",
            "73/240, Train_loss: 0.0047 Train_dice: 0.0047\n",
            "Epoch [23/100], Batch [74/240],  Cross Loss: 0.4966\n",
            "74/240, Train_loss: 0.4966 Train_dice: 0.4966\n",
            "Epoch [23/100], Batch [75/240],  Cross Loss: 0.0025\n",
            "75/240, Train_loss: 0.0025 Train_dice: 0.0025\n",
            "Epoch [23/100], Batch [76/240],  Cross Loss: 0.0039\n",
            "76/240, Train_loss: 0.0039 Train_dice: 0.0039\n",
            "Epoch [23/100], Batch [77/240],  Cross Loss: 0.0235\n",
            "77/240, Train_loss: 0.0235 Train_dice: 0.0235\n",
            "Epoch [23/100], Batch [78/240],  Cross Loss: 0.0258\n",
            "78/240, Train_loss: 0.0258 Train_dice: 0.0258\n",
            "Epoch [23/100], Batch [79/240],  Cross Loss: 0.0020\n",
            "79/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [23/100], Batch [80/240],  Cross Loss: 0.0287\n",
            "80/240, Train_loss: 0.0287 Train_dice: 0.0287\n",
            "Epoch [23/100], Batch [81/240],  Cross Loss: 0.0875\n",
            "81/240, Train_loss: 0.0875 Train_dice: 0.0875\n",
            "Epoch [23/100], Batch [82/240],  Cross Loss: 0.0173\n",
            "82/240, Train_loss: 0.0173 Train_dice: 0.0173\n",
            "Epoch [23/100], Batch [83/240],  Cross Loss: 0.0011\n",
            "83/240, Train_loss: 0.0011 Train_dice: 0.0011\n",
            "Epoch [23/100], Batch [84/240],  Cross Loss: 0.0005\n",
            "84/240, Train_loss: 0.0005 Train_dice: 0.0005\n",
            "Epoch [23/100], Batch [85/240],  Cross Loss: 0.3211\n",
            "85/240, Train_loss: 0.3211 Train_dice: 0.3211\n",
            "Epoch [23/100], Batch [86/240],  Cross Loss: 0.0066\n",
            "86/240, Train_loss: 0.0066 Train_dice: 0.0066\n",
            "Epoch [23/100], Batch [87/240],  Cross Loss: 0.0502\n",
            "87/240, Train_loss: 0.0502 Train_dice: 0.0502\n",
            "Epoch [23/100], Batch [88/240],  Cross Loss: 0.0073\n",
            "88/240, Train_loss: 0.0073 Train_dice: 0.0073\n",
            "Epoch [23/100], Batch [89/240],  Cross Loss: 0.0003\n",
            "89/240, Train_loss: 0.0003 Train_dice: 0.0003\n",
            "Epoch [23/100], Batch [90/240],  Cross Loss: 0.1677\n",
            "90/240, Train_loss: 0.1677 Train_dice: 0.1677\n",
            "Epoch [23/100], Batch [91/240],  Cross Loss: 0.0173\n",
            "91/240, Train_loss: 0.0173 Train_dice: 0.0173\n",
            "Epoch [23/100], Batch [92/240],  Cross Loss: 0.4407\n",
            "92/240, Train_loss: 0.4407 Train_dice: 0.4407\n",
            "Epoch [23/100], Batch [93/240],  Cross Loss: 0.0531\n",
            "93/240, Train_loss: 0.0531 Train_dice: 0.0531\n",
            "Epoch [23/100], Batch [94/240],  Cross Loss: 0.0547\n",
            "94/240, Train_loss: 0.0547 Train_dice: 0.0547\n",
            "Epoch [23/100], Batch [95/240],  Cross Loss: 0.2823\n",
            "95/240, Train_loss: 0.2823 Train_dice: 0.2823\n",
            "Epoch [23/100], Batch [96/240],  Cross Loss: 0.0005\n",
            "96/240, Train_loss: 0.0005 Train_dice: 0.0005\n",
            "Epoch [23/100], Batch [97/240],  Cross Loss: 0.0984\n",
            "97/240, Train_loss: 0.0984 Train_dice: 0.0984\n",
            "Epoch [23/100], Batch [98/240],  Cross Loss: 0.0167\n",
            "98/240, Train_loss: 0.0167 Train_dice: 0.0167\n",
            "Epoch [23/100], Batch [99/240],  Cross Loss: 0.0090\n",
            "99/240, Train_loss: 0.0090 Train_dice: 0.0090\n",
            "Epoch [23/100], Batch [100/240],  Cross Loss: 0.2040\n",
            "100/240, Train_loss: 0.2040 Train_dice: 0.2040\n",
            "Epoch [23/100], Batch [101/240],  Cross Loss: 0.1036\n",
            "101/240, Train_loss: 0.1036 Train_dice: 0.1036\n",
            "Epoch [23/100], Batch [102/240],  Cross Loss: 0.0029\n",
            "102/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [23/100], Batch [103/240],  Cross Loss: 0.0212\n",
            "103/240, Train_loss: 0.0212 Train_dice: 0.0212\n",
            "Epoch [23/100], Batch [104/240],  Cross Loss: 0.0039\n",
            "104/240, Train_loss: 0.0039 Train_dice: 0.0039\n",
            "Epoch [23/100], Batch [105/240],  Cross Loss: 0.1797\n",
            "105/240, Train_loss: 0.1797 Train_dice: 0.1797\n",
            "Epoch [23/100], Batch [106/240],  Cross Loss: 0.0373\n",
            "106/240, Train_loss: 0.0373 Train_dice: 0.0373\n",
            "Epoch [23/100], Batch [107/240],  Cross Loss: 0.0133\n",
            "107/240, Train_loss: 0.0133 Train_dice: 0.0133\n",
            "Epoch [23/100], Batch [108/240],  Cross Loss: 0.2672\n",
            "108/240, Train_loss: 0.2672 Train_dice: 0.2672\n",
            "Epoch [23/100], Batch [109/240],  Cross Loss: 0.0219\n",
            "109/240, Train_loss: 0.0219 Train_dice: 0.0219\n",
            "Epoch [23/100], Batch [110/240],  Cross Loss: 0.0009\n",
            "110/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [23/100], Batch [111/240],  Cross Loss: 0.0644\n",
            "111/240, Train_loss: 0.0644 Train_dice: 0.0644\n",
            "Epoch [23/100], Batch [112/240],  Cross Loss: 0.1287\n",
            "112/240, Train_loss: 0.1287 Train_dice: 0.1287\n",
            "Epoch [23/100], Batch [113/240],  Cross Loss: 0.0242\n",
            "113/240, Train_loss: 0.0242 Train_dice: 0.0242\n",
            "Epoch [23/100], Batch [114/240],  Cross Loss: 0.4841\n",
            "114/240, Train_loss: 0.4841 Train_dice: 0.4841\n",
            "Epoch [23/100], Batch [115/240],  Cross Loss: 0.1224\n",
            "115/240, Train_loss: 0.1224 Train_dice: 0.1224\n",
            "Epoch [23/100], Batch [116/240],  Cross Loss: 0.0121\n",
            "116/240, Train_loss: 0.0121 Train_dice: 0.0121\n",
            "Epoch [23/100], Batch [117/240],  Cross Loss: 0.0106\n",
            "117/240, Train_loss: 0.0106 Train_dice: 0.0106\n",
            "Epoch [23/100], Batch [118/240],  Cross Loss: 0.7443\n",
            "118/240, Train_loss: 0.7443 Train_dice: 0.7443\n",
            "Epoch [23/100], Batch [119/240],  Cross Loss: 0.0067\n",
            "119/240, Train_loss: 0.0067 Train_dice: 0.0067\n",
            "Epoch [23/100], Batch [120/240],  Cross Loss: 0.0353\n",
            "120/240, Train_loss: 0.0353 Train_dice: 0.0353\n",
            "Epoch [23/100], Batch [121/240],  Cross Loss: 0.0699\n",
            "121/240, Train_loss: 0.0699 Train_dice: 0.0699\n",
            "Epoch [23/100], Batch [122/240],  Cross Loss: 0.0886\n",
            "122/240, Train_loss: 0.0886 Train_dice: 0.0886\n",
            "Epoch [23/100], Batch [123/240],  Cross Loss: 0.0045\n",
            "123/240, Train_loss: 0.0045 Train_dice: 0.0045\n",
            "Epoch [23/100], Batch [124/240],  Cross Loss: 0.0016\n",
            "124/240, Train_loss: 0.0016 Train_dice: 0.0016\n",
            "Epoch [23/100], Batch [125/240],  Cross Loss: 0.0020\n",
            "125/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [23/100], Batch [126/240],  Cross Loss: 0.1031\n",
            "126/240, Train_loss: 0.1031 Train_dice: 0.1031\n",
            "Epoch [23/100], Batch [127/240],  Cross Loss: 0.0220\n",
            "127/240, Train_loss: 0.0220 Train_dice: 0.0220\n",
            "Epoch [23/100], Batch [128/240],  Cross Loss: 0.0419\n",
            "128/240, Train_loss: 0.0419 Train_dice: 0.0419\n",
            "Epoch [23/100], Batch [129/240],  Cross Loss: 0.0030\n",
            "129/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [23/100], Batch [130/240],  Cross Loss: 0.0013\n",
            "130/240, Train_loss: 0.0013 Train_dice: 0.0013\n",
            "Epoch [23/100], Batch [131/240],  Cross Loss: 0.0060\n",
            "131/240, Train_loss: 0.0060 Train_dice: 0.0060\n",
            "Epoch [23/100], Batch [132/240],  Cross Loss: 0.2301\n",
            "132/240, Train_loss: 0.2301 Train_dice: 0.2301\n",
            "Epoch [23/100], Batch [133/240],  Cross Loss: 0.2612\n",
            "133/240, Train_loss: 0.2612 Train_dice: 0.2612\n",
            "Epoch [23/100], Batch [134/240],  Cross Loss: 0.5295\n",
            "134/240, Train_loss: 0.5295 Train_dice: 0.5295\n",
            "Epoch [23/100], Batch [135/240],  Cross Loss: 0.0272\n",
            "135/240, Train_loss: 0.0272 Train_dice: 0.0272\n",
            "Epoch [23/100], Batch [136/240],  Cross Loss: 0.2625\n",
            "136/240, Train_loss: 0.2625 Train_dice: 0.2625\n",
            "Epoch [23/100], Batch [137/240],  Cross Loss: 0.6700\n",
            "137/240, Train_loss: 0.6700 Train_dice: 0.6700\n",
            "Epoch [23/100], Batch [138/240],  Cross Loss: 0.0043\n",
            "138/240, Train_loss: 0.0043 Train_dice: 0.0043\n",
            "Epoch [23/100], Batch [139/240],  Cross Loss: 0.0042\n",
            "139/240, Train_loss: 0.0042 Train_dice: 0.0042\n",
            "Epoch [23/100], Batch [140/240],  Cross Loss: 0.0367\n",
            "140/240, Train_loss: 0.0367 Train_dice: 0.0367\n",
            "Epoch [23/100], Batch [141/240],  Cross Loss: 0.0074\n",
            "141/240, Train_loss: 0.0074 Train_dice: 0.0074\n",
            "Epoch [23/100], Batch [142/240],  Cross Loss: 0.0999\n",
            "142/240, Train_loss: 0.0999 Train_dice: 0.0999\n",
            "Epoch [23/100], Batch [143/240],  Cross Loss: 0.0194\n",
            "143/240, Train_loss: 0.0194 Train_dice: 0.0194\n",
            "Epoch [23/100], Batch [144/240],  Cross Loss: 0.0056\n",
            "144/240, Train_loss: 0.0056 Train_dice: 0.0056\n",
            "Epoch [23/100], Batch [145/240],  Cross Loss: 0.0154\n",
            "145/240, Train_loss: 0.0154 Train_dice: 0.0154\n",
            "Epoch [23/100], Batch [146/240],  Cross Loss: 0.0008\n",
            "146/240, Train_loss: 0.0008 Train_dice: 0.0008\n",
            "Epoch [23/100], Batch [147/240],  Cross Loss: 0.0100\n",
            "147/240, Train_loss: 0.0100 Train_dice: 0.0100\n",
            "Epoch [23/100], Batch [148/240],  Cross Loss: 0.0132\n",
            "148/240, Train_loss: 0.0132 Train_dice: 0.0132\n",
            "Epoch [23/100], Batch [149/240],  Cross Loss: 0.0019\n",
            "149/240, Train_loss: 0.0019 Train_dice: 0.0019\n",
            "Epoch [23/100], Batch [150/240],  Cross Loss: 0.0040\n",
            "150/240, Train_loss: 0.0040 Train_dice: 0.0040\n",
            "Epoch [23/100], Batch [151/240],  Cross Loss: 0.0502\n",
            "151/240, Train_loss: 0.0502 Train_dice: 0.0502\n",
            "Epoch [23/100], Batch [152/240],  Cross Loss: 0.0239\n",
            "152/240, Train_loss: 0.0239 Train_dice: 0.0239\n",
            "Epoch [23/100], Batch [153/240],  Cross Loss: 0.0005\n",
            "153/240, Train_loss: 0.0005 Train_dice: 0.0005\n",
            "Epoch [23/100], Batch [154/240],  Cross Loss: 0.0731\n",
            "154/240, Train_loss: 0.0731 Train_dice: 0.0731\n",
            "Epoch [23/100], Batch [155/240],  Cross Loss: 0.0177\n",
            "155/240, Train_loss: 0.0177 Train_dice: 0.0177\n",
            "Epoch [23/100], Batch [156/240],  Cross Loss: 0.0004\n",
            "156/240, Train_loss: 0.0004 Train_dice: 0.0004\n",
            "Epoch [23/100], Batch [157/240],  Cross Loss: 0.0789\n",
            "157/240, Train_loss: 0.0789 Train_dice: 0.0789\n",
            "Epoch [23/100], Batch [158/240],  Cross Loss: 0.0652\n",
            "158/240, Train_loss: 0.0652 Train_dice: 0.0652\n",
            "Epoch [23/100], Batch [159/240],  Cross Loss: 0.2684\n",
            "159/240, Train_loss: 0.2684 Train_dice: 0.2684\n",
            "Epoch [23/100], Batch [160/240],  Cross Loss: 0.2274\n",
            "160/240, Train_loss: 0.2274 Train_dice: 0.2274\n",
            "Epoch [23/100], Batch [161/240],  Cross Loss: 0.0003\n",
            "161/240, Train_loss: 0.0003 Train_dice: 0.0003\n",
            "Epoch [23/100], Batch [162/240],  Cross Loss: 0.0018\n",
            "162/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [23/100], Batch [163/240],  Cross Loss: 0.0059\n",
            "163/240, Train_loss: 0.0059 Train_dice: 0.0059\n",
            "Epoch [23/100], Batch [164/240],  Cross Loss: 0.1624\n",
            "164/240, Train_loss: 0.1624 Train_dice: 0.1624\n",
            "Epoch [23/100], Batch [165/240],  Cross Loss: 0.0014\n",
            "165/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [23/100], Batch [166/240],  Cross Loss: 0.0026\n",
            "166/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [23/100], Batch [167/240],  Cross Loss: 0.0418\n",
            "167/240, Train_loss: 0.0418 Train_dice: 0.0418\n",
            "Epoch [23/100], Batch [168/240],  Cross Loss: 0.0014\n",
            "168/240, Train_loss: 0.0014 Train_dice: 0.0014\n",
            "Epoch [23/100], Batch [169/240],  Cross Loss: 0.4332\n",
            "169/240, Train_loss: 0.4332 Train_dice: 0.4332\n",
            "Epoch [23/100], Batch [170/240],  Cross Loss: 0.0005\n",
            "170/240, Train_loss: 0.0005 Train_dice: 0.0005\n",
            "Epoch [23/100], Batch [171/240],  Cross Loss: 0.1600\n",
            "171/240, Train_loss: 0.1600 Train_dice: 0.1600\n",
            "Epoch [23/100], Batch [172/240],  Cross Loss: 0.0140\n",
            "172/240, Train_loss: 0.0140 Train_dice: 0.0140\n",
            "Epoch [23/100], Batch [173/240],  Cross Loss: 0.0060\n",
            "173/240, Train_loss: 0.0060 Train_dice: 0.0060\n",
            "Epoch [23/100], Batch [174/240],  Cross Loss: 0.0365\n",
            "174/240, Train_loss: 0.0365 Train_dice: 0.0365\n",
            "Epoch [23/100], Batch [175/240],  Cross Loss: 0.0891\n",
            "175/240, Train_loss: 0.0891 Train_dice: 0.0891\n",
            "Epoch [23/100], Batch [176/240],  Cross Loss: 0.0375\n",
            "176/240, Train_loss: 0.0375 Train_dice: 0.0375\n",
            "Epoch [23/100], Batch [177/240],  Cross Loss: 0.0400\n",
            "177/240, Train_loss: 0.0400 Train_dice: 0.0400\n",
            "Epoch [23/100], Batch [178/240],  Cross Loss: 0.1380\n",
            "178/240, Train_loss: 0.1380 Train_dice: 0.1380\n",
            "Epoch [23/100], Batch [179/240],  Cross Loss: 0.7209\n",
            "179/240, Train_loss: 0.7209 Train_dice: 0.7209\n",
            "Epoch [23/100], Batch [180/240],  Cross Loss: 0.0020\n",
            "180/240, Train_loss: 0.0020 Train_dice: 0.0020\n",
            "Epoch [23/100], Batch [181/240],  Cross Loss: 0.0022\n",
            "181/240, Train_loss: 0.0022 Train_dice: 0.0022\n",
            "Epoch [23/100], Batch [182/240],  Cross Loss: 0.0255\n",
            "182/240, Train_loss: 0.0255 Train_dice: 0.0255\n",
            "Epoch [23/100], Batch [183/240],  Cross Loss: 0.0159\n",
            "183/240, Train_loss: 0.0159 Train_dice: 0.0159\n",
            "Epoch [23/100], Batch [184/240],  Cross Loss: 0.0039\n",
            "184/240, Train_loss: 0.0039 Train_dice: 0.0039\n",
            "Epoch [23/100], Batch [185/240],  Cross Loss: 0.1666\n",
            "185/240, Train_loss: 0.1666 Train_dice: 0.1666\n",
            "Epoch [23/100], Batch [186/240],  Cross Loss: 0.0012\n",
            "186/240, Train_loss: 0.0012 Train_dice: 0.0012\n",
            "Epoch [23/100], Batch [187/240],  Cross Loss: 0.0646\n",
            "187/240, Train_loss: 0.0646 Train_dice: 0.0646\n",
            "Epoch [23/100], Batch [188/240],  Cross Loss: 0.0045\n",
            "188/240, Train_loss: 0.0045 Train_dice: 0.0045\n",
            "Epoch [23/100], Batch [189/240],  Cross Loss: 0.0671\n",
            "189/240, Train_loss: 0.0671 Train_dice: 0.0671\n",
            "Epoch [23/100], Batch [190/240],  Cross Loss: 0.0177\n",
            "190/240, Train_loss: 0.0177 Train_dice: 0.0177\n",
            "Epoch [23/100], Batch [191/240],  Cross Loss: 0.0222\n",
            "191/240, Train_loss: 0.0222 Train_dice: 0.0222\n",
            "Epoch [23/100], Batch [192/240],  Cross Loss: 0.0245\n",
            "192/240, Train_loss: 0.0245 Train_dice: 0.0245\n",
            "Epoch [23/100], Batch [193/240],  Cross Loss: 0.0428\n",
            "193/240, Train_loss: 0.0428 Train_dice: 0.0428\n",
            "Epoch [23/100], Batch [194/240],  Cross Loss: 0.0028\n",
            "194/240, Train_loss: 0.0028 Train_dice: 0.0028\n",
            "Epoch [23/100], Batch [195/240],  Cross Loss: 0.0425\n",
            "195/240, Train_loss: 0.0425 Train_dice: 0.0425\n",
            "Epoch [23/100], Batch [196/240],  Cross Loss: 0.0615\n",
            "196/240, Train_loss: 0.0615 Train_dice: 0.0615\n",
            "Epoch [23/100], Batch [197/240],  Cross Loss: 0.0315\n",
            "197/240, Train_loss: 0.0315 Train_dice: 0.0315\n",
            "Epoch [23/100], Batch [198/240],  Cross Loss: 0.2092\n",
            "198/240, Train_loss: 0.2092 Train_dice: 0.2092\n",
            "Epoch [23/100], Batch [199/240],  Cross Loss: 0.0293\n",
            "199/240, Train_loss: 0.0293 Train_dice: 0.0293\n",
            "Epoch [23/100], Batch [200/240],  Cross Loss: 0.1709\n",
            "200/240, Train_loss: 0.1709 Train_dice: 0.1709\n",
            "Epoch [23/100], Batch [201/240],  Cross Loss: 0.0238\n",
            "201/240, Train_loss: 0.0238 Train_dice: 0.0238\n",
            "Epoch [23/100], Batch [202/240],  Cross Loss: 0.0093\n",
            "202/240, Train_loss: 0.0093 Train_dice: 0.0093\n",
            "Epoch [23/100], Batch [203/240],  Cross Loss: 0.0076\n",
            "203/240, Train_loss: 0.0076 Train_dice: 0.0076\n",
            "Epoch [23/100], Batch [204/240],  Cross Loss: 0.1066\n",
            "204/240, Train_loss: 0.1066 Train_dice: 0.1066\n",
            "Epoch [23/100], Batch [205/240],  Cross Loss: 0.0005\n",
            "205/240, Train_loss: 0.0005 Train_dice: 0.0005\n",
            "Epoch [23/100], Batch [206/240],  Cross Loss: 0.0009\n",
            "206/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [23/100], Batch [207/240],  Cross Loss: 0.0134\n",
            "207/240, Train_loss: 0.0134 Train_dice: 0.0134\n",
            "Epoch [23/100], Batch [208/240],  Cross Loss: 0.0071\n",
            "208/240, Train_loss: 0.0071 Train_dice: 0.0071\n",
            "Epoch [23/100], Batch [209/240],  Cross Loss: 0.1423\n",
            "209/240, Train_loss: 0.1423 Train_dice: 0.1423\n",
            "Epoch [23/100], Batch [210/240],  Cross Loss: 0.0035\n",
            "210/240, Train_loss: 0.0035 Train_dice: 0.0035\n",
            "Epoch [23/100], Batch [211/240],  Cross Loss: 0.0412\n",
            "211/240, Train_loss: 0.0412 Train_dice: 0.0412\n",
            "Epoch [23/100], Batch [212/240],  Cross Loss: 0.0108\n",
            "212/240, Train_loss: 0.0108 Train_dice: 0.0108\n",
            "Epoch [23/100], Batch [213/240],  Cross Loss: 0.0006\n",
            "213/240, Train_loss: 0.0006 Train_dice: 0.0006\n",
            "Epoch [23/100], Batch [214/240],  Cross Loss: 0.0021\n",
            "214/240, Train_loss: 0.0021 Train_dice: 0.0021\n",
            "Epoch [23/100], Batch [215/240],  Cross Loss: 0.4513\n",
            "215/240, Train_loss: 0.4513 Train_dice: 0.4513\n",
            "Epoch [23/100], Batch [216/240],  Cross Loss: 0.0132\n",
            "216/240, Train_loss: 0.0132 Train_dice: 0.0132\n",
            "Epoch [23/100], Batch [217/240],  Cross Loss: 0.0391\n",
            "217/240, Train_loss: 0.0391 Train_dice: 0.0391\n",
            "Epoch [23/100], Batch [218/240],  Cross Loss: 0.1294\n",
            "218/240, Train_loss: 0.1294 Train_dice: 0.1294\n",
            "Epoch [23/100], Batch [219/240],  Cross Loss: 0.0024\n",
            "219/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [23/100], Batch [220/240],  Cross Loss: 0.1478\n",
            "220/240, Train_loss: 0.1478 Train_dice: 0.1478\n",
            "Epoch [23/100], Batch [221/240],  Cross Loss: 0.0339\n",
            "221/240, Train_loss: 0.0339 Train_dice: 0.0339\n",
            "Epoch [23/100], Batch [222/240],  Cross Loss: 0.0010\n",
            "222/240, Train_loss: 0.0010 Train_dice: 0.0010\n",
            "Epoch [23/100], Batch [223/240],  Cross Loss: 0.0112\n",
            "223/240, Train_loss: 0.0112 Train_dice: 0.0112\n",
            "Epoch [23/100], Batch [224/240],  Cross Loss: 0.0077\n",
            "224/240, Train_loss: 0.0077 Train_dice: 0.0077\n",
            "Epoch [23/100], Batch [225/240],  Cross Loss: 0.5252\n",
            "225/240, Train_loss: 0.5252 Train_dice: 0.5252\n",
            "Epoch [23/100], Batch [226/240],  Cross Loss: 0.3417\n",
            "226/240, Train_loss: 0.3417 Train_dice: 0.3417\n",
            "Epoch [23/100], Batch [227/240],  Cross Loss: 0.1505\n",
            "227/240, Train_loss: 0.1505 Train_dice: 0.1505\n",
            "Epoch [23/100], Batch [228/240],  Cross Loss: 0.0454\n",
            "228/240, Train_loss: 0.0454 Train_dice: 0.0454\n",
            "Epoch [23/100], Batch [229/240],  Cross Loss: 0.0015\n",
            "229/240, Train_loss: 0.0015 Train_dice: 0.0015\n",
            "Epoch [23/100], Batch [230/240],  Cross Loss: 0.0106\n",
            "230/240, Train_loss: 0.0106 Train_dice: 0.0106\n",
            "Epoch [23/100], Batch [231/240],  Cross Loss: 0.0054\n",
            "231/240, Train_loss: 0.0054 Train_dice: 0.0054\n",
            "Epoch [23/100], Batch [232/240],  Cross Loss: 0.0117\n",
            "232/240, Train_loss: 0.0117 Train_dice: 0.0117\n",
            "Epoch [23/100], Batch [233/240],  Cross Loss: 0.0294\n",
            "233/240, Train_loss: 0.0294 Train_dice: 0.0294\n",
            "Epoch [23/100], Batch [234/240],  Cross Loss: 0.1119\n",
            "234/240, Train_loss: 0.1119 Train_dice: 0.1119\n",
            "Epoch [23/100], Batch [235/240],  Cross Loss: 0.0026\n",
            "235/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [23/100], Batch [236/240],  Cross Loss: 0.0029\n",
            "236/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [23/100], Batch [237/240],  Cross Loss: 0.0076\n",
            "237/240, Train_loss: 0.0076 Train_dice: 0.0076\n",
            "Epoch [23/100], Batch [238/240],  Cross Loss: 0.0987\n",
            "238/240, Train_loss: 0.0987 Train_dice: 0.0987\n",
            "Epoch [23/100], Batch [239/240],  Cross Loss: 0.2831\n",
            "239/240, Train_loss: 0.2831 Train_dice: 0.2831\n",
            "Epoch [23/100], Batch [240/240],  Cross Loss: 0.2073\n",
            "240/240, Train_loss: 0.2073 Train_dice: 0.2073\n",
            "--------------------\n",
            "Epoch_loss: 0.0990\n",
            "Epoch_metric: tensor(0.0990, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 24/100\n",
            "Epoch [24/100], Batch [1/240],  Cross Loss: 0.0076\n",
            "1/240, Train_loss: 0.0076 Train_dice: 0.0076\n",
            "Epoch [24/100], Batch [2/240],  Cross Loss: 0.5156\n",
            "2/240, Train_loss: 0.5156 Train_dice: 0.5156\n",
            "Epoch [24/100], Batch [3/240],  Cross Loss: 0.0038\n",
            "3/240, Train_loss: 0.0038 Train_dice: 0.0038\n",
            "Epoch [24/100], Batch [4/240],  Cross Loss: 0.0160\n",
            "4/240, Train_loss: 0.0160 Train_dice: 0.0160\n",
            "Epoch [24/100], Batch [5/240],  Cross Loss: 0.2418\n",
            "5/240, Train_loss: 0.2418 Train_dice: 0.2418\n",
            "Epoch [24/100], Batch [6/240],  Cross Loss: 0.0059\n",
            "6/240, Train_loss: 0.0059 Train_dice: 0.0059\n",
            "Epoch [24/100], Batch [7/240],  Cross Loss: 0.0051\n",
            "7/240, Train_loss: 0.0051 Train_dice: 0.0051\n",
            "Epoch [24/100], Batch [8/240],  Cross Loss: 0.2000\n",
            "8/240, Train_loss: 0.2000 Train_dice: 0.2000\n",
            "Epoch [24/100], Batch [9/240],  Cross Loss: 0.0709\n",
            "9/240, Train_loss: 0.0709 Train_dice: 0.0709\n",
            "Epoch [24/100], Batch [10/240],  Cross Loss: 0.0474\n",
            "10/240, Train_loss: 0.0474 Train_dice: 0.0474\n",
            "Epoch [24/100], Batch [11/240],  Cross Loss: 0.0284\n",
            "11/240, Train_loss: 0.0284 Train_dice: 0.0284\n",
            "Epoch [24/100], Batch [12/240],  Cross Loss: 0.0026\n",
            "12/240, Train_loss: 0.0026 Train_dice: 0.0026\n",
            "Epoch [24/100], Batch [13/240],  Cross Loss: 0.0808\n",
            "13/240, Train_loss: 0.0808 Train_dice: 0.0808\n",
            "Epoch [24/100], Batch [14/240],  Cross Loss: 0.0062\n",
            "14/240, Train_loss: 0.0062 Train_dice: 0.0062\n",
            "Epoch [24/100], Batch [15/240],  Cross Loss: 0.0046\n",
            "15/240, Train_loss: 0.0046 Train_dice: 0.0046\n",
            "Epoch [24/100], Batch [16/240],  Cross Loss: 0.0149\n",
            "16/240, Train_loss: 0.0149 Train_dice: 0.0149\n",
            "Epoch [24/100], Batch [17/240],  Cross Loss: 0.0158\n",
            "17/240, Train_loss: 0.0158 Train_dice: 0.0158\n",
            "Epoch [24/100], Batch [18/240],  Cross Loss: 0.0176\n",
            "18/240, Train_loss: 0.0176 Train_dice: 0.0176\n",
            "Epoch [24/100], Batch [19/240],  Cross Loss: 0.3038\n",
            "19/240, Train_loss: 0.3038 Train_dice: 0.3038\n",
            "Epoch [24/100], Batch [20/240],  Cross Loss: 0.0024\n",
            "20/240, Train_loss: 0.0024 Train_dice: 0.0024\n",
            "Epoch [24/100], Batch [21/240],  Cross Loss: 0.0128\n",
            "21/240, Train_loss: 0.0128 Train_dice: 0.0128\n",
            "Epoch [24/100], Batch [22/240],  Cross Loss: 0.0018\n",
            "22/240, Train_loss: 0.0018 Train_dice: 0.0018\n",
            "Epoch [24/100], Batch [23/240],  Cross Loss: 0.4901\n",
            "23/240, Train_loss: 0.4901 Train_dice: 0.4901\n",
            "Epoch [24/100], Batch [24/240],  Cross Loss: 0.0123\n",
            "24/240, Train_loss: 0.0123 Train_dice: 0.0123\n",
            "Epoch [24/100], Batch [25/240],  Cross Loss: 0.0103\n",
            "25/240, Train_loss: 0.0103 Train_dice: 0.0103\n",
            "Epoch [24/100], Batch [26/240],  Cross Loss: 0.0029\n",
            "26/240, Train_loss: 0.0029 Train_dice: 0.0029\n",
            "Epoch [24/100], Batch [27/240],  Cross Loss: 0.0011\n",
            "27/240, Train_loss: 0.0011 Train_dice: 0.0011\n",
            "Epoch [24/100], Batch [28/240],  Cross Loss: 0.0049\n",
            "28/240, Train_loss: 0.0049 Train_dice: 0.0049\n",
            "Epoch [24/100], Batch [29/240],  Cross Loss: 0.0030\n",
            "29/240, Train_loss: 0.0030 Train_dice: 0.0030\n",
            "Epoch [24/100], Batch [30/240],  Cross Loss: 0.0009\n",
            "30/240, Train_loss: 0.0009 Train_dice: 0.0009\n",
            "Epoch [24/100], Batch [31/240],  Cross Loss: 0.1408\n",
            "31/240, Train_loss: 0.1408 Train_dice: 0.1408\n",
            "Epoch [24/100], Batch [32/240],  Cross Loss: 0.0114\n",
            "32/240, Train_loss: 0.0114 Train_dice: 0.0114\n",
            "Epoch [24/100], Batch [33/240],  Cross Loss: 0.2354\n",
            "33/240, Train_loss: 0.2354 Train_dice: 0.2354\n",
            "Epoch [24/100], Batch [34/240],  Cross Loss: 0.2456\n",
            "34/240, Train_loss: 0.2456 Train_dice: 0.2456\n",
            "Epoch [24/100], Batch [35/240],  Cross Loss: 0.0088\n",
            "35/240, Train_loss: 0.0088 Train_dice: 0.0088\n",
            "Epoch [24/100], Batch [36/240],  Cross Loss: 0.5544\n",
            "36/240, Train_loss: 0.5544 Train_dice: 0.5544\n",
            "Epoch [24/100], Batch [37/240],  Cross Loss: 0.0006\n",
            "37/240, Train_loss: 0.0006 Train_dice: 0.0006\n",
            "Epoch [24/100], Batch [38/240],  Cross Loss: 0.0028\n",
            "38/240, Train_loss: 0.0028 Train_dice: 0.0028\n",
            "Epoch [24/100], Batch [39/240],  Cross Loss: 0.0393\n",
            "39/240, Train_loss: 0.0393 Train_dice: 0.0393\n",
            "Epoch [24/100], Batch [40/240],  Cross Loss: 0.0076\n",
            "40/240, Train_loss: 0.0076 Train_dice: 0.0076\n",
            "Epoch [24/100], Batch [41/240],  Cross Loss: 0.0138\n",
            "41/240, Train_loss: 0.0138 Train_dice: 0.0138\n",
            "Epoch [24/100], Batch [42/240],  Cross Loss: 0.0225\n",
            "42/240, Train_loss: 0.0225 Train_dice: 0.0225\n",
            "Epoch [24/100], Batch [43/240],  Cross Loss: 0.0366\n",
            "43/240, Train_loss: 0.0366 Train_dice: 0.0366\n",
            "Epoch [24/100], Batch [44/240],  Cross Loss: 0.0404\n",
            "44/240, Train_loss: 0.0404 Train_dice: 0.0404\n",
            "Epoch [24/100], Batch [45/240],  Cross Loss: 0.0017\n",
            "45/240, Train_loss: 0.0017 Train_dice: 0.0017\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "def updateLogs(path, data):\n",
        "    f = open(path,'a')\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "\n",
        "model_dir = './results/lesion/Balanced/'\n",
        "\n",
        "\n",
        "start_from = 1\n",
        "test_interval = 2\n",
        "num_epochs = 100\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "\n",
        "save_loss_train = []\n",
        "save_loss_test = []\n",
        "save_metric_train = []\n",
        "save_metric_test = []\n",
        "if (start_from != 1):\n",
        "  save_loss_train, save_metric_train, save_loss_test, save_metric_test= [x.tolist() for x in load_metrices(load_from)]\n",
        "  if(len(save_metric_test)):\n",
        "        best_metric = max(save_metric_test)\n",
        "  best_metric_epoch = -2\n",
        "train_loader, test_loader = data_in\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    train_epoch_loss = 0\n",
        "    train_step = 0\n",
        "    epoch_metric_train = 0\n",
        "\n",
        "    for batch_id, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        train_step += 1\n",
        "        \n",
        "        volume = batch_data[\"image\"]\n",
        "        label = batch_data[\"label\"]\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          volume, label = (volume.to(device), label.to(device))\n",
        "\n",
        "        \n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        mask_output, bb_output = model(volume)\n",
        "        \n",
        "        #Compute loss for segmentation mask\n",
        "        # mask_loss = loss_function(label, mask_output)\n",
        "        \n",
        "        bb_target = [0,0,0,0,0,0]\n",
        "        \n",
        "        value =  int(batch_data['Gleason Grade Group'][0])\n",
        "        bb_target[value] = 1\n",
        "        \n",
        "       \n",
        "        bb_target = torch.tensor(bb_target).view(1, -1).to(device)\n",
        "        \n",
        "        \n",
        "        train_loss = loss(bb_output, bb_target.float())\n",
        "        # Backward pass\n",
        "        train_loss.backward()\n",
        "        \n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        \n",
        "       # Print training progress\n",
        "        print('Epoch [{}/{}], Batch [{}/{}],  Cross Loss: {:.4f}'\n",
        "              .format(epoch+1, num_epochs, batch_id+1, len(train_loader),  train_loss.item()))\n",
        "        updateLogs(os.path.join(model_dir, \"logs.txt\"), f'Epoch [{epoch+1}/{num_epochs}], Batch [{ batch_id+1}/{ len(train_loader)}], Log Loss:  {train_loss.item():.4f}\\n')\n",
        "        updateLogs(os.path.join(model_dir, \"logs.txt\"), f'True {bb_target.cpu().detach().numpy()}\\n Predicted {bb_output.cpu().detach().numpy()} \\n')\n",
        "\n",
        "\n",
        "        train_epoch_loss += train_loss.item()\n",
        "        print(\n",
        "                f\"{train_step}/{len(train_loader) // train_loader.batch_size}, \"\n",
        "                f\"Train_loss: {train_loss.item():.4f}\", end=\" \")\n",
        "\n",
        "        train_metric = loss(bb_output, bb_target.float())\n",
        "        epoch_metric_train += train_metric\n",
        "        print(f'Train_dice: {train_metric.cpu().detach().numpy():.4f}')\n",
        "\n",
        "    print('-'*20)\n",
        "        \n",
        "    train_epoch_loss /= train_step\n",
        "    print(f'Epoch_loss: {train_epoch_loss:.4f}')\n",
        "    save_loss_train.append(train_epoch_loss)\n",
        "    np.save(os.path.join(model_dir, 'loss_train.npy'), save_loss_train)\n",
        "        \n",
        "    epoch_metric_train /= train_step\n",
        "    print(f'Epoch_metric: {epoch_metric_train}')\n",
        "\n",
        "    save_metric_train.append(epoch_metric_train.cpu().detach().numpy())\n",
        "    np.save(os.path.join(model_dir, 'metric_train.npy'), save_metric_train)\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(\n",
        "            model_dir, \"current_metric_model.pth\"))\n",
        "        \n",
        "    updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"{'-'*20}{epoch+1} \\nEpoch_loss: {train_epoch_loss:.4f}\\nEpoch_metric: {epoch_metric_train}\\n\")\n",
        "\n",
        "    if (epoch + 1) % test_interval == 0:\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          test_epoch_loss = 0\n",
        "          test_metric = 0\n",
        "          epoch_metric_test = 0\n",
        "          test_step = 0\n",
        "          total_loss = 0\n",
        "          for batch_id, batch_data in enumerate(test_loader):\n",
        "              test_step += 1\n",
        "\n",
        "              volume = batch_data[\"image\"]\n",
        "              label = batch_data[\"label\"]\n",
        "              if torch.cuda.is_available():\n",
        "                  volume, label = (volume.to(device), label.to(device))\n",
        "              \n",
        "              # Forward pass\n",
        "             \n",
        "              mask_output, bb_output = model(volume)\n",
        "              \n",
        "              bb_target = [0,0,0,0,0,0]\n",
        "        \n",
        "              value =  int(batch_data['Gleason Grade Group'][0])\n",
        "              bb_target[value] = 1\n",
        "            \n",
        "          \n",
        "              bb_target = torch.tensor(bb_target).view(1, -1).to(device)\n",
        "                  \n",
        "              # Compute loss for bounding box classifier\n",
        "\n",
        "              test_loss = loss( bb_output, bb_target.float())\n",
        "              \n",
        "              test_epoch_loss += test_loss.item()\n",
        "              test_metric = loss( bb_output, bb_target.float())\n",
        "              \n",
        "              epoch_metric_test += test_metric\n",
        "          \n",
        "          # Calculate the average loss across all batches in the test_loader\n",
        "          test_epoch_loss /= test_step\n",
        "          print(f'test_loss_epoch: {test_epoch_loss:.4f}')\n",
        "          save_loss_test.append(test_epoch_loss)\n",
        "          np.save(os.path.join(model_dir, 'loss_test.npy'), save_loss_test)\n",
        "\n",
        "          epoch_metric_test /= test_step\n",
        "          print(f'test_dice_epoch: {epoch_metric_test}')\n",
        "          save_metric_test.append(epoch_metric_test.cpu().detach().numpy())\n",
        "          np.save(os.path.join(model_dir, 'metric_test.npy'), save_metric_test)\n",
        "\n",
        "\n",
        "          if epoch_metric_test > best_metric:\n",
        "              best_metric = epoch_metric_test\n",
        "              best_metric_epoch = epoch + 1\n",
        "              torch.save(model.state_dict(), os.path.join(\n",
        "              model_dir, \"best_metric_model.pth\"))\n",
        "                \n",
        "          print(\n",
        "                    f\"current epoch: {epoch + 1} current mean dice: {epoch_metric_test}\"\n",
        "                    f\"\\nbest mean dice: {best_metric} \"\n",
        "                     \n",
        "                    f\"at epoch: {best_metric_epoch}\"\n",
        "                )\n",
        "          \n",
        "          \n",
        "          updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"{'-'*30}\\ncurrent epoch: {epoch + 1} \\n\"\n",
        "                    f'test_dice_epoch: {epoch_metric_test}\\n'\n",
        "                    f'test_loss_epoch: {test_epoch_loss:.4f}\\n'\n",
        "                    f\"best mean dice: {best_metric} \"\n",
        "                    f\"at epoch: {best_metric_epoch}\\n\")\n",
        "          \n",
        "        updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"train completed, best_metric: {best_metric}\"\n",
        "        f\"at epoch: {best_metric_epoch}\\n\")\n",
        "\n",
        "        update_history([start_from, num_epochs, best_metric.cpu().detach().numpy(), best_metric_epoch],model_dir=model_dir)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOE9Lac4k6cy"
      },
      "source": [
        "# ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1K40oUSk-9Y"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define a basic convolutional block\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.stride = stride\n",
        "\n",
        "        # If the input and output dimensions are different, use a 1x1 convolutional layer\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet architecture\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=6):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv3d(4, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(num_blocks - 1):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the ResNet architecture\n",
        "# Instantiate the ResNet architecture\n",
        "def ResNet18(num_classes=6):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j1o3LAr2l8QO",
        "outputId": "8e2d55c3-4f4c-4afa-adf9-a1fb5cdd6104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11177094\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-12e7f8dc7e7a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mResNet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b13fe7c6a111>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b13fe7c6a111>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 64, 128, 128, 16]"
          ]
        }
      ],
      "source": [
        "  import torch\n",
        "\n",
        "  x =ResNet18(num_classes=6)\n",
        "  print(sum(p.numel() for p in x.parameters()))\n",
        "  print(x(torch.randn(1,4,128,128,16)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zcL1O0rZbm5"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1), bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1), bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut.add_module('shortcut_conv', nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))\n",
        "            self.shortcut.add_module('shortcut_bn', nn.BatchNorm3d(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"residual\")\n",
        "        residual = self.shortcut(x)\n",
        "        print(x.shape)\n",
        "        x = self.conv1(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        print(x.shape)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        print(x.shape)\n",
        "        x += residual\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4muGWiZXnbn",
        "outputId": "3b8d62c2-7b2e-4e47-c825-4d6810d5838e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14645894\n",
            "residual\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "residual\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "residual\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "residual\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "residual\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "torch.Size([1, 512, 32, 32, 4])\n",
            "torch.Size([1, 512, 32, 32, 4])\n",
            "torch.Size([1, 512, 32, 32, 4])\n",
            "torch.Size([1, 512, 32, 32, 4])\n",
            "torch.Size([1, 6])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 1)#3\n",
        "        self.layer2 = self._make_layer(64, 128, 1)# 4\n",
        "        self.layer3 = self._make_layer(128, 256,1)# 6\n",
        "        self.layer4 = self._make_layer(256, 512,1)# 3\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(512, 6)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks):\n",
        "        layers = []\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(ResidualBlock(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer2(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer3(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer4(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "x =ResNet()\n",
        "print(sum(p.numel() for p in x.parameters()))\n",
        "print(x(torch.randn(1,4,128,128,16)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW524hv8ncLj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 1)#3\n",
        "        self.layer2 = self._make_layer(64, 128, 1)# 4\n",
        "        self.layer3 = self._make_layer(128, 256,1)# 6\n",
        "        self.layer4 = self._make_layer(256, 512,1)# 3\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(512, 6)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks):\n",
        "        layers = []\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(ResidualBlock(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer2(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer3(x)\n",
        "        print(x.shape)\n",
        "        x = self.layer4(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "x =ResNet()\n",
        "print(sum(p.numel() for p in x.parameters()))\n",
        "print(x(torch.randn(1,4,128,128,16)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txse4NidVs-S"
      },
      "source": [
        "# Trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NygYfhuEVriD"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# # Define U-Net model architecture with bounding box classifier\n",
        "# class UNetWithBBClassifier(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, num_classes):\n",
        "#         super(UNetWithBBClassifier, self).__init__()\n",
        "#         self.encoder = nn.Sequential(\n",
        "#             nn.Conv3d(in_channels, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "#         self.decoder = nn.Sequential(\n",
        "#             nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv3d(64, 1, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             #nn.ConvTranspose3d(64, out_channels, kernel_size=2, stride=2)\n",
        "#         )\n",
        "#         # Calculate the input size for the bounding box classifier\n",
        "#         encoder_output_size = 64 * 64 * 64 * 8 #64 * (input_height // (2 ** num_downsamples)) * (input_width // (2 ** num_downsamples)) * (input_depth // (2 ** num_downsamples))\n",
        "#         self.bb_classifier = nn.Sequential(\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(encoder_output_size, 256),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Linear(256, 128),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Linear(128, num_classes),\n",
        "#             nn.Softmax(dim=1)\n",
        "#         )\n",
        "#         self.segmentation_mask = nn.Conv3d(out_channels, num_classes, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x1 = self.encoder(x)\n",
        "#         print(x1.shape)\n",
        "#         x2 = self.decoder(x1)\n",
        "#         bb_output = self.bb_classifier(x1.view(x1.size(0), -1))\n",
        "#         mask_output = self.segmentation_mask(x2)\n",
        "#         return mask_output, bb_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZZBjRPcp1Au"
      },
      "outputs": [],
      "source": [
        "# Define Gleason score prediction loss function\n",
        "class GleasonScoreLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GleasonScoreLoss, self).__init__()\n",
        "        self.criterion = nn.CrossEntropyLoss()  # You can choose an appropriate loss function for Gleason score prediction\n",
        "\n",
        "    def forward(self, predicted_scores, target_scores):\n",
        "        loss = self.criterion(predicted_scores, target_scores)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ9Scqg4Yb9K"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class UNetWithBBClassifierLoss(nn.Module):\n",
        "#     def __init__(self, weight_seg=1.0, weight_bb=1.0):\n",
        "#         super(UNetWithBBClassifierLoss, self).__init__()\n",
        "#         self.weight_seg = weight_seg\n",
        "#         self.weight_bb = weight_bb\n",
        "\n",
        "#     def forward(self, output, target_seg, target_bb):\n",
        "#         # Unpack the output tuple\n",
        "#         seg_output, bb_output = output\n",
        "\n",
        "#         # Compute the segmentation loss using cross-entropy\n",
        "#         # seg_criterion = nn.CrossEntropyLoss()\n",
        "#         # seg_loss = seg_criterion(seg_output, target_seg)\n",
        "#         seg_criterion = nn.CrossEntropyLoss()\n",
        "#         seg_loss = seg_criterion(seg_output, target_seg)\n",
        "\n",
        "#         # Compute the bounding box classification loss using cross-entropy\n",
        "#         bb_criterion = nn.CrossEntropyLoss()\n",
        "#         bb_loss = bb_criterion(bb_output, target_bb)\n",
        "\n",
        "#         # Combine the segmentation and bounding box classification losses\n",
        "#         total_loss = self.weight_seg * seg_loss + self.weight_bb * bb_loss\n",
        "\n",
        "#         return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKX-N7w2Q_MO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNetWithBBClassifierLoss(nn.Module):\n",
        "    def __init__(self, weight_seg=0, weight_bb=1.0):\n",
        "        super(UNetWithBBClassifierLoss, self).__init__()\n",
        "        self.weight_seg = weight_seg\n",
        "        self.weight_bb = weight_bb\n",
        "\n",
        "    def forward(self, seg_output, target_seg, bb_output,  target_bb):\n",
        "\n",
        "        # Compute the segmentation loss using cross-entropy\n",
        "        seg_criterion =  DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
        "        seg_loss = seg_criterion(seg_output, target_seg)\n",
        "        print(\"seg_loss :\", seg_loss)\n",
        "        # Compute the bounding box classification loss using cross-entropy\n",
        "        bb_criterion = nn.LogLoss() #CrossEntropyLoss()\n",
        "        bb_loss = bb_criterion(bb_output, target_bb)\n",
        "        print(\"bb_loss :\",bb_loss)\n",
        "        #bb_loss = F.smooth_l1_loss(bb_output,  target_bb, reduction='mean', beta=1.0)\n",
        "        # Combine the segmentation and bounding box classification losses\n",
        "        total_loss = self.weight_seg * seg_loss + self.weight_bb * bb_loss\n",
        "\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4RUsYAEJPuE",
        "outputId": "cfa6a3b5-3cd1-413d-a0d5-b5e7105dd185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Perform one-hot encoding on the string column\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "one_hot = encoder.fit_transform(df[['Gleason Grade Group']])  # Extract the column as a DataFrame and perform one-hot encoding\n",
        "one_hot_tensor = torch.tensor(one_hot) \n",
        "\n",
        "# Print the resulting tensor\n",
        "print(len(one_hot_tensor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bujWet3gtfNb"
      },
      "outputs": [],
      "source": [
        "#loss_function = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XW7BLhTUXFB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajWTB28Ou26A"
      },
      "outputs": [],
      "source": [
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation during testing\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):  # Update with your test data and data loader\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        mask_output, bb_output = model(data)\n",
        "        \n",
        "        # Compute loss for segmentation mask (optional)\n",
        "        # mask_loss = loss_function(mask_output, target)\n",
        "        \n",
        "        # Compute loss for bounding box classifier\n",
        "        bb_target = target.view(target.size(0), -1)\n",
        "        bb_loss = criterion(bb_output, bb_target)\n",
        "        \n",
        "        # Update test loss\n",
        "        test_loss += bb_loss.item()  # Add the batch loss to the total test loss\n",
        "        \n",
        "        # Perform any other evaluation or metrics calculation here\n",
        "        \n",
        "# Average test loss over all batches\n",
        "test_loss /= len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKuIT2WdHomI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self, n_input_channels=256, n_features=64, n_output_channels=6, anchor_stride=2, dim=3):\n",
        "        super(YourModel, self).__init__()\n",
        "        self.n_classes =6\n",
        "        self.dim = dim\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv3d(n_input_channels, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_output_channels, kernel_size=3, stride=anchor_stride, padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        \n",
        "        # Rearrange dimensions based on self.dim\n",
        "        if self.dim == 2:\n",
        "            x = x.permute(0, 2, 3, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "        else:\n",
        "            x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHxUviG0F9WH"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  x = YourModel()\n",
        "  print(sum(p.numel() for p in x.parameters()))\n",
        "  print(x(torch.randn(1,256,32,32,16)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQSPYGh7oGdX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "\n",
        "# Define the two loss functions\n",
        "dsc_loss = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
        "cross_entropy_loss = FocalLoss() #torch.nn.BCELoss()\n",
        "\n",
        "# Define the two optimizers\n",
        "optimizer_out = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer_bb = torch.optim.Adam(model.bb_classifier.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgFSNpTEaTcd"
      },
      "outputs": [],
      "source": [
        "from monai.losses import FocalLoss\n",
        "\n",
        "# Define the Focal Loss function\n",
        "focal_loss = FocalLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iNHCrT7pFB7",
        "outputId": "02c608e3-dbd0-4cc8-99dc-5ce4cc855f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/monai/losses/dice.py:144: UserWarning: single channel prediction, `to_onehot_y=True` ignored.\n",
            "  warnings.warn(\"single channel prediction, `to_onehot_y=True` ignored.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Batch [1/239], Dice Loss: 0.9993, Cross Loss: 0.2378\n",
            "1/239, Train_loss: 0.23780295252799988 Train_dice: tensor(0.2378, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [2/239], Dice Loss: 0.9980, Cross Loss: 0.2239\n",
            "2/239, Train_loss: 0.22385910153388977 Train_dice: tensor(0.2239, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [3/239], Dice Loss: 0.9981, Cross Loss: 0.2227\n",
            "3/239, Train_loss: 0.22269460558891296 Train_dice: tensor(0.2227, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [4/239], Dice Loss: 0.9974, Cross Loss: 0.2138\n",
            "4/239, Train_loss: 0.21382629871368408 Train_dice: tensor(0.2138, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [5/239], Dice Loss: 0.9907, Cross Loss: 0.1866\n",
            "5/239, Train_loss: 0.18659573793411255 Train_dice: tensor(0.1866, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [6/239], Dice Loss: 0.9920, Cross Loss: 0.1877\n",
            "6/239, Train_loss: 0.18771815299987793 Train_dice: tensor(0.1877, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [7/239], Dice Loss: 0.9670, Cross Loss: 0.2073\n",
            "7/239, Train_loss: 0.20729057490825653 Train_dice: tensor(0.2073, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [8/239], Dice Loss: 0.9984, Cross Loss: 0.2164\n",
            "8/239, Train_loss: 0.2164442390203476 Train_dice: tensor(0.2164, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [9/239], Dice Loss: 0.9992, Cross Loss: 0.2258\n",
            "9/239, Train_loss: 0.2258002609014511 Train_dice: tensor(0.2258, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [10/239], Dice Loss: 0.9872, Cross Loss: 0.1959\n",
            "10/239, Train_loss: 0.19589529931545258 Train_dice: tensor(0.1959, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [11/239], Dice Loss: 0.9878, Cross Loss: 0.1965\n",
            "11/239, Train_loss: 0.19649827480316162 Train_dice: tensor(0.1965, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [12/239], Dice Loss: 0.9938, Cross Loss: 0.1931\n",
            "12/239, Train_loss: 0.19314977526664734 Train_dice: tensor(0.1931, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [13/239], Dice Loss: 0.9953, Cross Loss: 0.1907\n",
            "13/239, Train_loss: 0.1906648725271225 Train_dice: tensor(0.1907, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [14/239], Dice Loss: 0.9871, Cross Loss: 0.1913\n",
            "14/239, Train_loss: 0.1912812888622284 Train_dice: tensor(0.1913, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [15/239], Dice Loss: 0.9891, Cross Loss: 0.1885\n",
            "15/239, Train_loss: 0.1885383576154709 Train_dice: tensor(0.1885, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [16/239], Dice Loss: 0.9941, Cross Loss: 0.1841\n",
            "16/239, Train_loss: 0.18408799171447754 Train_dice: tensor(0.1841, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [17/239], Dice Loss: 0.9845, Cross Loss: 0.1796\n",
            "17/239, Train_loss: 0.17961280047893524 Train_dice: tensor(0.1796, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [18/239], Dice Loss: 0.9976, Cross Loss: 0.1759\n",
            "18/239, Train_loss: 0.17589136958122253 Train_dice: tensor(0.1759, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [19/239], Dice Loss: 0.9857, Cross Loss: 0.2276\n",
            "19/239, Train_loss: 0.227583646774292 Train_dice: tensor(0.2276, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [20/239], Dice Loss: 0.9372, Cross Loss: 0.2351\n",
            "20/239, Train_loss: 0.23507222533226013 Train_dice: tensor(0.2351, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [21/239], Dice Loss: 0.9965, Cross Loss: 0.2269\n",
            "21/239, Train_loss: 0.22694794833660126 Train_dice: tensor(0.2269, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [22/239], Dice Loss: 0.9955, Cross Loss: 0.1689\n",
            "22/239, Train_loss: 0.16890716552734375 Train_dice: tensor(0.1689, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [23/239], Dice Loss: 0.9733, Cross Loss: 0.2311\n",
            "23/239, Train_loss: 0.2311311960220337 Train_dice: tensor(0.2311, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [24/239], Dice Loss: 0.9502, Cross Loss: 0.2322\n",
            "24/239, Train_loss: 0.23215100169181824 Train_dice: tensor(0.2322, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [25/239], Dice Loss: 0.9958, Cross Loss: 0.1728\n",
            "25/239, Train_loss: 0.17275582253932953 Train_dice: tensor(0.1728, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [26/239], Dice Loss: 0.9941, Cross Loss: 0.1695\n",
            "26/239, Train_loss: 0.16951638460159302 Train_dice: tensor(0.1695, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [27/239], Dice Loss: 0.9971, Cross Loss: 0.1655\n",
            "27/239, Train_loss: 0.16552649438381195 Train_dice: tensor(0.1655, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [28/239], Dice Loss: 0.9675, Cross Loss: 0.1624\n",
            "28/239, Train_loss: 0.16244889795780182 Train_dice: tensor(0.1624, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [29/239], Dice Loss: 0.9921, Cross Loss: 0.1641\n",
            "29/239, Train_loss: 0.164062961935997 Train_dice: tensor(0.1641, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [30/239], Dice Loss: 0.9988, Cross Loss: 0.1596\n",
            "30/239, Train_loss: 0.15962176024913788 Train_dice: tensor(0.1596, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [31/239], Dice Loss: 0.9856, Cross Loss: 0.1589\n",
            "31/239, Train_loss: 0.1588946282863617 Train_dice: tensor(0.1589, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [32/239], Dice Loss: 0.9941, Cross Loss: 0.1556\n",
            "32/239, Train_loss: 0.15564881265163422 Train_dice: tensor(0.1556, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [33/239], Dice Loss: 0.9351, Cross Loss: 0.2537\n",
            "33/239, Train_loss: 0.25373375415802 Train_dice: tensor(0.2537, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [34/239], Dice Loss: 0.9966, Cross Loss: 0.1530\n",
            "34/239, Train_loss: 0.15298426151275635 Train_dice: tensor(0.1530, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [35/239], Dice Loss: 0.9897, Cross Loss: 0.2531\n",
            "35/239, Train_loss: 0.25306278467178345 Train_dice: tensor(0.2531, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [36/239], Dice Loss: 0.9930, Cross Loss: 0.1530\n",
            "36/239, Train_loss: 0.1530241221189499 Train_dice: tensor(0.1530, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [37/239], Dice Loss: 0.9927, Cross Loss: 0.1518\n",
            "37/239, Train_loss: 0.15179190039634705 Train_dice: tensor(0.1518, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [38/239], Dice Loss: 0.9861, Cross Loss: 0.1519\n",
            "38/239, Train_loss: 0.151941180229187 Train_dice: tensor(0.1519, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [39/239], Dice Loss: 0.9650, Cross Loss: 0.2565\n",
            "39/239, Train_loss: 0.25651150941848755 Train_dice: tensor(0.2565, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [40/239], Dice Loss: 0.9665, Cross Loss: 0.2561\n",
            "40/239, Train_loss: 0.256112277507782 Train_dice: tensor(0.2561, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [41/239], Dice Loss: 0.9911, Cross Loss: 0.1506\n",
            "41/239, Train_loss: 0.1506267786026001 Train_dice: tensor(0.1506, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [42/239], Dice Loss: 0.9354, Cross Loss: 0.1507\n",
            "42/239, Train_loss: 0.15074631571769714 Train_dice: tensor(0.1507, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [43/239], Dice Loss: 0.9927, Cross Loss: 0.1505\n",
            "43/239, Train_loss: 0.15054172277450562 Train_dice: tensor(0.1505, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [44/239], Dice Loss: 0.9532, Cross Loss: 0.1500\n",
            "44/239, Train_loss: 0.1499655842781067 Train_dice: tensor(0.1500, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [45/239], Dice Loss: 0.9626, Cross Loss: 0.2578\n",
            "45/239, Train_loss: 0.2577595114707947 Train_dice: tensor(0.2578, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [46/239], Dice Loss: 0.9972, Cross Loss: 0.1500\n",
            "46/239, Train_loss: 0.15001636743545532 Train_dice: tensor(0.1500, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [47/239], Dice Loss: 0.9834, Cross Loss: 0.1496\n",
            "47/239, Train_loss: 0.14961740374565125 Train_dice: tensor(0.1496, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [48/239], Dice Loss: 0.9771, Cross Loss: 0.2584\n",
            "48/239, Train_loss: 0.2584323287010193 Train_dice: tensor(0.2584, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [49/239], Dice Loss: 0.9906, Cross Loss: 0.1497\n",
            "49/239, Train_loss: 0.1496673822402954 Train_dice: tensor(0.1497, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [50/239], Dice Loss: 0.9647, Cross Loss: 0.1494\n",
            "50/239, Train_loss: 0.1494373381137848 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [51/239], Dice Loss: 0.9495, Cross Loss: 0.1493\n",
            "51/239, Train_loss: 0.14934825897216797 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [52/239], Dice Loss: 0.9806, Cross Loss: 0.1493\n",
            "52/239, Train_loss: 0.14925456047058105 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [53/239], Dice Loss: 0.9880, Cross Loss: 0.1493\n",
            "53/239, Train_loss: 0.14927318692207336 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [54/239], Dice Loss: 0.9846, Cross Loss: 0.2593\n",
            "54/239, Train_loss: 0.2593235373497009 Train_dice: tensor(0.2593, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [55/239], Dice Loss: 0.9634, Cross Loss: 0.1492\n",
            "55/239, Train_loss: 0.1492227017879486 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [56/239], Dice Loss: 0.9971, Cross Loss: 0.1492\n",
            "56/239, Train_loss: 0.1491832733154297 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [57/239], Dice Loss: 0.9819, Cross Loss: 0.2596\n",
            "57/239, Train_loss: 0.2596209645271301 Train_dice: tensor(0.2596, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [58/239], Dice Loss: 0.9873, Cross Loss: 0.1489\n",
            "58/239, Train_loss: 0.1489114910364151 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [59/239], Dice Loss: 0.9760, Cross Loss: 0.1490\n",
            "59/239, Train_loss: 0.14902310073375702 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [60/239], Dice Loss: 0.9458, Cross Loss: 0.1488\n",
            "60/239, Train_loss: 0.14884643256664276 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [61/239], Dice Loss: 0.9879, Cross Loss: 0.1488\n",
            "61/239, Train_loss: 0.1488458216190338 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [62/239], Dice Loss: 0.9728, Cross Loss: 0.2601\n",
            "62/239, Train_loss: 0.2600753903388977 Train_dice: tensor(0.2601, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [63/239], Dice Loss: 0.9857, Cross Loss: 0.1490\n",
            "63/239, Train_loss: 0.14901551604270935 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [64/239], Dice Loss: 0.9933, Cross Loss: 0.1488\n",
            "64/239, Train_loss: 0.14882925152778625 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [65/239], Dice Loss: 0.9630, Cross Loss: 0.1487\n",
            "65/239, Train_loss: 0.14874543249607086 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [66/239], Dice Loss: 0.9660, Cross Loss: 0.1488\n",
            "66/239, Train_loss: 0.14882437884807587 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [67/239], Dice Loss: 0.8947, Cross Loss: 0.1487\n",
            "67/239, Train_loss: 0.14873595535755157 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [68/239], Dice Loss: 0.9797, Cross Loss: 0.1487\n",
            "68/239, Train_loss: 0.14872437715530396 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [69/239], Dice Loss: 0.9969, Cross Loss: 0.2605\n",
            "69/239, Train_loss: 0.26046064496040344 Train_dice: tensor(0.2605, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [70/239], Dice Loss: 0.9494, Cross Loss: 0.2602\n",
            "70/239, Train_loss: 0.26024115085601807 Train_dice: tensor(0.2602, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [71/239], Dice Loss: 0.9351, Cross Loss: 0.1487\n",
            "71/239, Train_loss: 0.14873486757278442 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [72/239], Dice Loss: 0.8151, Cross Loss: 0.2605\n",
            "72/239, Train_loss: 0.2605196237564087 Train_dice: tensor(0.2605, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [73/239], Dice Loss: 0.9657, Cross Loss: 0.1487\n",
            "73/239, Train_loss: 0.14869728684425354 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [74/239], Dice Loss: 0.9798, Cross Loss: 0.1487\n",
            "74/239, Train_loss: 0.14869865775108337 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [75/239], Dice Loss: 0.9916, Cross Loss: 0.1488\n",
            "75/239, Train_loss: 0.14875178039073944 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [76/239], Dice Loss: 0.9912, Cross Loss: 0.1487\n",
            "76/239, Train_loss: 0.14868682622909546 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [77/239], Dice Loss: 0.9920, Cross Loss: 0.1487\n",
            "77/239, Train_loss: 0.14868782460689545 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [78/239], Dice Loss: 0.9602, Cross Loss: 0.1487\n",
            "78/239, Train_loss: 0.14868679642677307 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [79/239], Dice Loss: 0.9889, Cross Loss: 0.1487\n",
            "79/239, Train_loss: 0.14865827560424805 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [80/239], Dice Loss: 0.9857, Cross Loss: 0.1486\n",
            "80/239, Train_loss: 0.14855453372001648 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [81/239], Dice Loss: 0.9941, Cross Loss: 0.1486\n",
            "81/239, Train_loss: 0.14857077598571777 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [82/239], Dice Loss: 0.9481, Cross Loss: 0.1485\n",
            "82/239, Train_loss: 0.14852765202522278 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [83/239], Dice Loss: 0.9869, Cross Loss: 0.1485\n",
            "83/239, Train_loss: 0.14850929379463196 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [84/239], Dice Loss: 0.9801, Cross Loss: 0.1485\n",
            "84/239, Train_loss: 0.14852464199066162 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [85/239], Dice Loss: 0.9651, Cross Loss: 0.2607\n",
            "85/239, Train_loss: 0.26071643829345703 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [86/239], Dice Loss: 0.9685, Cross Loss: 0.1485\n",
            "86/239, Train_loss: 0.14851421117782593 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [87/239], Dice Loss: 0.9926, Cross Loss: 0.1485\n",
            "87/239, Train_loss: 0.14852474629878998 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [88/239], Dice Loss: 0.9976, Cross Loss: 0.1485\n",
            "88/239, Train_loss: 0.14846499264240265 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [89/239], Dice Loss: 0.9982, Cross Loss: 0.1485\n",
            "89/239, Train_loss: 0.14845576882362366 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [90/239], Dice Loss: 0.9704, Cross Loss: 0.1484\n",
            "90/239, Train_loss: 0.14844508469104767 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [91/239], Dice Loss: 0.9648, Cross Loss: 0.2606\n",
            "91/239, Train_loss: 0.2605997323989868 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [92/239], Dice Loss: 0.9762, Cross Loss: 0.1484\n",
            "92/239, Train_loss: 0.14843982458114624 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [93/239], Dice Loss: 0.9464, Cross Loss: 0.1484\n",
            "93/239, Train_loss: 0.14843595027923584 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [94/239], Dice Loss: 0.9671, Cross Loss: 0.2609\n",
            "94/239, Train_loss: 0.26088947057724 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [95/239], Dice Loss: 0.9677, Cross Loss: 0.1486\n",
            "95/239, Train_loss: 0.1486291140317917 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [96/239], Dice Loss: 0.9997, Cross Loss: 0.1486\n",
            "96/239, Train_loss: 0.14857451617717743 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [97/239], Dice Loss: 0.9998, Cross Loss: 0.1486\n",
            "97/239, Train_loss: 0.14863526821136475 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [98/239], Dice Loss: 0.9969, Cross Loss: 0.1484\n",
            "98/239, Train_loss: 0.14844590425491333 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [99/239], Dice Loss: 0.9999, Cross Loss: 0.2606\n",
            "99/239, Train_loss: 0.26058077812194824 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [100/239], Dice Loss: 0.9993, Cross Loss: 0.2610\n",
            "100/239, Train_loss: 0.2610166072845459 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [101/239], Dice Loss: 0.9950, Cross Loss: 0.2602\n",
            "101/239, Train_loss: 0.26024866104125977 Train_dice: tensor(0.2602, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [102/239], Dice Loss: 0.9546, Cross Loss: 0.1485\n",
            "102/239, Train_loss: 0.14848831295967102 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [103/239], Dice Loss: 0.9993, Cross Loss: 0.1494\n",
            "103/239, Train_loss: 0.14939603209495544 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [104/239], Dice Loss: 0.9121, Cross Loss: 0.2592\n",
            "104/239, Train_loss: 0.2591772675514221 Train_dice: tensor(0.2592, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [105/239], Dice Loss: 0.9737, Cross Loss: 0.1502\n",
            "105/239, Train_loss: 0.15021055936813354 Train_dice: tensor(0.1502, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [106/239], Dice Loss: 0.9819, Cross Loss: 0.1501\n",
            "106/239, Train_loss: 0.15007156133651733 Train_dice: tensor(0.1501, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [107/239], Dice Loss: 0.9454, Cross Loss: 0.1493\n",
            "107/239, Train_loss: 0.14927195012569427 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [108/239], Dice Loss: 0.9735, Cross Loss: 0.2603\n",
            "108/239, Train_loss: 0.2602556347846985 Train_dice: tensor(0.2603, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [109/239], Dice Loss: 0.9898, Cross Loss: 0.1491\n",
            "109/239, Train_loss: 0.1491299271583557 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [110/239], Dice Loss: 0.9748, Cross Loss: 0.1488\n",
            "110/239, Train_loss: 0.14882037043571472 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [111/239], Dice Loss: 0.9836, Cross Loss: 0.1486\n",
            "111/239, Train_loss: 0.1485910266637802 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [112/239], Dice Loss: 0.9526, Cross Loss: 0.1485\n",
            "112/239, Train_loss: 0.1484798789024353 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [113/239], Dice Loss: 0.9657, Cross Loss: 0.1484\n",
            "113/239, Train_loss: 0.14844417572021484 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [114/239], Dice Loss: 0.9451, Cross Loss: 0.1484\n",
            "114/239, Train_loss: 0.14843936264514923 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [115/239], Dice Loss: 0.9904, Cross Loss: 0.1484\n",
            "115/239, Train_loss: 0.14842236042022705 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [116/239], Dice Loss: 0.9848, Cross Loss: 0.1484\n",
            "116/239, Train_loss: 0.14841559529304504 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [117/239], Dice Loss: 0.9815, Cross Loss: 0.1484\n",
            "117/239, Train_loss: 0.14840279519557953 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [118/239], Dice Loss: 0.9910, Cross Loss: 0.1484\n",
            "118/239, Train_loss: 0.14839577674865723 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [119/239], Dice Loss: 0.9856, Cross Loss: 0.1484\n",
            "119/239, Train_loss: 0.1483839601278305 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [120/239], Dice Loss: 0.9824, Cross Loss: 0.1484\n",
            "120/239, Train_loss: 0.1483738273382187 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [121/239], Dice Loss: 0.9565, Cross Loss: 0.1484\n",
            "121/239, Train_loss: 0.148362934589386 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [122/239], Dice Loss: 0.9051, Cross Loss: 0.2611\n",
            "122/239, Train_loss: 0.2611023187637329 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [123/239], Dice Loss: 0.9938, Cross Loss: 0.1484\n",
            "123/239, Train_loss: 0.14835336804389954 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [124/239], Dice Loss: 0.8210, Cross Loss: 0.2611\n",
            "124/239, Train_loss: 0.2610979974269867 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [125/239], Dice Loss: 0.9943, Cross Loss: 0.2611\n",
            "125/239, Train_loss: 0.26113080978393555 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [126/239], Dice Loss: 0.9807, Cross Loss: 0.1483\n",
            "126/239, Train_loss: 0.14833995699882507 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [127/239], Dice Loss: 0.9843, Cross Loss: 0.1483\n",
            "127/239, Train_loss: 0.1483326405286789 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [128/239], Dice Loss: 0.9499, Cross Loss: 0.2611\n",
            "128/239, Train_loss: 0.2610543668270111 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [129/239], Dice Loss: 0.9800, Cross Loss: 0.1483\n",
            "129/239, Train_loss: 0.1483387053012848 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [130/239], Dice Loss: 0.9698, Cross Loss: 0.2610\n",
            "130/239, Train_loss: 0.2610486149787903 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [131/239], Dice Loss: 0.9825, Cross Loss: 0.1483\n",
            "131/239, Train_loss: 0.14834824204444885 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [132/239], Dice Loss: 0.9984, Cross Loss: 0.1483\n",
            "132/239, Train_loss: 0.14834102988243103 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [133/239], Dice Loss: 0.9678, Cross Loss: 0.1483\n",
            "133/239, Train_loss: 0.14833858609199524 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [134/239], Dice Loss: 0.9281, Cross Loss: 0.2611\n",
            "134/239, Train_loss: 0.2610931694507599 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [135/239], Dice Loss: 0.9425, Cross Loss: 0.1483\n",
            "135/239, Train_loss: 0.1483244150876999 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [136/239], Dice Loss: 0.9970, Cross Loss: 0.2611\n",
            "136/239, Train_loss: 0.2610793113708496 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [137/239], Dice Loss: 0.8154, Cross Loss: 0.2612\n",
            "137/239, Train_loss: 0.26117387413978577 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [138/239], Dice Loss: 0.9659, Cross Loss: 0.1483\n",
            "138/239, Train_loss: 0.1483454555273056 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [139/239], Dice Loss: 0.9975, Cross Loss: 0.2610\n",
            "139/239, Train_loss: 0.26103633642196655 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [140/239], Dice Loss: 0.9700, Cross Loss: 0.1484\n",
            "140/239, Train_loss: 0.1483641117811203 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [141/239], Dice Loss: 0.9992, Cross Loss: 0.2610\n",
            "141/239, Train_loss: 0.2610013484954834 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [142/239], Dice Loss: 0.9427, Cross Loss: 0.1484\n",
            "142/239, Train_loss: 0.14835266768932343 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [143/239], Dice Loss: 0.9999, Cross Loss: 0.2611\n",
            "143/239, Train_loss: 0.2611064016819 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [144/239], Dice Loss: 0.9204, Cross Loss: 0.1484\n",
            "144/239, Train_loss: 0.1484033763408661 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [145/239], Dice Loss: 0.9047, Cross Loss: 0.1484\n",
            "145/239, Train_loss: 0.14835774898529053 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [146/239], Dice Loss: 0.9431, Cross Loss: 0.1484\n",
            "146/239, Train_loss: 0.14835867285728455 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [147/239], Dice Loss: 0.9570, Cross Loss: 0.1484\n",
            "147/239, Train_loss: 0.14841139316558838 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [148/239], Dice Loss: 0.9844, Cross Loss: 0.1484\n",
            "148/239, Train_loss: 0.148399218916893 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [149/239], Dice Loss: 0.7631, Cross Loss: 0.2610\n",
            "149/239, Train_loss: 0.2609867453575134 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [150/239], Dice Loss: 0.9972, Cross Loss: 0.1484\n",
            "150/239, Train_loss: 0.1483820676803589 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [151/239], Dice Loss: 0.8638, Cross Loss: 0.2610\n",
            "151/239, Train_loss: 0.260962575674057 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [152/239], Dice Loss: 0.9686, Cross Loss: 0.1484\n",
            "152/239, Train_loss: 0.14837782084941864 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [153/239], Dice Loss: 0.9933, Cross Loss: 0.1484\n",
            "153/239, Train_loss: 0.1483958214521408 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [154/239], Dice Loss: 0.8334, Cross Loss: 0.2609\n",
            "154/239, Train_loss: 0.26094383001327515 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [155/239], Dice Loss: 0.9241, Cross Loss: 0.1485\n",
            "155/239, Train_loss: 0.14848965406417847 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [156/239], Dice Loss: 0.8719, Cross Loss: 0.2610\n",
            "156/239, Train_loss: 0.2609604001045227 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [157/239], Dice Loss: 0.8919, Cross Loss: 0.2611\n",
            "157/239, Train_loss: 0.26105067133903503 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [158/239], Dice Loss: 0.9839, Cross Loss: 0.2610\n",
            "158/239, Train_loss: 0.26096493005752563 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [159/239], Dice Loss: 0.9911, Cross Loss: 0.1484\n",
            "159/239, Train_loss: 0.14839942753314972 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [160/239], Dice Loss: 0.8897, Cross Loss: 0.2609\n",
            "160/239, Train_loss: 0.2609081268310547 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [161/239], Dice Loss: 0.9610, Cross Loss: 0.1484\n",
            "161/239, Train_loss: 0.1484348177909851 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [162/239], Dice Loss: 0.9886, Cross Loss: 0.2608\n",
            "162/239, Train_loss: 0.2608255445957184 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [163/239], Dice Loss: 0.9948, Cross Loss: 0.1485\n",
            "163/239, Train_loss: 0.14853249490261078 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [164/239], Dice Loss: 0.9795, Cross Loss: 0.1485\n",
            "164/239, Train_loss: 0.1485276222229004 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [165/239], Dice Loss: 0.8681, Cross Loss: 0.2609\n",
            "165/239, Train_loss: 0.2609443962574005 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [166/239], Dice Loss: 0.9984, Cross Loss: 0.2609\n",
            "166/239, Train_loss: 0.26088958978652954 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [167/239], Dice Loss: 0.9445, Cross Loss: 0.2607\n",
            "167/239, Train_loss: 0.2607390284538269 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [168/239], Dice Loss: 0.8530, Cross Loss: 0.2607\n",
            "168/239, Train_loss: 0.2606709897518158 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [169/239], Dice Loss: 0.9812, Cross Loss: 0.2609\n",
            "169/239, Train_loss: 0.26087686419487 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [170/239], Dice Loss: 0.9960, Cross Loss: 0.2607\n",
            "170/239, Train_loss: 0.260739803314209 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [171/239], Dice Loss: 0.9343, Cross Loss: 0.2605\n",
            "171/239, Train_loss: 0.2604721188545227 Train_dice: tensor(0.2605, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [172/239], Dice Loss: 0.9440, Cross Loss: 0.1487\n",
            "172/239, Train_loss: 0.14865195751190186 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [173/239], Dice Loss: 0.8492, Cross Loss: 0.2607\n",
            "173/239, Train_loss: 0.26070913672447205 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [174/239], Dice Loss: 0.8874, Cross Loss: 0.2602\n",
            "174/239, Train_loss: 0.2601791024208069 Train_dice: tensor(0.2602, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [175/239], Dice Loss: 0.9607, Cross Loss: 0.2599\n",
            "175/239, Train_loss: 0.25994598865509033 Train_dice: tensor(0.2599, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [176/239], Dice Loss: 0.9535, Cross Loss: 0.1491\n",
            "176/239, Train_loss: 0.1490616500377655 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [177/239], Dice Loss: 0.8745, Cross Loss: 0.1492\n",
            "177/239, Train_loss: 0.14915934205055237 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [178/239], Dice Loss: 0.9243, Cross Loss: 0.2594\n",
            "178/239, Train_loss: 0.25943854451179504 Train_dice: tensor(0.2594, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [179/239], Dice Loss: 0.9803, Cross Loss: 0.1494\n",
            "179/239, Train_loss: 0.14938871562480927 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [180/239], Dice Loss: 0.8993, Cross Loss: 0.1494\n",
            "180/239, Train_loss: 0.14944937825202942 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [181/239], Dice Loss: 0.8566, Cross Loss: 0.2598\n",
            "181/239, Train_loss: 0.25979775190353394 Train_dice: tensor(0.2598, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [182/239], Dice Loss: 0.8617, Cross Loss: 0.1491\n",
            "182/239, Train_loss: 0.14914122223854065 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [183/239], Dice Loss: 0.9317, Cross Loss: 0.2594\n",
            "183/239, Train_loss: 0.25936996936798096 Train_dice: tensor(0.2594, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [184/239], Dice Loss: 0.9768, Cross Loss: 0.1495\n",
            "184/239, Train_loss: 0.14946232736110687 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [185/239], Dice Loss: 0.7906, Cross Loss: 0.1495\n",
            "185/239, Train_loss: 0.14949189126491547 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [186/239], Dice Loss: 0.8402, Cross Loss: 0.1496\n",
            "186/239, Train_loss: 0.14964431524276733 Train_dice: tensor(0.1496, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [187/239], Dice Loss: 0.9810, Cross Loss: 0.1495\n",
            "187/239, Train_loss: 0.14951631426811218 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [188/239], Dice Loss: 0.6701, Cross Loss: 0.2590\n",
            "188/239, Train_loss: 0.25902241468429565 Train_dice: tensor(0.2590, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [189/239], Dice Loss: 0.8819, Cross Loss: 0.1494\n",
            "189/239, Train_loss: 0.14938504993915558 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [190/239], Dice Loss: 0.9840, Cross Loss: 0.1492\n",
            "190/239, Train_loss: 0.14924991130828857 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [191/239], Dice Loss: 0.8706, Cross Loss: 0.2589\n",
            "191/239, Train_loss: 0.25894999504089355 Train_dice: tensor(0.2589, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [192/239], Dice Loss: 0.9452, Cross Loss: 0.1495\n",
            "192/239, Train_loss: 0.1494508683681488 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [193/239], Dice Loss: 0.9860, Cross Loss: 0.2589\n",
            "193/239, Train_loss: 0.25889700651168823 Train_dice: tensor(0.2589, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [194/239], Dice Loss: 0.9956, Cross Loss: 0.1491\n",
            "194/239, Train_loss: 0.14914876222610474 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [195/239], Dice Loss: 0.9869, Cross Loss: 0.1492\n",
            "195/239, Train_loss: 0.14916075766086578 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [196/239], Dice Loss: 0.9873, Cross Loss: 0.2591\n",
            "196/239, Train_loss: 0.25914931297302246 Train_dice: tensor(0.2591, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [197/239], Dice Loss: 0.9894, Cross Loss: 0.1494\n",
            "197/239, Train_loss: 0.14944833517074585 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [198/239], Dice Loss: 0.9879, Cross Loss: 0.1495\n",
            "198/239, Train_loss: 0.1494957059621811 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [199/239], Dice Loss: 0.7767, Cross Loss: 0.1491\n",
            "199/239, Train_loss: 0.14910036325454712 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [200/239], Dice Loss: 0.8390, Cross Loss: 0.1490\n",
            "200/239, Train_loss: 0.14904922246932983 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [201/239], Dice Loss: 0.9195, Cross Loss: 0.2598\n",
            "201/239, Train_loss: 0.25975221395492554 Train_dice: tensor(0.2598, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [202/239], Dice Loss: 0.8218, Cross Loss: 0.2593\n",
            "202/239, Train_loss: 0.2593034505844116 Train_dice: tensor(0.2593, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [203/239], Dice Loss: 0.9419, Cross Loss: 0.1494\n",
            "203/239, Train_loss: 0.14938372373580933 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [204/239], Dice Loss: 0.9411, Cross Loss: 0.1493\n",
            "204/239, Train_loss: 0.1493057906627655 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [205/239], Dice Loss: 0.8871, Cross Loss: 0.2591\n",
            "205/239, Train_loss: 0.25906965136528015 Train_dice: tensor(0.2591, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [206/239], Dice Loss: 0.9755, Cross Loss: 0.1491\n",
            "206/239, Train_loss: 0.1490916758775711 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [207/239], Dice Loss: 0.9081, Cross Loss: 0.1495\n",
            "207/239, Train_loss: 0.14947864413261414 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [208/239], Dice Loss: 0.8194, Cross Loss: 0.1494\n",
            "208/239, Train_loss: 0.14940857887268066 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [209/239], Dice Loss: 0.9093, Cross Loss: 0.2592\n",
            "209/239, Train_loss: 0.2592366635799408 Train_dice: tensor(0.2592, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [210/239], Dice Loss: 0.9962, Cross Loss: 0.1494\n",
            "210/239, Train_loss: 0.14937305450439453 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [211/239], Dice Loss: 0.9326, Cross Loss: 0.1494\n",
            "211/239, Train_loss: 0.14943420886993408 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [212/239], Dice Loss: 0.9233, Cross Loss: 0.1493\n",
            "212/239, Train_loss: 0.14932921528816223 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [213/239], Dice Loss: 0.6970, Cross Loss: 0.2593\n",
            "213/239, Train_loss: 0.2592821717262268 Train_dice: tensor(0.2593, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [214/239], Dice Loss: 0.8173, Cross Loss: 0.2593\n",
            "214/239, Train_loss: 0.25928911566734314 Train_dice: tensor(0.2593, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [215/239], Dice Loss: 0.8549, Cross Loss: 0.1491\n",
            "215/239, Train_loss: 0.14912360906600952 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [216/239], Dice Loss: 0.9094, Cross Loss: 0.1491\n",
            "216/239, Train_loss: 0.14913266897201538 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [217/239], Dice Loss: 0.7313, Cross Loss: 0.2591\n",
            "217/239, Train_loss: 0.25906193256378174 Train_dice: tensor(0.2591, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [218/239], Dice Loss: 0.7351, Cross Loss: 0.2592\n",
            "218/239, Train_loss: 0.2592485845088959 Train_dice: tensor(0.2592, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [219/239], Dice Loss: 0.8683, Cross Loss: 0.1493\n",
            "219/239, Train_loss: 0.14932647347450256 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [220/239], Dice Loss: 0.9907, Cross Loss: 0.1494\n",
            "220/239, Train_loss: 0.14937622845172882 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [221/239], Dice Loss: 0.9774, Cross Loss: 0.1494\n",
            "221/239, Train_loss: 0.14939714968204498 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [222/239], Dice Loss: 0.9979, Cross Loss: 0.1493\n",
            "222/239, Train_loss: 0.14931292831897736 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [223/239], Dice Loss: 0.9999, Cross Loss: 0.1497\n",
            "223/239, Train_loss: 0.14968442916870117 Train_dice: tensor(0.1497, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [224/239], Dice Loss: 0.6465, Cross Loss: 0.1495\n",
            "224/239, Train_loss: 0.1495310664176941 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [225/239], Dice Loss: 0.9917, Cross Loss: 0.1492\n",
            "225/239, Train_loss: 0.14918845891952515 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [226/239], Dice Loss: 0.9992, Cross Loss: 0.1490\n",
            "226/239, Train_loss: 0.1490018367767334 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [227/239], Dice Loss: 0.9987, Cross Loss: 0.1489\n",
            "227/239, Train_loss: 0.1488681137561798 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [228/239], Dice Loss: 0.9990, Cross Loss: 0.1488\n",
            "228/239, Train_loss: 0.14882045984268188 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [229/239], Dice Loss: 0.9986, Cross Loss: 0.1488\n",
            "229/239, Train_loss: 0.1487557739019394 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [230/239], Dice Loss: 0.8444, Cross Loss: 0.1487\n",
            "230/239, Train_loss: 0.14869946241378784 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [231/239], Dice Loss: 0.9607, Cross Loss: 0.1486\n",
            "231/239, Train_loss: 0.14864560961723328 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [232/239], Dice Loss: 0.9699, Cross Loss: 0.2604\n",
            "232/239, Train_loss: 0.26039180159568787 Train_dice: tensor(0.2604, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [233/239], Dice Loss: 0.8215, Cross Loss: 0.1485\n",
            "233/239, Train_loss: 0.14852012693881989 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [234/239], Dice Loss: 0.9999, Cross Loss: 0.1485\n",
            "234/239, Train_loss: 0.14851124584674835 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [235/239], Dice Loss: 0.6791, Cross Loss: 0.2608\n",
            "235/239, Train_loss: 0.260750949382782 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [236/239], Dice Loss: 0.9899, Cross Loss: 0.2606\n",
            "236/239, Train_loss: 0.26063165068626404 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [237/239], Dice Loss: 0.9999, Cross Loss: 0.1485\n",
            "237/239, Train_loss: 0.14847171306610107 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [238/239], Dice Loss: 0.9780, Cross Loss: 0.1485\n",
            "238/239, Train_loss: 0.14847467839717865 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [239/239], Dice Loss: 0.9945, Cross Loss: 0.2600\n",
            "239/239, Train_loss: 0.26000264286994934 Train_dice: tensor(0.2600, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1868\n",
            "Epoch_metric: tensor(0.1868, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.1865\n",
            "test_dice_epoch: tensor(0.1865, device='cuda:0')\n",
            "current epoch: 1 current mean dice: tensor(0.1865, device='cuda:0')\n",
            "best mean dice: tensor(0.1865, device='cuda:0') at epoch: 1\n",
            "----------\n",
            "epoch 2/100\n",
            "Epoch [2/100], Batch [1/239], Dice Loss: 0.9466, Cross Loss: 0.2596\n",
            "1/239, Train_loss: 0.2596440613269806 Train_dice: tensor(0.2596, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [2/239], Dice Loss: 0.8051, Cross Loss: 0.2600\n",
            "2/239, Train_loss: 0.2600366175174713 Train_dice: tensor(0.2600, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [3/239], Dice Loss: 0.9942, Cross Loss: 0.2579\n",
            "3/239, Train_loss: 0.25785666704177856 Train_dice: tensor(0.2579, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [4/239], Dice Loss: 0.9437, Cross Loss: 0.2570\n",
            "4/239, Train_loss: 0.257007360458374 Train_dice: tensor(0.2570, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [5/239], Dice Loss: 0.9283, Cross Loss: 0.1504\n",
            "5/239, Train_loss: 0.1504037082195282 Train_dice: tensor(0.1504, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [6/239], Dice Loss: 0.9087, Cross Loss: 0.1502\n",
            "6/239, Train_loss: 0.15024901926517487 Train_dice: tensor(0.1502, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [7/239], Dice Loss: 0.7763, Cross Loss: 0.2575\n",
            "7/239, Train_loss: 0.2574708163738251 Train_dice: tensor(0.2575, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [8/239], Dice Loss: 0.9528, Cross Loss: 0.2589\n",
            "8/239, Train_loss: 0.25890395045280457 Train_dice: tensor(0.2589, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [9/239], Dice Loss: 0.9833, Cross Loss: 0.2599\n",
            "9/239, Train_loss: 0.2599044442176819 Train_dice: tensor(0.2599, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [10/239], Dice Loss: 0.8958, Cross Loss: 0.1488\n",
            "10/239, Train_loss: 0.1487710177898407 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [11/239], Dice Loss: 0.8235, Cross Loss: 0.1490\n",
            "11/239, Train_loss: 0.149031400680542 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [12/239], Dice Loss: 0.9251, Cross Loss: 0.1487\n",
            "12/239, Train_loss: 0.14871147274971008 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [13/239], Dice Loss: 0.9591, Cross Loss: 0.1487\n",
            "13/239, Train_loss: 0.14872246980667114 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [14/239], Dice Loss: 0.7791, Cross Loss: 0.1490\n",
            "14/239, Train_loss: 0.14896848797798157 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [15/239], Dice Loss: 0.7974, Cross Loss: 0.1488\n",
            "15/239, Train_loss: 0.14880309998989105 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [16/239], Dice Loss: 0.9287, Cross Loss: 0.1486\n",
            "16/239, Train_loss: 0.14863204956054688 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [17/239], Dice Loss: 0.7858, Cross Loss: 0.1486\n",
            "17/239, Train_loss: 0.1486019343137741 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [18/239], Dice Loss: 0.9715, Cross Loss: 0.1486\n",
            "18/239, Train_loss: 0.1485500931739807 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [19/239], Dice Loss: 0.7967, Cross Loss: 0.2607\n",
            "19/239, Train_loss: 0.2606847286224365 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [20/239], Dice Loss: 0.6524, Cross Loss: 0.2609\n",
            "20/239, Train_loss: 0.2609030604362488 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [21/239], Dice Loss: 0.9662, Cross Loss: 0.2607\n",
            "21/239, Train_loss: 0.2607044577598572 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [22/239], Dice Loss: 0.9471, Cross Loss: 0.1485\n",
            "22/239, Train_loss: 0.14850476384162903 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [23/239], Dice Loss: 0.7334, Cross Loss: 0.2607\n",
            "23/239, Train_loss: 0.2606644332408905 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [24/239], Dice Loss: 0.8330, Cross Loss: 0.2607\n",
            "24/239, Train_loss: 0.2606702446937561 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [25/239], Dice Loss: 0.9395, Cross Loss: 0.1486\n",
            "25/239, Train_loss: 0.1485743373632431 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [26/239], Dice Loss: 0.9401, Cross Loss: 0.1486\n",
            "26/239, Train_loss: 0.14864115417003632 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [27/239], Dice Loss: 0.9651, Cross Loss: 0.1487\n",
            "27/239, Train_loss: 0.14865708351135254 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [28/239], Dice Loss: 0.7693, Cross Loss: 0.1487\n",
            "28/239, Train_loss: 0.14866217970848083 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [29/239], Dice Loss: 0.8906, Cross Loss: 0.1486\n",
            "29/239, Train_loss: 0.14860711991786957 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [30/239], Dice Loss: 0.9866, Cross Loss: 0.1486\n",
            "30/239, Train_loss: 0.14857925474643707 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [31/239], Dice Loss: 0.8554, Cross Loss: 0.1486\n",
            "31/239, Train_loss: 0.148557648062706 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [32/239], Dice Loss: 0.9467, Cross Loss: 0.1485\n",
            "32/239, Train_loss: 0.1485031396150589 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [33/239], Dice Loss: 0.6654, Cross Loss: 0.2609\n",
            "33/239, Train_loss: 0.2608948349952698 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [34/239], Dice Loss: 0.9841, Cross Loss: 0.1485\n",
            "34/239, Train_loss: 0.14848092198371887 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [35/239], Dice Loss: 0.8973, Cross Loss: 0.2608\n",
            "35/239, Train_loss: 0.2607859671115875 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [36/239], Dice Loss: 0.9110, Cross Loss: 0.1485\n",
            "36/239, Train_loss: 0.14846734702587128 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [37/239], Dice Loss: 0.9572, Cross Loss: 0.1485\n",
            "37/239, Train_loss: 0.1484609693288803 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [38/239], Dice Loss: 0.8546, Cross Loss: 0.1485\n",
            "38/239, Train_loss: 0.14845022559165955 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [39/239], Dice Loss: 0.8410, Cross Loss: 0.2608\n",
            "39/239, Train_loss: 0.2608107328414917 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [40/239], Dice Loss: 0.8181, Cross Loss: 0.2608\n",
            "40/239, Train_loss: 0.2608221769332886 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [41/239], Dice Loss: 0.9449, Cross Loss: 0.1484\n",
            "41/239, Train_loss: 0.14843393862247467 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [42/239], Dice Loss: 0.6903, Cross Loss: 0.1484\n",
            "42/239, Train_loss: 0.14842894673347473 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [43/239], Dice Loss: 0.9310, Cross Loss: 0.1484\n",
            "43/239, Train_loss: 0.1484287977218628 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [44/239], Dice Loss: 0.7866, Cross Loss: 0.1484\n",
            "44/239, Train_loss: 0.14842122793197632 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [45/239], Dice Loss: 0.7856, Cross Loss: 0.2609\n",
            "45/239, Train_loss: 0.2609122693538666 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [46/239], Dice Loss: 0.9647, Cross Loss: 0.1484\n",
            "46/239, Train_loss: 0.14840996265411377 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [47/239], Dice Loss: 0.8995, Cross Loss: 0.1484\n",
            "47/239, Train_loss: 0.14841386675834656 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [48/239], Dice Loss: 0.7963, Cross Loss: 0.2609\n",
            "48/239, Train_loss: 0.2608807682991028 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [49/239], Dice Loss: 0.8838, Cross Loss: 0.1484\n",
            "49/239, Train_loss: 0.14844438433647156 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [50/239], Dice Loss: 0.7971, Cross Loss: 0.1484\n",
            "50/239, Train_loss: 0.1484294980764389 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [51/239], Dice Loss: 0.7496, Cross Loss: 0.1484\n",
            "51/239, Train_loss: 0.14843831956386566 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [52/239], Dice Loss: 0.8327, Cross Loss: 0.1484\n",
            "52/239, Train_loss: 0.14843352138996124 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [53/239], Dice Loss: 0.8958, Cross Loss: 0.1484\n",
            "53/239, Train_loss: 0.148440420627594 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [54/239], Dice Loss: 0.8506, Cross Loss: 0.2609\n",
            "54/239, Train_loss: 0.2609347701072693 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [55/239], Dice Loss: 0.7425, Cross Loss: 0.1484\n",
            "55/239, Train_loss: 0.14840516448020935 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [56/239], Dice Loss: 0.9309, Cross Loss: 0.1484\n",
            "56/239, Train_loss: 0.14840060472488403 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [57/239], Dice Loss: 0.8288, Cross Loss: 0.2610\n",
            "57/239, Train_loss: 0.2609540820121765 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [58/239], Dice Loss: 0.8846, Cross Loss: 0.1484\n",
            "58/239, Train_loss: 0.14837254583835602 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [59/239], Dice Loss: 0.8355, Cross Loss: 0.1484\n",
            "59/239, Train_loss: 0.14837253093719482 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [60/239], Dice Loss: 0.8278, Cross Loss: 0.1484\n",
            "60/239, Train_loss: 0.1483592987060547 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [61/239], Dice Loss: 0.9609, Cross Loss: 0.1484\n",
            "61/239, Train_loss: 0.1483539193868637 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [62/239], Dice Loss: 0.8050, Cross Loss: 0.2611\n",
            "62/239, Train_loss: 0.26109588146209717 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [63/239], Dice Loss: 0.8754, Cross Loss: 0.1484\n",
            "63/239, Train_loss: 0.14835557341575623 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [64/239], Dice Loss: 0.9240, Cross Loss: 0.1483\n",
            "64/239, Train_loss: 0.14834675192832947 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [65/239], Dice Loss: 0.7355, Cross Loss: 0.1483\n",
            "65/239, Train_loss: 0.14834460616111755 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [66/239], Dice Loss: 0.7359, Cross Loss: 0.1483\n",
            "66/239, Train_loss: 0.14833402633666992 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [67/239], Dice Loss: 0.7284, Cross Loss: 0.1483\n",
            "67/239, Train_loss: 0.14832937717437744 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [68/239], Dice Loss: 0.8237, Cross Loss: 0.1483\n",
            "68/239, Train_loss: 0.1483292579650879 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [69/239], Dice Loss: 0.9714, Cross Loss: 0.2611\n",
            "69/239, Train_loss: 0.2610880136489868 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [70/239], Dice Loss: 0.8405, Cross Loss: 0.2611\n",
            "70/239, Train_loss: 0.2611008584499359 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [71/239], Dice Loss: 0.7152, Cross Loss: 0.1483\n",
            "71/239, Train_loss: 0.14831486344337463 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [72/239], Dice Loss: 0.8371, Cross Loss: 0.2612\n",
            "72/239, Train_loss: 0.26118865609169006 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [73/239], Dice Loss: 0.8821, Cross Loss: 0.1483\n",
            "73/239, Train_loss: 0.148313969373703 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [74/239], Dice Loss: 0.8874, Cross Loss: 0.1483\n",
            "74/239, Train_loss: 0.14831215143203735 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [75/239], Dice Loss: 0.9636, Cross Loss: 0.1483\n",
            "75/239, Train_loss: 0.14831143617630005 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [76/239], Dice Loss: 0.9351, Cross Loss: 0.1483\n",
            "76/239, Train_loss: 0.14830917119979858 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [77/239], Dice Loss: 0.9581, Cross Loss: 0.1483\n",
            "77/239, Train_loss: 0.14831791818141937 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [78/239], Dice Loss: 0.8583, Cross Loss: 0.1483\n",
            "78/239, Train_loss: 0.14830665290355682 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [79/239], Dice Loss: 0.9432, Cross Loss: 0.1483\n",
            "79/239, Train_loss: 0.14830781519412994 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [80/239], Dice Loss: 0.9006, Cross Loss: 0.1483\n",
            "80/239, Train_loss: 0.14833365380764008 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [81/239], Dice Loss: 0.9683, Cross Loss: 0.1483\n",
            "81/239, Train_loss: 0.1482921540737152 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [82/239], Dice Loss: 0.6921, Cross Loss: 0.1483\n",
            "82/239, Train_loss: 0.14831441640853882 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [83/239], Dice Loss: 0.9333, Cross Loss: 0.1483\n",
            "83/239, Train_loss: 0.14830827713012695 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [84/239], Dice Loss: 0.9531, Cross Loss: 0.1483\n",
            "84/239, Train_loss: 0.14828042685985565 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [85/239], Dice Loss: 0.8372, Cross Loss: 0.2611\n",
            "85/239, Train_loss: 0.26113590598106384 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [86/239], Dice Loss: 0.8187, Cross Loss: 0.1483\n",
            "86/239, Train_loss: 0.14829683303833008 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [87/239], Dice Loss: 0.9449, Cross Loss: 0.1483\n",
            "87/239, Train_loss: 0.1482928991317749 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [88/239], Dice Loss: 0.9829, Cross Loss: 0.1483\n",
            "88/239, Train_loss: 0.14827629923820496 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [89/239], Dice Loss: 0.9877, Cross Loss: 0.1483\n",
            "89/239, Train_loss: 0.14827287197113037 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [90/239], Dice Loss: 0.9510, Cross Loss: 0.1483\n",
            "90/239, Train_loss: 0.14826947450637817 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [91/239], Dice Loss: 0.7914, Cross Loss: 0.2612\n",
            "91/239, Train_loss: 0.2612118422985077 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [92/239], Dice Loss: 0.8693, Cross Loss: 0.1483\n",
            "92/239, Train_loss: 0.14826220273971558 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [93/239], Dice Loss: 0.7752, Cross Loss: 0.1483\n",
            "93/239, Train_loss: 0.1482602059841156 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [94/239], Dice Loss: 0.9323, Cross Loss: 0.2613\n",
            "94/239, Train_loss: 0.2612610459327698 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [95/239], Dice Loss: 0.8509, Cross Loss: 0.1483\n",
            "95/239, Train_loss: 0.14826032519340515 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [96/239], Dice Loss: 0.9794, Cross Loss: 0.1483\n",
            "96/239, Train_loss: 0.14825689792633057 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [97/239], Dice Loss: 0.8581, Cross Loss: 0.1483\n",
            "97/239, Train_loss: 0.14825446903705597 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [98/239], Dice Loss: 0.8473, Cross Loss: 0.1483\n",
            "98/239, Train_loss: 0.1482534557580948 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [99/239], Dice Loss: 0.9669, Cross Loss: 0.2612\n",
            "99/239, Train_loss: 0.26123350858688354 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [100/239], Dice Loss: 0.9870, Cross Loss: 0.2613\n",
            "100/239, Train_loss: 0.261285662651062 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [101/239], Dice Loss: 0.7571, Cross Loss: 0.2613\n",
            "101/239, Train_loss: 0.26127123832702637 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [102/239], Dice Loss: 0.8290, Cross Loss: 0.1483\n",
            "102/239, Train_loss: 0.1482531726360321 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [103/239], Dice Loss: 0.9794, Cross Loss: 0.1482\n",
            "103/239, Train_loss: 0.14824876189231873 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [104/239], Dice Loss: 0.8510, Cross Loss: 0.2612\n",
            "104/239, Train_loss: 0.26123544573783875 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [105/239], Dice Loss: 0.8516, Cross Loss: 0.1483\n",
            "105/239, Train_loss: 0.14825445413589478 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [106/239], Dice Loss: 0.9658, Cross Loss: 0.1483\n",
            "106/239, Train_loss: 0.14825478196144104 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [107/239], Dice Loss: 0.7765, Cross Loss: 0.1483\n",
            "107/239, Train_loss: 0.1482512503862381 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [108/239], Dice Loss: 0.8438, Cross Loss: 0.2612\n",
            "108/239, Train_loss: 0.26124095916748047 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [109/239], Dice Loss: 0.9302, Cross Loss: 0.1482\n",
            "109/239, Train_loss: 0.14824944734573364 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [110/239], Dice Loss: 0.8862, Cross Loss: 0.1482\n",
            "110/239, Train_loss: 0.14824926853179932 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [111/239], Dice Loss: 0.9312, Cross Loss: 0.1482\n",
            "111/239, Train_loss: 0.14824891090393066 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [112/239], Dice Loss: 0.8012, Cross Loss: 0.1482\n",
            "112/239, Train_loss: 0.14824837446212769 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [113/239], Dice Loss: 0.8528, Cross Loss: 0.1483\n",
            "113/239, Train_loss: 0.14827340841293335 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [114/239], Dice Loss: 0.7775, Cross Loss: 0.1483\n",
            "114/239, Train_loss: 0.14827242493629456 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [115/239], Dice Loss: 0.9548, Cross Loss: 0.1482\n",
            "115/239, Train_loss: 0.1482471525669098 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [116/239], Dice Loss: 0.9170, Cross Loss: 0.1482\n",
            "116/239, Train_loss: 0.14824554324150085 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [117/239], Dice Loss: 0.9245, Cross Loss: 0.1482\n",
            "117/239, Train_loss: 0.1482420563697815 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [118/239], Dice Loss: 0.9251, Cross Loss: 0.1482\n",
            "118/239, Train_loss: 0.14824044704437256 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [119/239], Dice Loss: 0.9171, Cross Loss: 0.1482\n",
            "119/239, Train_loss: 0.14824795722961426 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [120/239], Dice Loss: 0.9134, Cross Loss: 0.1482\n",
            "120/239, Train_loss: 0.14823868870735168 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [121/239], Dice Loss: 0.7912, Cross Loss: 0.1482\n",
            "121/239, Train_loss: 0.1482377052307129 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [122/239], Dice Loss: 0.7370, Cross Loss: 0.2613\n",
            "122/239, Train_loss: 0.26130178570747375 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [123/239], Dice Loss: 0.9308, Cross Loss: 0.1482\n",
            "123/239, Train_loss: 0.14823374152183533 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [124/239], Dice Loss: 0.7031, Cross Loss: 0.2613\n",
            "124/239, Train_loss: 0.2612992227077484 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [125/239], Dice Loss: 0.9197, Cross Loss: 0.2613\n",
            "125/239, Train_loss: 0.2613067626953125 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [126/239], Dice Loss: 0.8924, Cross Loss: 0.1482\n",
            "126/239, Train_loss: 0.14823246002197266 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [127/239], Dice Loss: 0.9153, Cross Loss: 0.1482\n",
            "127/239, Train_loss: 0.14823320508003235 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [128/239], Dice Loss: 0.8913, Cross Loss: 0.2613\n",
            "128/239, Train_loss: 0.2612815797328949 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [129/239], Dice Loss: 0.8625, Cross Loss: 0.1482\n",
            "129/239, Train_loss: 0.14822928607463837 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [130/239], Dice Loss: 0.7455, Cross Loss: 0.2613\n",
            "130/239, Train_loss: 0.26128193736076355 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [131/239], Dice Loss: 0.8892, Cross Loss: 0.1482\n",
            "131/239, Train_loss: 0.14823108911514282 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [132/239], Dice Loss: 0.9974, Cross Loss: 0.1482\n",
            "132/239, Train_loss: 0.14822962880134583 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [133/239], Dice Loss: 0.8112, Cross Loss: 0.1482\n",
            "133/239, Train_loss: 0.14822912216186523 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [134/239], Dice Loss: 0.8273, Cross Loss: 0.2613\n",
            "134/239, Train_loss: 0.26128554344177246 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [135/239], Dice Loss: 0.7883, Cross Loss: 0.1482\n",
            "135/239, Train_loss: 0.14822939038276672 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [136/239], Dice Loss: 0.9985, Cross Loss: 0.2613\n",
            "136/239, Train_loss: 0.26128488779067993 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [137/239], Dice Loss: 0.6925, Cross Loss: 0.2613\n",
            "137/239, Train_loss: 0.26131734251976013 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [138/239], Dice Loss: 0.9151, Cross Loss: 0.1482\n",
            "138/239, Train_loss: 0.14823175966739655 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [139/239], Dice Loss: 0.8378, Cross Loss: 0.2613\n",
            "139/239, Train_loss: 0.2612835764884949 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [140/239], Dice Loss: 0.9160, Cross Loss: 0.1482\n",
            "140/239, Train_loss: 0.14823046326637268 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [141/239], Dice Loss: 0.6910, Cross Loss: 0.2613\n",
            "141/239, Train_loss: 0.2612748444080353 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [142/239], Dice Loss: 0.8930, Cross Loss: 0.1482\n",
            "142/239, Train_loss: 0.1482294797897339 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [143/239], Dice Loss: 0.9349, Cross Loss: 0.2613\n",
            "143/239, Train_loss: 0.261311799287796 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [144/239], Dice Loss: 0.8220, Cross Loss: 0.1482\n",
            "144/239, Train_loss: 0.14823755621910095 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [145/239], Dice Loss: 0.7852, Cross Loss: 0.1482\n",
            "145/239, Train_loss: 0.14823167026042938 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [146/239], Dice Loss: 0.8110, Cross Loss: 0.1482\n",
            "146/239, Train_loss: 0.14823178946971893 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [147/239], Dice Loss: 0.8969, Cross Loss: 0.1482\n",
            "147/239, Train_loss: 0.14823219180107117 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [148/239], Dice Loss: 0.9265, Cross Loss: 0.1482\n",
            "148/239, Train_loss: 0.14823195338249207 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [149/239], Dice Loss: 0.6531, Cross Loss: 0.2613\n",
            "149/239, Train_loss: 0.261282742023468 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [150/239], Dice Loss: 0.9922, Cross Loss: 0.1482\n",
            "150/239, Train_loss: 0.14823082089424133 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [151/239], Dice Loss: 0.6956, Cross Loss: 0.2613\n",
            "151/239, Train_loss: 0.26127925515174866 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [152/239], Dice Loss: 0.9805, Cross Loss: 0.1482\n",
            "152/239, Train_loss: 0.14823389053344727 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [153/239], Dice Loss: 0.9077, Cross Loss: 0.1482\n",
            "153/239, Train_loss: 0.14823052287101746 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [154/239], Dice Loss: 0.7279, Cross Loss: 0.2613\n",
            "154/239, Train_loss: 0.2612765431404114 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [155/239], Dice Loss: 0.8400, Cross Loss: 0.1483\n",
            "155/239, Train_loss: 0.14825305342674255 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [156/239], Dice Loss: 0.8322, Cross Loss: 0.2613\n",
            "156/239, Train_loss: 0.2612752318382263 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [157/239], Dice Loss: 0.8828, Cross Loss: 0.2613\n",
            "157/239, Train_loss: 0.2612822353839874 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [158/239], Dice Loss: 0.7775, Cross Loss: 0.2613\n",
            "158/239, Train_loss: 0.26127737760543823 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [159/239], Dice Loss: 0.9223, Cross Loss: 0.1482\n",
            "159/239, Train_loss: 0.14823314547538757 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [160/239], Dice Loss: 0.8039, Cross Loss: 0.2613\n",
            "160/239, Train_loss: 0.2612728476524353 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [161/239], Dice Loss: 0.9054, Cross Loss: 0.1482\n",
            "161/239, Train_loss: 0.14823517203330994 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [162/239], Dice Loss: 0.9608, Cross Loss: 0.2613\n",
            "162/239, Train_loss: 0.26127249002456665 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [163/239], Dice Loss: 0.9934, Cross Loss: 0.1482\n",
            "163/239, Train_loss: 0.14824137091636658 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [164/239], Dice Loss: 0.9672, Cross Loss: 0.1482\n",
            "164/239, Train_loss: 0.14824199676513672 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [165/239], Dice Loss: 0.7572, Cross Loss: 0.2613\n",
            "165/239, Train_loss: 0.261269748210907 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [166/239], Dice Loss: 0.9896, Cross Loss: 0.2613\n",
            "166/239, Train_loss: 0.2612634301185608 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [167/239], Dice Loss: 0.9181, Cross Loss: 0.2613\n",
            "167/239, Train_loss: 0.261267751455307 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [168/239], Dice Loss: 0.7964, Cross Loss: 0.2613\n",
            "168/239, Train_loss: 0.2612648010253906 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [169/239], Dice Loss: 0.8883, Cross Loss: 0.2613\n",
            "169/239, Train_loss: 0.26129150390625 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [170/239], Dice Loss: 0.9698, Cross Loss: 0.2612\n",
            "170/239, Train_loss: 0.26124680042266846 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [171/239], Dice Loss: 0.8999, Cross Loss: 0.2612\n",
            "171/239, Train_loss: 0.2612420320510864 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [172/239], Dice Loss: 0.8902, Cross Loss: 0.1483\n",
            "172/239, Train_loss: 0.14825350046157837 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [173/239], Dice Loss: 0.8783, Cross Loss: 0.2613\n",
            "173/239, Train_loss: 0.26128363609313965 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [174/239], Dice Loss: 0.7579, Cross Loss: 0.2612\n",
            "174/239, Train_loss: 0.26122722029685974 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [175/239], Dice Loss: 0.9216, Cross Loss: 0.2612\n",
            "175/239, Train_loss: 0.26121580600738525 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [176/239], Dice Loss: 0.8473, Cross Loss: 0.1483\n",
            "176/239, Train_loss: 0.1482636034488678 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [177/239], Dice Loss: 0.7354, Cross Loss: 0.1483\n",
            "177/239, Train_loss: 0.14826659858226776 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [178/239], Dice Loss: 0.8992, Cross Loss: 0.2612\n",
            "178/239, Train_loss: 0.26119816303253174 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [179/239], Dice Loss: 0.9846, Cross Loss: 0.1483\n",
            "179/239, Train_loss: 0.1482740342617035 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [180/239], Dice Loss: 0.7995, Cross Loss: 0.1483\n",
            "180/239, Train_loss: 0.14827625453472137 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [181/239], Dice Loss: 0.9375, Cross Loss: 0.2613\n",
            "181/239, Train_loss: 0.26126164197921753 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [182/239], Dice Loss: 0.8041, Cross Loss: 0.1483\n",
            "182/239, Train_loss: 0.14827440679073334 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [183/239], Dice Loss: 0.9635, Cross Loss: 0.2612\n",
            "183/239, Train_loss: 0.26119595766067505 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [184/239], Dice Loss: 0.9270, Cross Loss: 0.1483\n",
            "184/239, Train_loss: 0.14828018844127655 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [185/239], Dice Loss: 0.8833, Cross Loss: 0.1483\n",
            "185/239, Train_loss: 0.1482817530632019 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [186/239], Dice Loss: 0.7513, Cross Loss: 0.1483\n",
            "186/239, Train_loss: 0.14828339219093323 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [187/239], Dice Loss: 0.9654, Cross Loss: 0.1483\n",
            "187/239, Train_loss: 0.14828358590602875 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [188/239], Dice Loss: 0.7309, Cross Loss: 0.2612\n",
            "188/239, Train_loss: 0.2611663341522217 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [189/239], Dice Loss: 0.8055, Cross Loss: 0.1483\n",
            "189/239, Train_loss: 0.14829033613204956 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [190/239], Dice Loss: 0.9700, Cross Loss: 0.1483\n",
            "190/239, Train_loss: 0.14828640222549438 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [191/239], Dice Loss: 0.8160, Cross Loss: 0.2612\n",
            "191/239, Train_loss: 0.26117050647735596 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [192/239], Dice Loss: 0.9340, Cross Loss: 0.1483\n",
            "192/239, Train_loss: 0.14828075468540192 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [193/239], Dice Loss: 0.9854, Cross Loss: 0.2612\n",
            "193/239, Train_loss: 0.2611848711967468 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [194/239], Dice Loss: 0.9923, Cross Loss: 0.1483\n",
            "194/239, Train_loss: 0.1482844352722168 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [195/239], Dice Loss: 0.9918, Cross Loss: 0.1483\n",
            "195/239, Train_loss: 0.1482848972082138 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [196/239], Dice Loss: 0.9983, Cross Loss: 0.2612\n",
            "196/239, Train_loss: 0.2611689567565918 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [197/239], Dice Loss: 0.9987, Cross Loss: 0.1483\n",
            "197/239, Train_loss: 0.14828969538211823 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [198/239], Dice Loss: 0.9985, Cross Loss: 0.1483\n",
            "198/239, Train_loss: 0.1482902467250824 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [199/239], Dice Loss: 0.8177, Cross Loss: 0.1483\n",
            "199/239, Train_loss: 0.14828717708587646 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [200/239], Dice Loss: 0.7896, Cross Loss: 0.1483\n",
            "200/239, Train_loss: 0.14828623831272125 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [201/239], Dice Loss: 0.8954, Cross Loss: 0.2612\n",
            "201/239, Train_loss: 0.26117509603500366 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [202/239], Dice Loss: 0.7516, Cross Loss: 0.2612\n",
            "202/239, Train_loss: 0.2611628770828247 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [203/239], Dice Loss: 0.8722, Cross Loss: 0.1483\n",
            "203/239, Train_loss: 0.14828775823116302 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [204/239], Dice Loss: 0.9840, Cross Loss: 0.1483\n",
            "204/239, Train_loss: 0.14828839898109436 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [205/239], Dice Loss: 0.8379, Cross Loss: 0.2612\n",
            "205/239, Train_loss: 0.2611733675003052 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [206/239], Dice Loss: 0.9243, Cross Loss: 0.1483\n",
            "206/239, Train_loss: 0.14828144013881683 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [207/239], Dice Loss: 0.8146, Cross Loss: 0.1483\n",
            "207/239, Train_loss: 0.14828632771968842 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [208/239], Dice Loss: 0.8828, Cross Loss: 0.1483\n",
            "208/239, Train_loss: 0.1482858657836914 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [209/239], Dice Loss: 0.8916, Cross Loss: 0.2612\n",
            "209/239, Train_loss: 0.261154443025589 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [210/239], Dice Loss: 0.9992, Cross Loss: 0.1483\n",
            "210/239, Train_loss: 0.1482880562543869 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [211/239], Dice Loss: 0.8850, Cross Loss: 0.1483\n",
            "211/239, Train_loss: 0.14828841388225555 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [212/239], Dice Loss: 0.8814, Cross Loss: 0.1483\n",
            "212/239, Train_loss: 0.14828768372535706 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [213/239], Dice Loss: 0.6979, Cross Loss: 0.2612\n",
            "213/239, Train_loss: 0.26117295026779175 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [214/239], Dice Loss: 0.7397, Cross Loss: 0.2612\n",
            "214/239, Train_loss: 0.2611718475818634 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [215/239], Dice Loss: 0.7425, Cross Loss: 0.1483\n",
            "215/239, Train_loss: 0.14828576147556305 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [216/239], Dice Loss: 0.8666, Cross Loss: 0.1483\n",
            "216/239, Train_loss: 0.1482861042022705 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [217/239], Dice Loss: 0.6941, Cross Loss: 0.2611\n",
            "217/239, Train_loss: 0.26114481687545776 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [218/239], Dice Loss: 0.6811, Cross Loss: 0.2612\n",
            "218/239, Train_loss: 0.26116856932640076 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [219/239], Dice Loss: 0.8568, Cross Loss: 0.1483\n",
            "219/239, Train_loss: 0.1482895314693451 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [220/239], Dice Loss: 0.9836, Cross Loss: 0.1483\n",
            "220/239, Train_loss: 0.14829012751579285 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [221/239], Dice Loss: 0.9942, Cross Loss: 0.1483\n",
            "221/239, Train_loss: 0.14832419157028198 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [222/239], Dice Loss: 0.9992, Cross Loss: 0.1483\n",
            "222/239, Train_loss: 0.14829008281230927 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [223/239], Dice Loss: 0.9920, Cross Loss: 0.1484\n",
            "223/239, Train_loss: 0.14836350083351135 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [224/239], Dice Loss: 0.6128, Cross Loss: 0.1484\n",
            "224/239, Train_loss: 0.14836221933364868 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [225/239], Dice Loss: 0.9842, Cross Loss: 0.1483\n",
            "225/239, Train_loss: 0.14831407368183136 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [226/239], Dice Loss: 0.9993, Cross Loss: 0.1483\n",
            "226/239, Train_loss: 0.1483154296875 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [227/239], Dice Loss: 0.9999, Cross Loss: 0.1483\n",
            "227/239, Train_loss: 0.14830881357192993 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [228/239], Dice Loss: 0.9982, Cross Loss: 0.1483\n",
            "228/239, Train_loss: 0.14829912781715393 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [229/239], Dice Loss: 0.9984, Cross Loss: 0.1483\n",
            "229/239, Train_loss: 0.14831708371639252 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [230/239], Dice Loss: 0.9957, Cross Loss: 0.1483\n",
            "230/239, Train_loss: 0.14830680191516876 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [231/239], Dice Loss: 0.9509, Cross Loss: 0.1483\n",
            "231/239, Train_loss: 0.1482948511838913 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [232/239], Dice Loss: 0.9824, Cross Loss: 0.2612\n",
            "232/239, Train_loss: 0.26120835542678833 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [233/239], Dice Loss: 0.7613, Cross Loss: 0.1483\n",
            "233/239, Train_loss: 0.14826302230358124 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [234/239], Dice Loss: 0.9992, Cross Loss: 0.1483\n",
            "234/239, Train_loss: 0.14826110005378723 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [235/239], Dice Loss: 0.6054, Cross Loss: 0.2613\n",
            "235/239, Train_loss: 0.26126718521118164 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [236/239], Dice Loss: 0.9958, Cross Loss: 0.2612\n",
            "236/239, Train_loss: 0.2612173557281494 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [237/239], Dice Loss: 0.9999, Cross Loss: 0.1483\n",
            "237/239, Train_loss: 0.14825914800167084 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [238/239], Dice Loss: 0.9243, Cross Loss: 0.1483\n",
            "238/239, Train_loss: 0.14826273918151855 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [239/239], Dice Loss: 0.9983, Cross Loss: 0.2612\n",
            "239/239, Train_loss: 0.26122361421585083 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1860\n",
            "Epoch_metric: tensor(0.1860, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.1860\n",
            "test_dice_epoch: tensor(0.1860, device='cuda:0')\n",
            "current epoch: 2 current mean dice: tensor(0.1860, device='cuda:0')\n",
            "best mean dice: tensor(0.1865, device='cuda:0') at epoch: 1\n",
            "----------\n",
            "epoch 3/100\n",
            "Epoch [3/100], Batch [1/239], Dice Loss: 0.9701, Cross Loss: 0.2613\n",
            "1/239, Train_loss: 0.2612703740596771 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [2/239], Dice Loss: 0.7543, Cross Loss: 0.2612\n",
            "2/239, Train_loss: 0.26122117042541504 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [3/239], Dice Loss: 0.9677, Cross Loss: 0.2612\n",
            "3/239, Train_loss: 0.26121947169303894 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [4/239], Dice Loss: 0.9312, Cross Loss: 0.2612\n",
            "4/239, Train_loss: 0.26121336221694946 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [5/239], Dice Loss: 0.9170, Cross Loss: 0.1483\n",
            "5/239, Train_loss: 0.14825986325740814 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [6/239], Dice Loss: 0.9689, Cross Loss: 0.1483\n",
            "6/239, Train_loss: 0.14826148748397827 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [7/239], Dice Loss: 0.7443, Cross Loss: 0.2612\n",
            "7/239, Train_loss: 0.26120680570602417 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [8/239], Dice Loss: 0.9470, Cross Loss: 0.2612\n",
            "8/239, Train_loss: 0.26119673252105713 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [9/239], Dice Loss: 0.9767, Cross Loss: 0.2612\n",
            "9/239, Train_loss: 0.26124581694602966 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [10/239], Dice Loss: 0.8231, Cross Loss: 0.1483\n",
            "10/239, Train_loss: 0.14827480912208557 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [11/239], Dice Loss: 0.7757, Cross Loss: 0.1483\n",
            "11/239, Train_loss: 0.14827841520309448 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [12/239], Dice Loss: 0.8638, Cross Loss: 0.1483\n",
            "12/239, Train_loss: 0.14827968180179596 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [13/239], Dice Loss: 0.9798, Cross Loss: 0.1483\n",
            "13/239, Train_loss: 0.14827877283096313 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [14/239], Dice Loss: 0.7612, Cross Loss: 0.1483\n",
            "14/239, Train_loss: 0.14828309416770935 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [15/239], Dice Loss: 0.7661, Cross Loss: 0.1483\n",
            "15/239, Train_loss: 0.14828327298164368 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [16/239], Dice Loss: 0.8912, Cross Loss: 0.1483\n",
            "16/239, Train_loss: 0.1482815444469452 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [17/239], Dice Loss: 0.7950, Cross Loss: 0.1483\n",
            "17/239, Train_loss: 0.14828012883663177 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [18/239], Dice Loss: 0.9522, Cross Loss: 0.1483\n",
            "18/239, Train_loss: 0.14827333390712738 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [19/239], Dice Loss: 0.7474, Cross Loss: 0.2612\n",
            "19/239, Train_loss: 0.2611836791038513 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [20/239], Dice Loss: 0.6150, Cross Loss: 0.2612\n",
            "20/239, Train_loss: 0.26124608516693115 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [21/239], Dice Loss: 0.9540, Cross Loss: 0.2612\n",
            "21/239, Train_loss: 0.2611922025680542 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [22/239], Dice Loss: 0.9410, Cross Loss: 0.1483\n",
            "22/239, Train_loss: 0.14827649295330048 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [23/239], Dice Loss: 0.7819, Cross Loss: 0.2612\n",
            "23/239, Train_loss: 0.26119181513786316 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [24/239], Dice Loss: 0.6523, Cross Loss: 0.2612\n",
            "24/239, Train_loss: 0.26118069887161255 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [25/239], Dice Loss: 0.9553, Cross Loss: 0.1483\n",
            "25/239, Train_loss: 0.14827945828437805 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [26/239], Dice Loss: 0.9244, Cross Loss: 0.1483\n",
            "26/239, Train_loss: 0.14828504621982574 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [27/239], Dice Loss: 0.9736, Cross Loss: 0.1483\n",
            "27/239, Train_loss: 0.14828604459762573 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [28/239], Dice Loss: 0.8005, Cross Loss: 0.1483\n",
            "28/239, Train_loss: 0.14828239381313324 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [29/239], Dice Loss: 0.8998, Cross Loss: 0.1483\n",
            "29/239, Train_loss: 0.1482865810394287 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [30/239], Dice Loss: 0.9797, Cross Loss: 0.1483\n",
            "30/239, Train_loss: 0.1482853889465332 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [31/239], Dice Loss: 0.7949, Cross Loss: 0.1483\n",
            "31/239, Train_loss: 0.14828237891197205 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [32/239], Dice Loss: 0.9421, Cross Loss: 0.1483\n",
            "32/239, Train_loss: 0.1482773721218109 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [33/239], Dice Loss: 0.5411, Cross Loss: 0.2613\n",
            "33/239, Train_loss: 0.2612544298171997 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [34/239], Dice Loss: 0.9864, Cross Loss: 0.1483\n",
            "34/239, Train_loss: 0.14827284216880798 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [35/239], Dice Loss: 0.9021, Cross Loss: 0.2612\n",
            "35/239, Train_loss: 0.26118916273117065 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [36/239], Dice Loss: 0.8969, Cross Loss: 0.1483\n",
            "36/239, Train_loss: 0.1482749879360199 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [37/239], Dice Loss: 0.9218, Cross Loss: 0.1483\n",
            "37/239, Train_loss: 0.1482723504304886 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [38/239], Dice Loss: 0.7874, Cross Loss: 0.1483\n",
            "38/239, Train_loss: 0.1482764035463333 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [39/239], Dice Loss: 0.7795, Cross Loss: 0.2612\n",
            "39/239, Train_loss: 0.26119333505630493 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [40/239], Dice Loss: 0.8124, Cross Loss: 0.2612\n",
            "40/239, Train_loss: 0.26119330525398254 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [41/239], Dice Loss: 0.9891, Cross Loss: 0.1483\n",
            "41/239, Train_loss: 0.14827144145965576 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [42/239], Dice Loss: 0.5793, Cross Loss: 0.1483\n",
            "42/239, Train_loss: 0.14827200770378113 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [43/239], Dice Loss: 0.9452, Cross Loss: 0.1483\n",
            "43/239, Train_loss: 0.14827190339565277 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [44/239], Dice Loss: 0.6628, Cross Loss: 0.1483\n",
            "44/239, Train_loss: 0.14827030897140503 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [45/239], Dice Loss: 0.7629, Cross Loss: 0.2612\n",
            "45/239, Train_loss: 0.2612000107765198 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [46/239], Dice Loss: 0.9728, Cross Loss: 0.1483\n",
            "46/239, Train_loss: 0.14827001094818115 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [47/239], Dice Loss: 0.7900, Cross Loss: 0.1483\n",
            "47/239, Train_loss: 0.14826762676239014 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [48/239], Dice Loss: 0.8745, Cross Loss: 0.2612\n",
            "48/239, Train_loss: 0.2611919939517975 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [49/239], Dice Loss: 0.9012, Cross Loss: 0.1483\n",
            "49/239, Train_loss: 0.14827430248260498 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [50/239], Dice Loss: 0.8005, Cross Loss: 0.1483\n",
            "50/239, Train_loss: 0.14826995134353638 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [51/239], Dice Loss: 0.6076, Cross Loss: 0.1483\n",
            "51/239, Train_loss: 0.14826995134353638 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [52/239], Dice Loss: 0.8027, Cross Loss: 0.1483\n",
            "52/239, Train_loss: 0.14826953411102295 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [53/239], Dice Loss: 0.7976, Cross Loss: 0.1483\n",
            "53/239, Train_loss: 0.14826154708862305 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [54/239], Dice Loss: 0.7803, Cross Loss: 0.2612\n",
            "54/239, Train_loss: 0.2611762285232544 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [55/239], Dice Loss: 0.6085, Cross Loss: 0.1483\n",
            "55/239, Train_loss: 0.1482817530632019 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [56/239], Dice Loss: 0.9474, Cross Loss: 0.1483\n",
            "56/239, Train_loss: 0.14827996492385864 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [57/239], Dice Loss: 0.7272, Cross Loss: 0.2612\n",
            "57/239, Train_loss: 0.2612200975418091 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [58/239], Dice Loss: 0.9925, Cross Loss: 0.1483\n",
            "58/239, Train_loss: 0.1482582688331604 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [59/239], Dice Loss: 0.9711, Cross Loss: 0.1483\n",
            "59/239, Train_loss: 0.14825740456581116 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [60/239], Dice Loss: 0.6812, Cross Loss: 0.1483\n",
            "60/239, Train_loss: 0.14825987815856934 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [61/239], Dice Loss: 0.9835, Cross Loss: 0.1483\n",
            "61/239, Train_loss: 0.14825351536273956 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [62/239], Dice Loss: 0.6966, Cross Loss: 0.2613\n",
            "62/239, Train_loss: 0.2612566649913788 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [63/239], Dice Loss: 0.8736, Cross Loss: 0.1483\n",
            "63/239, Train_loss: 0.1482643336057663 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [64/239], Dice Loss: 0.7155, Cross Loss: 0.1483\n",
            "64/239, Train_loss: 0.1482548713684082 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [65/239], Dice Loss: 0.6725, Cross Loss: 0.1483\n",
            "65/239, Train_loss: 0.14825591444969177 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [66/239], Dice Loss: 0.6345, Cross Loss: 0.1483\n",
            "66/239, Train_loss: 0.14825654029846191 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [67/239], Dice Loss: 0.9821, Cross Loss: 0.1483\n",
            "67/239, Train_loss: 0.14825014770030975 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [68/239], Dice Loss: 0.7886, Cross Loss: 0.1483\n",
            "68/239, Train_loss: 0.14825046062469482 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [69/239], Dice Loss: 0.9891, Cross Loss: 0.2612\n",
            "69/239, Train_loss: 0.2612435221672058 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [70/239], Dice Loss: 0.9852, Cross Loss: 0.2612\n",
            "70/239, Train_loss: 0.26124200224876404 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [71/239], Dice Loss: 0.5667, Cross Loss: 0.1482\n",
            "71/239, Train_loss: 0.14824822545051575 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [72/239], Dice Loss: 0.9740, Cross Loss: 0.2613\n",
            "72/239, Train_loss: 0.2612858712673187 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [73/239], Dice Loss: 0.9823, Cross Loss: 0.1482\n",
            "73/239, Train_loss: 0.14824925363063812 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [74/239], Dice Loss: 0.8594, Cross Loss: 0.1482\n",
            "74/239, Train_loss: 0.1482487916946411 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [75/239], Dice Loss: 0.9752, Cross Loss: 0.1483\n",
            "75/239, Train_loss: 0.14825020730495453 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [76/239], Dice Loss: 0.8735, Cross Loss: 0.1482\n",
            "76/239, Train_loss: 0.14824606478214264 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [77/239], Dice Loss: 0.9943, Cross Loss: 0.1482\n",
            "77/239, Train_loss: 0.14824208617210388 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [78/239], Dice Loss: 0.9515, Cross Loss: 0.1482\n",
            "78/239, Train_loss: 0.14824333786964417 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [79/239], Dice Loss: 0.9867, Cross Loss: 0.1482\n",
            "79/239, Train_loss: 0.1482435166835785 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [80/239], Dice Loss: 0.8237, Cross Loss: 0.1482\n",
            "80/239, Train_loss: 0.14823931455612183 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [81/239], Dice Loss: 0.9937, Cross Loss: 0.1482\n",
            "81/239, Train_loss: 0.1482428014278412 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [82/239], Dice Loss: 0.5008, Cross Loss: 0.1482\n",
            "82/239, Train_loss: 0.1482386589050293 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [83/239], Dice Loss: 0.9301, Cross Loss: 0.1482\n",
            "83/239, Train_loss: 0.14823739230632782 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [84/239], Dice Loss: 0.9436, Cross Loss: 0.1482\n",
            "84/239, Train_loss: 0.14823594689369202 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [85/239], Dice Loss: 0.8032, Cross Loss: 0.2613\n",
            "85/239, Train_loss: 0.26127150654792786 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [86/239], Dice Loss: 0.7942, Cross Loss: 0.1482\n",
            "86/239, Train_loss: 0.14823377132415771 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [87/239], Dice Loss: 0.9278, Cross Loss: 0.1482\n",
            "87/239, Train_loss: 0.14823293685913086 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [88/239], Dice Loss: 0.9482, Cross Loss: 0.1482\n",
            "88/239, Train_loss: 0.1482306867837906 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [89/239], Dice Loss: 0.9756, Cross Loss: 0.1482\n",
            "89/239, Train_loss: 0.14822974801063538 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [90/239], Dice Loss: 0.8947, Cross Loss: 0.1482\n",
            "90/239, Train_loss: 0.1482287496328354 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [91/239], Dice Loss: 0.7345, Cross Loss: 0.2613\n",
            "91/239, Train_loss: 0.2612815499305725 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [92/239], Dice Loss: 0.9320, Cross Loss: 0.1482\n",
            "92/239, Train_loss: 0.14822593331336975 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [93/239], Dice Loss: 0.9055, Cross Loss: 0.1482\n",
            "93/239, Train_loss: 0.14822524785995483 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [94/239], Dice Loss: 0.8806, Cross Loss: 0.2613\n",
            "94/239, Train_loss: 0.26131361722946167 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [95/239], Dice Loss: 0.8628, Cross Loss: 0.1482\n",
            "95/239, Train_loss: 0.1482270509004593 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [96/239], Dice Loss: 0.9595, Cross Loss: 0.1482\n",
            "96/239, Train_loss: 0.14822575449943542 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [97/239], Dice Loss: 0.8881, Cross Loss: 0.1482\n",
            "97/239, Train_loss: 0.14822494983673096 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [98/239], Dice Loss: 0.8442, Cross Loss: 0.1482\n",
            "98/239, Train_loss: 0.14822128415107727 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [99/239], Dice Loss: 0.8975, Cross Loss: 0.2613\n",
            "99/239, Train_loss: 0.2612926959991455 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [100/239], Dice Loss: 0.9678, Cross Loss: 0.2613\n",
            "100/239, Train_loss: 0.26132526993751526 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [101/239], Dice Loss: 0.8168, Cross Loss: 0.2613\n",
            "101/239, Train_loss: 0.26131346821784973 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [102/239], Dice Loss: 0.8676, Cross Loss: 0.1482\n",
            "102/239, Train_loss: 0.14822262525558472 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [103/239], Dice Loss: 0.8634, Cross Loss: 0.1482\n",
            "103/239, Train_loss: 0.14822323620319366 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [104/239], Dice Loss: 0.8867, Cross Loss: 0.2613\n",
            "104/239, Train_loss: 0.2612904906272888 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [105/239], Dice Loss: 0.8094, Cross Loss: 0.1482\n",
            "105/239, Train_loss: 0.14822757244110107 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [106/239], Dice Loss: 0.9815, Cross Loss: 0.1482\n",
            "106/239, Train_loss: 0.14822779595851898 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [107/239], Dice Loss: 0.7758, Cross Loss: 0.1482\n",
            "107/239, Train_loss: 0.14822368323802948 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [108/239], Dice Loss: 0.8331, Cross Loss: 0.2613\n",
            "108/239, Train_loss: 0.2612980008125305 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [109/239], Dice Loss: 0.8774, Cross Loss: 0.1482\n",
            "109/239, Train_loss: 0.14822272956371307 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [110/239], Dice Loss: 0.9409, Cross Loss: 0.1482\n",
            "110/239, Train_loss: 0.14822262525558472 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [111/239], Dice Loss: 0.9529, Cross Loss: 0.1482\n",
            "111/239, Train_loss: 0.14822247624397278 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [112/239], Dice Loss: 0.7772, Cross Loss: 0.1482\n",
            "112/239, Train_loss: 0.1482221931219101 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [113/239], Dice Loss: 0.8858, Cross Loss: 0.1482\n",
            "113/239, Train_loss: 0.14822286367416382 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [114/239], Dice Loss: 0.7837, Cross Loss: 0.1482\n",
            "114/239, Train_loss: 0.14822247624397278 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [115/239], Dice Loss: 0.9506, Cross Loss: 0.1482\n",
            "115/239, Train_loss: 0.14822277426719666 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [116/239], Dice Loss: 0.9092, Cross Loss: 0.1482\n",
            "116/239, Train_loss: 0.14822211861610413 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [117/239], Dice Loss: 0.9290, Cross Loss: 0.1482\n",
            "117/239, Train_loss: 0.14821851253509521 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [118/239], Dice Loss: 0.9259, Cross Loss: 0.1482\n",
            "118/239, Train_loss: 0.1482178270816803 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [119/239], Dice Loss: 0.9027, Cross Loss: 0.1482\n",
            "119/239, Train_loss: 0.14821864664554596 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [120/239], Dice Loss: 0.9243, Cross Loss: 0.1482\n",
            "120/239, Train_loss: 0.14821791648864746 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [121/239], Dice Loss: 0.7834, Cross Loss: 0.1482\n",
            "121/239, Train_loss: 0.14821717143058777 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [122/239], Dice Loss: 0.7245, Cross Loss: 0.2613\n",
            "122/239, Train_loss: 0.2612893283367157 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [123/239], Dice Loss: 0.9182, Cross Loss: 0.1482\n",
            "123/239, Train_loss: 0.14822152256965637 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [124/239], Dice Loss: 0.6986, Cross Loss: 0.2613\n",
            "124/239, Train_loss: 0.26129722595214844 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [125/239], Dice Loss: 0.8723, Cross Loss: 0.2613\n",
            "125/239, Train_loss: 0.26133453845977783 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [126/239], Dice Loss: 0.8868, Cross Loss: 0.1482\n",
            "126/239, Train_loss: 0.14821507036685944 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [127/239], Dice Loss: 0.8933, Cross Loss: 0.1482\n",
            "127/239, Train_loss: 0.14821356534957886 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [128/239], Dice Loss: 0.9061, Cross Loss: 0.2613\n",
            "128/239, Train_loss: 0.26130902767181396 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [129/239], Dice Loss: 0.8333, Cross Loss: 0.1482\n",
            "129/239, Train_loss: 0.14821353554725647 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [130/239], Dice Loss: 0.7169, Cross Loss: 0.2613\n",
            "130/239, Train_loss: 0.26131299138069153 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [131/239], Dice Loss: 0.8472, Cross Loss: 0.1482\n",
            "131/239, Train_loss: 0.14821556210517883 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [132/239], Dice Loss: 0.9982, Cross Loss: 0.1482\n",
            "132/239, Train_loss: 0.14821431040763855 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [133/239], Dice Loss: 0.7248, Cross Loss: 0.1482\n",
            "133/239, Train_loss: 0.1482141762971878 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [134/239], Dice Loss: 0.8352, Cross Loss: 0.2613\n",
            "134/239, Train_loss: 0.26131686568260193 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [135/239], Dice Loss: 0.6443, Cross Loss: 0.1482\n",
            "135/239, Train_loss: 0.14821425080299377 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [136/239], Dice Loss: 0.9974, Cross Loss: 0.2613\n",
            "136/239, Train_loss: 0.2613145709037781 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [137/239], Dice Loss: 0.5685, Cross Loss: 0.2613\n",
            "137/239, Train_loss: 0.26133808493614197 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [138/239], Dice Loss: 0.8826, Cross Loss: 0.1482\n",
            "138/239, Train_loss: 0.14821617305278778 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [139/239], Dice Loss: 0.8606, Cross Loss: 0.2613\n",
            "139/239, Train_loss: 0.26131361722946167 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [140/239], Dice Loss: 0.9600, Cross Loss: 0.1482\n",
            "140/239, Train_loss: 0.14821526408195496 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [141/239], Dice Loss: 0.8308, Cross Loss: 0.2613\n",
            "141/239, Train_loss: 0.2613094747066498 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [142/239], Dice Loss: 0.9039, Cross Loss: 0.1482\n",
            "142/239, Train_loss: 0.14821550250053406 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [143/239], Dice Loss: 0.9709, Cross Loss: 0.2613\n",
            "143/239, Train_loss: 0.2613333463668823 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [144/239], Dice Loss: 0.9070, Cross Loss: 0.1482\n",
            "144/239, Train_loss: 0.14821641147136688 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [145/239], Dice Loss: 0.7009, Cross Loss: 0.1482\n",
            "145/239, Train_loss: 0.14821703732013702 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [146/239], Dice Loss: 0.8468, Cross Loss: 0.1482\n",
            "146/239, Train_loss: 0.14821727573871613 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [147/239], Dice Loss: 0.9207, Cross Loss: 0.1482\n",
            "147/239, Train_loss: 0.1482165902853012 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [148/239], Dice Loss: 0.9359, Cross Loss: 0.1482\n",
            "148/239, Train_loss: 0.14821667969226837 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [149/239], Dice Loss: 0.5693, Cross Loss: 0.2613\n",
            "149/239, Train_loss: 0.26131319999694824 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [150/239], Dice Loss: 0.9906, Cross Loss: 0.1482\n",
            "150/239, Train_loss: 0.14821605384349823 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [151/239], Dice Loss: 0.7088, Cross Loss: 0.2613\n",
            "151/239, Train_loss: 0.2613092064857483 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [152/239], Dice Loss: 0.9558, Cross Loss: 0.1482\n",
            "152/239, Train_loss: 0.1482168734073639 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [153/239], Dice Loss: 0.8932, Cross Loss: 0.1482\n",
            "153/239, Train_loss: 0.14821544289588928 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [154/239], Dice Loss: 0.7095, Cross Loss: 0.2613\n",
            "154/239, Train_loss: 0.2613191306591034 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [155/239], Dice Loss: 0.8649, Cross Loss: 0.1482\n",
            "155/239, Train_loss: 0.14822602272033691 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [156/239], Dice Loss: 0.8146, Cross Loss: 0.2612\n",
            "156/239, Train_loss: 0.2612296938896179 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [157/239], Dice Loss: 0.8344, Cross Loss: 0.2613\n",
            "157/239, Train_loss: 0.2612699270248413 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [158/239], Dice Loss: 0.7179, Cross Loss: 0.2613\n",
            "158/239, Train_loss: 0.2612583041191101 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [159/239], Dice Loss: 0.9306, Cross Loss: 0.1482\n",
            "159/239, Train_loss: 0.1482412964105606 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [160/239], Dice Loss: 0.8094, Cross Loss: 0.2613\n",
            "160/239, Train_loss: 0.2612997591495514 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [161/239], Dice Loss: 0.8487, Cross Loss: 0.1482\n",
            "161/239, Train_loss: 0.14822246134281158 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [162/239], Dice Loss: 0.9254, Cross Loss: 0.2613\n",
            "162/239, Train_loss: 0.2613012194633484 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [163/239], Dice Loss: 0.9927, Cross Loss: 0.1482\n",
            "163/239, Train_loss: 0.14822348952293396 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [164/239], Dice Loss: 0.9323, Cross Loss: 0.1482\n",
            "164/239, Train_loss: 0.1482243537902832 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [165/239], Dice Loss: 0.8213, Cross Loss: 0.2613\n",
            "165/239, Train_loss: 0.2612989842891693 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [166/239], Dice Loss: 0.9884, Cross Loss: 0.2613\n",
            "166/239, Train_loss: 0.26129183173179626 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [167/239], Dice Loss: 0.8757, Cross Loss: 0.2613\n",
            "167/239, Train_loss: 0.2612870931625366 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [168/239], Dice Loss: 0.7079, Cross Loss: 0.2613\n",
            "168/239, Train_loss: 0.26128312945365906 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [169/239], Dice Loss: 0.7366, Cross Loss: 0.2613\n",
            "169/239, Train_loss: 0.26130324602127075 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [170/239], Dice Loss: 0.9403, Cross Loss: 0.2613\n",
            "170/239, Train_loss: 0.2612836956977844 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [171/239], Dice Loss: 0.8763, Cross Loss: 0.2612\n",
            "171/239, Train_loss: 0.26123151183128357 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [172/239], Dice Loss: 0.8561, Cross Loss: 0.1482\n",
            "172/239, Train_loss: 0.14824415743350983 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [173/239], Dice Loss: 0.8840, Cross Loss: 0.2613\n",
            "173/239, Train_loss: 0.2612961530685425 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [174/239], Dice Loss: 0.6624, Cross Loss: 0.2612\n",
            "174/239, Train_loss: 0.2612268924713135 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [175/239], Dice Loss: 0.9266, Cross Loss: 0.2612\n",
            "175/239, Train_loss: 0.2612040638923645 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [176/239], Dice Loss: 0.9104, Cross Loss: 0.1483\n",
            "176/239, Train_loss: 0.148271381855011 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [177/239], Dice Loss: 0.6609, Cross Loss: 0.1483\n",
            "177/239, Train_loss: 0.14827725291252136 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [178/239], Dice Loss: 0.8540, Cross Loss: 0.2612\n",
            "178/239, Train_loss: 0.2611784338951111 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [179/239], Dice Loss: 0.9300, Cross Loss: 0.1483\n",
            "179/239, Train_loss: 0.14826494455337524 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [180/239], Dice Loss: 0.7979, Cross Loss: 0.1483\n",
            "180/239, Train_loss: 0.14826880395412445 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [181/239], Dice Loss: 0.9131, Cross Loss: 0.2612\n",
            "181/239, Train_loss: 0.26122790575027466 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [182/239], Dice Loss: 0.7282, Cross Loss: 0.1483\n",
            "182/239, Train_loss: 0.14829370379447937 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [183/239], Dice Loss: 0.9345, Cross Loss: 0.2611\n",
            "183/239, Train_loss: 0.26111894845962524 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [184/239], Dice Loss: 0.9101, Cross Loss: 0.1483\n",
            "184/239, Train_loss: 0.14830739796161652 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [185/239], Dice Loss: 0.8602, Cross Loss: 0.1483\n",
            "185/239, Train_loss: 0.14831355214118958 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [186/239], Dice Loss: 0.7906, Cross Loss: 0.1483\n",
            "186/239, Train_loss: 0.14832672476768494 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [187/239], Dice Loss: 0.9646, Cross Loss: 0.1483\n",
            "187/239, Train_loss: 0.14832614362239838 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [188/239], Dice Loss: 0.7779, Cross Loss: 0.2612\n",
            "188/239, Train_loss: 0.2611531913280487 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [189/239], Dice Loss: 0.7158, Cross Loss: 0.1483\n",
            "189/239, Train_loss: 0.14830026030540466 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [190/239], Dice Loss: 0.9562, Cross Loss: 0.1483\n",
            "190/239, Train_loss: 0.14828737080097198 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [191/239], Dice Loss: 0.7766, Cross Loss: 0.2611\n",
            "191/239, Train_loss: 0.261109322309494 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [192/239], Dice Loss: 0.9327, Cross Loss: 0.1483\n",
            "192/239, Train_loss: 0.1483110785484314 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [193/239], Dice Loss: 0.9781, Cross Loss: 0.2611\n",
            "193/239, Train_loss: 0.2611258029937744 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [194/239], Dice Loss: 0.9968, Cross Loss: 0.1483\n",
            "194/239, Train_loss: 0.14831411838531494 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [195/239], Dice Loss: 0.9822, Cross Loss: 0.1483\n",
            "195/239, Train_loss: 0.14831587672233582 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [196/239], Dice Loss: 0.9978, Cross Loss: 0.2611\n",
            "196/239, Train_loss: 0.26111361384391785 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [197/239], Dice Loss: 0.9985, Cross Loss: 0.1483\n",
            "197/239, Train_loss: 0.1483253836631775 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [198/239], Dice Loss: 0.9984, Cross Loss: 0.1483\n",
            "198/239, Train_loss: 0.1483285129070282 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [199/239], Dice Loss: 0.8405, Cross Loss: 0.1483\n",
            "199/239, Train_loss: 0.14829273521900177 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [200/239], Dice Loss: 0.7799, Cross Loss: 0.1483\n",
            "200/239, Train_loss: 0.1482900083065033 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [201/239], Dice Loss: 0.8353, Cross Loss: 0.2612\n",
            "201/239, Train_loss: 0.2611757516860962 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [202/239], Dice Loss: 0.6878, Cross Loss: 0.2610\n",
            "202/239, Train_loss: 0.2610044479370117 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [203/239], Dice Loss: 0.8338, Cross Loss: 0.1484\n",
            "203/239, Train_loss: 0.14844214916229248 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [204/239], Dice Loss: 0.9930, Cross Loss: 0.1484\n",
            "204/239, Train_loss: 0.14843249320983887 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [205/239], Dice Loss: 0.7642, Cross Loss: 0.2610\n",
            "205/239, Train_loss: 0.2609942555427551 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [206/239], Dice Loss: 0.8978, Cross Loss: 0.1483\n",
            "206/239, Train_loss: 0.14827756583690643 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [207/239], Dice Loss: 0.8027, Cross Loss: 0.1483\n",
            "207/239, Train_loss: 0.14832627773284912 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [208/239], Dice Loss: 0.8297, Cross Loss: 0.1483\n",
            "208/239, Train_loss: 0.1483287811279297 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [209/239], Dice Loss: 0.8894, Cross Loss: 0.2611\n",
            "209/239, Train_loss: 0.26110514998435974 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [210/239], Dice Loss: 0.9996, Cross Loss: 0.1483\n",
            "210/239, Train_loss: 0.14830923080444336 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [211/239], Dice Loss: 0.9017, Cross Loss: 0.1483\n",
            "211/239, Train_loss: 0.14830254018306732 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [212/239], Dice Loss: 0.8841, Cross Loss: 0.1483\n",
            "212/239, Train_loss: 0.1482999324798584 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [213/239], Dice Loss: 0.6691, Cross Loss: 0.2607\n",
            "213/239, Train_loss: 0.2607191801071167 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [214/239], Dice Loss: 0.7085, Cross Loss: 0.2604\n",
            "214/239, Train_loss: 0.260398805141449 Train_dice: tensor(0.2604, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [215/239], Dice Loss: 0.6335, Cross Loss: 0.1484\n",
            "215/239, Train_loss: 0.14835450053215027 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [216/239], Dice Loss: 0.9145, Cross Loss: 0.1484\n",
            "216/239, Train_loss: 0.1484423726797104 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [217/239], Dice Loss: 0.6653, Cross Loss: 0.2611\n",
            "217/239, Train_loss: 0.26109474897384644 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [218/239], Dice Loss: 0.6558, Cross Loss: 0.2609\n",
            "218/239, Train_loss: 0.260873407125473 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [219/239], Dice Loss: 0.8928, Cross Loss: 0.1485\n",
            "219/239, Train_loss: 0.1485312432050705 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [220/239], Dice Loss: 0.9885, Cross Loss: 0.1484\n",
            "220/239, Train_loss: 0.14838236570358276 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [221/239], Dice Loss: 0.9870, Cross Loss: 0.1511\n",
            "221/239, Train_loss: 0.1510554850101471 Train_dice: tensor(0.1511, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [222/239], Dice Loss: 0.9976, Cross Loss: 0.1494\n",
            "222/239, Train_loss: 0.14936888217926025 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [223/239], Dice Loss: 0.9890, Cross Loss: 0.1492\n",
            "223/239, Train_loss: 0.14915969967842102 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [224/239], Dice Loss: 0.5986, Cross Loss: 0.1489\n",
            "224/239, Train_loss: 0.14888222515583038 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [225/239], Dice Loss: 0.9651, Cross Loss: 0.1489\n",
            "225/239, Train_loss: 0.1489284187555313 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [226/239], Dice Loss: 0.9980, Cross Loss: 0.1488\n",
            "226/239, Train_loss: 0.14879751205444336 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [227/239], Dice Loss: 0.9999, Cross Loss: 0.1486\n",
            "227/239, Train_loss: 0.14858028292655945 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [228/239], Dice Loss: 0.9965, Cross Loss: 0.1485\n",
            "228/239, Train_loss: 0.14847850799560547 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [229/239], Dice Loss: 0.9968, Cross Loss: 0.1493\n",
            "229/239, Train_loss: 0.14932745695114136 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [230/239], Dice Loss: 0.9934, Cross Loss: 0.1489\n",
            "230/239, Train_loss: 0.14891937375068665 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [231/239], Dice Loss: 0.9916, Cross Loss: 0.1486\n",
            "231/239, Train_loss: 0.14862695336341858 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [232/239], Dice Loss: 0.9913, Cross Loss: 0.2611\n",
            "232/239, Train_loss: 0.26109206676483154 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [233/239], Dice Loss: 0.8223, Cross Loss: 0.1483\n",
            "233/239, Train_loss: 0.14827272295951843 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [234/239], Dice Loss: 0.9992, Cross Loss: 0.1483\n",
            "234/239, Train_loss: 0.14825771749019623 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [235/239], Dice Loss: 0.6359, Cross Loss: 0.2613\n",
            "235/239, Train_loss: 0.261309951543808 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [236/239], Dice Loss: 0.9953, Cross Loss: 0.2613\n",
            "236/239, Train_loss: 0.26130035519599915 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [237/239], Dice Loss: 1.0000, Cross Loss: 0.1482\n",
            "237/239, Train_loss: 0.14822418987751007 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [238/239], Dice Loss: 0.9287, Cross Loss: 0.1482\n",
            "238/239, Train_loss: 0.148229718208313 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [239/239], Dice Loss: 0.9992, Cross Loss: 0.2613\n",
            "239/239, Train_loss: 0.2612849473953247 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1861\n",
            "Epoch_metric: tensor(0.1861, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.1860\n",
            "test_dice_epoch: tensor(0.1860, device='cuda:0')\n",
            "current epoch: 3 current mean dice: tensor(0.1860, device='cuda:0')\n",
            "best mean dice: tensor(0.1865, device='cuda:0') at epoch: 1\n",
            "----------\n",
            "epoch 4/100\n",
            "Epoch [4/100], Batch [1/239], Dice Loss: 0.9707, Cross Loss: 0.2613\n",
            "1/239, Train_loss: 0.2612929344177246 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [2/239], Dice Loss: 0.8609, Cross Loss: 0.2613\n",
            "2/239, Train_loss: 0.26129090785980225 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [3/239], Dice Loss: 0.9703, Cross Loss: 0.2613\n",
            "3/239, Train_loss: 0.2612883746623993 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [4/239], Dice Loss: 0.9562, Cross Loss: 0.2613\n",
            "4/239, Train_loss: 0.2612758278846741 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [5/239], Dice Loss: 0.8753, Cross Loss: 0.1482\n",
            "5/239, Train_loss: 0.1482175588607788 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [6/239], Dice Loss: 0.9152, Cross Loss: 0.1482\n",
            "6/239, Train_loss: 0.14821942150592804 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [7/239], Dice Loss: 0.5560, Cross Loss: 0.2613\n",
            "7/239, Train_loss: 0.2613041400909424 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [8/239], Dice Loss: 0.9937, Cross Loss: 0.2613\n",
            "8/239, Train_loss: 0.2612801790237427 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [9/239], Dice Loss: 0.9937, Cross Loss: 0.2613\n",
            "9/239, Train_loss: 0.261303186416626 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [10/239], Dice Loss: 0.8505, Cross Loss: 0.1482\n",
            "10/239, Train_loss: 0.148233100771904 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [11/239], Dice Loss: 0.7896, Cross Loss: 0.1482\n",
            "11/239, Train_loss: 0.1482340395450592 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [12/239], Dice Loss: 0.9153, Cross Loss: 0.1482\n",
            "12/239, Train_loss: 0.14824511110782623 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [13/239], Dice Loss: 0.9270, Cross Loss: 0.1482\n",
            "13/239, Train_loss: 0.14823637902736664 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [14/239], Dice Loss: 0.7250, Cross Loss: 0.1483\n",
            "14/239, Train_loss: 0.14825592935085297 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [15/239], Dice Loss: 0.7696, Cross Loss: 0.1482\n",
            "15/239, Train_loss: 0.14824925363063812 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [16/239], Dice Loss: 0.9376, Cross Loss: 0.1483\n",
            "16/239, Train_loss: 0.14825090765953064 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [17/239], Dice Loss: 0.6313, Cross Loss: 0.1482\n",
            "17/239, Train_loss: 0.14824670553207397 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [18/239], Dice Loss: 0.9406, Cross Loss: 0.1482\n",
            "18/239, Train_loss: 0.14822983741760254 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [19/239], Dice Loss: 0.6652, Cross Loss: 0.2613\n",
            "19/239, Train_loss: 0.261279433965683 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [20/239], Dice Loss: 0.6228, Cross Loss: 0.2613\n",
            "20/239, Train_loss: 0.2613254189491272 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [21/239], Dice Loss: 0.8605, Cross Loss: 0.2613\n",
            "21/239, Train_loss: 0.2612881064414978 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [22/239], Dice Loss: 0.9119, Cross Loss: 0.1482\n",
            "22/239, Train_loss: 0.1482362151145935 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [23/239], Dice Loss: 0.6738, Cross Loss: 0.2613\n",
            "23/239, Train_loss: 0.26131248474121094 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [24/239], Dice Loss: 0.8444, Cross Loss: 0.2613\n",
            "24/239, Train_loss: 0.26128801703453064 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [25/239], Dice Loss: 0.9729, Cross Loss: 0.1482\n",
            "25/239, Train_loss: 0.1482241153717041 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [26/239], Dice Loss: 0.9552, Cross Loss: 0.1482\n",
            "26/239, Train_loss: 0.14822079241275787 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [27/239], Dice Loss: 0.9427, Cross Loss: 0.1482\n",
            "27/239, Train_loss: 0.14822086691856384 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [28/239], Dice Loss: 0.7315, Cross Loss: 0.1482\n",
            "28/239, Train_loss: 0.14821940660476685 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [29/239], Dice Loss: 0.8712, Cross Loss: 0.1482\n",
            "29/239, Train_loss: 0.14822697639465332 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [30/239], Dice Loss: 0.9839, Cross Loss: 0.1482\n",
            "30/239, Train_loss: 0.1482260823249817 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [31/239], Dice Loss: 0.7293, Cross Loss: 0.1482\n",
            "31/239, Train_loss: 0.14822375774383545 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [32/239], Dice Loss: 0.8946, Cross Loss: 0.1482\n",
            "32/239, Train_loss: 0.14821955561637878 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [33/239], Dice Loss: 0.4845, Cross Loss: 0.2613\n",
            "33/239, Train_loss: 0.26132017374038696 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [34/239], Dice Loss: 0.9969, Cross Loss: 0.1482\n",
            "34/239, Train_loss: 0.14821980893611908 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [35/239], Dice Loss: 0.9100, Cross Loss: 0.2613\n",
            "35/239, Train_loss: 0.26130014657974243 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [36/239], Dice Loss: 0.9092, Cross Loss: 0.1482\n",
            "36/239, Train_loss: 0.1482229232788086 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [37/239], Dice Loss: 0.9084, Cross Loss: 0.1482\n",
            "37/239, Train_loss: 0.1482158899307251 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [38/239], Dice Loss: 0.8356, Cross Loss: 0.1482\n",
            "38/239, Train_loss: 0.14821942150592804 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [39/239], Dice Loss: 0.6677, Cross Loss: 0.2613\n",
            "39/239, Train_loss: 0.26129987835884094 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [40/239], Dice Loss: 0.7255, Cross Loss: 0.2613\n",
            "40/239, Train_loss: 0.26129403710365295 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [41/239], Dice Loss: 0.9652, Cross Loss: 0.1482\n",
            "41/239, Train_loss: 0.148220032453537 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [42/239], Dice Loss: 0.5784, Cross Loss: 0.1482\n",
            "42/239, Train_loss: 0.1482229232788086 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [43/239], Dice Loss: 0.9444, Cross Loss: 0.1482\n",
            "43/239, Train_loss: 0.14822286367416382 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [44/239], Dice Loss: 0.6670, Cross Loss: 0.1482\n",
            "44/239, Train_loss: 0.14821851253509521 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [45/239], Dice Loss: 0.7472, Cross Loss: 0.2613\n",
            "45/239, Train_loss: 0.261318176984787 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [46/239], Dice Loss: 0.9708, Cross Loss: 0.1482\n",
            "46/239, Train_loss: 0.1482142060995102 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [47/239], Dice Loss: 0.7455, Cross Loss: 0.1482\n",
            "47/239, Train_loss: 0.14821842312812805 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [48/239], Dice Loss: 0.8942, Cross Loss: 0.2613\n",
            "48/239, Train_loss: 0.26129505038261414 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [49/239], Dice Loss: 0.8516, Cross Loss: 0.1482\n",
            "49/239, Train_loss: 0.14822593331336975 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [50/239], Dice Loss: 0.8453, Cross Loss: 0.1482\n",
            "50/239, Train_loss: 0.14821821451187134 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [51/239], Dice Loss: 0.6047, Cross Loss: 0.1482\n",
            "51/239, Train_loss: 0.14821794629096985 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [52/239], Dice Loss: 0.7702, Cross Loss: 0.1482\n",
            "52/239, Train_loss: 0.14821749925613403 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [53/239], Dice Loss: 0.8254, Cross Loss: 0.1482\n",
            "53/239, Train_loss: 0.14821869134902954 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [54/239], Dice Loss: 0.7495, Cross Loss: 0.2613\n",
            "54/239, Train_loss: 0.2613070607185364 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [55/239], Dice Loss: 0.5361, Cross Loss: 0.1482\n",
            "55/239, Train_loss: 0.14821961522102356 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [56/239], Dice Loss: 0.9396, Cross Loss: 0.1482\n",
            "56/239, Train_loss: 0.14821946620941162 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [57/239], Dice Loss: 0.7262, Cross Loss: 0.2613\n",
            "57/239, Train_loss: 0.26131224632263184 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [58/239], Dice Loss: 0.9641, Cross Loss: 0.1482\n",
            "58/239, Train_loss: 0.14821146428585052 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [59/239], Dice Loss: 0.9676, Cross Loss: 0.1482\n",
            "59/239, Train_loss: 0.14821277558803558 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [60/239], Dice Loss: 0.7218, Cross Loss: 0.1482\n",
            "60/239, Train_loss: 0.14821039140224457 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [61/239], Dice Loss: 0.9497, Cross Loss: 0.1482\n",
            "61/239, Train_loss: 0.14820942282676697 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [62/239], Dice Loss: 0.6837, Cross Loss: 0.2613\n",
            "62/239, Train_loss: 0.2613232731819153 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [63/239], Dice Loss: 0.9080, Cross Loss: 0.1482\n",
            "63/239, Train_loss: 0.1482231765985489 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [64/239], Dice Loss: 0.8241, Cross Loss: 0.1482\n",
            "64/239, Train_loss: 0.14821583032608032 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [65/239], Dice Loss: 0.6472, Cross Loss: 0.1482\n",
            "65/239, Train_loss: 0.14821335673332214 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [66/239], Dice Loss: 0.6423, Cross Loss: 0.1482\n",
            "66/239, Train_loss: 0.14821965992450714 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [67/239], Dice Loss: 0.9360, Cross Loss: 0.1482\n",
            "67/239, Train_loss: 0.14821003377437592 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [68/239], Dice Loss: 0.7744, Cross Loss: 0.1482\n",
            "68/239, Train_loss: 0.1482226848602295 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [69/239], Dice Loss: 0.9903, Cross Loss: 0.2613\n",
            "69/239, Train_loss: 0.26131466031074524 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [70/239], Dice Loss: 0.9612, Cross Loss: 0.2613\n",
            "70/239, Train_loss: 0.26132625341415405 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [71/239], Dice Loss: 0.5442, Cross Loss: 0.1482\n",
            "71/239, Train_loss: 0.14820975065231323 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [72/239], Dice Loss: 0.9468, Cross Loss: 0.2613\n",
            "72/239, Train_loss: 0.26133501529693604 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [73/239], Dice Loss: 0.9622, Cross Loss: 0.1482\n",
            "73/239, Train_loss: 0.1482125073671341 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [74/239], Dice Loss: 0.8784, Cross Loss: 0.1482\n",
            "74/239, Train_loss: 0.1482124775648117 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [75/239], Dice Loss: 0.9481, Cross Loss: 0.1482\n",
            "75/239, Train_loss: 0.14820757508277893 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [76/239], Dice Loss: 0.8478, Cross Loss: 0.1482\n",
            "76/239, Train_loss: 0.14821109175682068 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [77/239], Dice Loss: 0.9896, Cross Loss: 0.1482\n",
            "77/239, Train_loss: 0.14820635318756104 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [78/239], Dice Loss: 0.9460, Cross Loss: 0.1482\n",
            "78/239, Train_loss: 0.14820997416973114 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [79/239], Dice Loss: 0.9783, Cross Loss: 0.1482\n",
            "79/239, Train_loss: 0.1482204645872116 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [80/239], Dice Loss: 0.7733, Cross Loss: 0.1482\n",
            "80/239, Train_loss: 0.14820557832717896 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [81/239], Dice Loss: 0.9908, Cross Loss: 0.1482\n",
            "81/239, Train_loss: 0.14820735156536102 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [82/239], Dice Loss: 0.4378, Cross Loss: 0.1482\n",
            "82/239, Train_loss: 0.14821195602416992 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [83/239], Dice Loss: 0.9227, Cross Loss: 0.1482\n",
            "83/239, Train_loss: 0.14821118116378784 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [84/239], Dice Loss: 0.9355, Cross Loss: 0.1482\n",
            "84/239, Train_loss: 0.14820654690265656 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [85/239], Dice Loss: 0.7857, Cross Loss: 0.2613\n",
            "85/239, Train_loss: 0.2613295614719391 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [86/239], Dice Loss: 0.7937, Cross Loss: 0.1482\n",
            "86/239, Train_loss: 0.14820793271064758 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [87/239], Dice Loss: 0.9216, Cross Loss: 0.1482\n",
            "87/239, Train_loss: 0.148207426071167 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [88/239], Dice Loss: 0.9385, Cross Loss: 0.1482\n",
            "88/239, Train_loss: 0.14820483326911926 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [89/239], Dice Loss: 0.9806, Cross Loss: 0.1482\n",
            "89/239, Train_loss: 0.14820435643196106 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [90/239], Dice Loss: 0.8722, Cross Loss: 0.1482\n",
            "90/239, Train_loss: 0.14820387959480286 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [91/239], Dice Loss: 0.7084, Cross Loss: 0.2613\n",
            "91/239, Train_loss: 0.2613229751586914 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [92/239], Dice Loss: 0.9109, Cross Loss: 0.1482\n",
            "92/239, Train_loss: 0.14820155501365662 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [93/239], Dice Loss: 0.8667, Cross Loss: 0.1482\n",
            "93/239, Train_loss: 0.1482013761997223 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [94/239], Dice Loss: 0.8462, Cross Loss: 0.2613\n",
            "94/239, Train_loss: 0.2613460123538971 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [95/239], Dice Loss: 0.8651, Cross Loss: 0.1482\n",
            "95/239, Train_loss: 0.1482025384902954 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [96/239], Dice Loss: 0.9683, Cross Loss: 0.1482\n",
            "96/239, Train_loss: 0.14820247888565063 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [97/239], Dice Loss: 0.8930, Cross Loss: 0.1482\n",
            "97/239, Train_loss: 0.14820383489131927 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [98/239], Dice Loss: 0.8456, Cross Loss: 0.1482\n",
            "98/239, Train_loss: 0.14820191264152527 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [99/239], Dice Loss: 0.8955, Cross Loss: 0.2613\n",
            "99/239, Train_loss: 0.2613329589366913 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [100/239], Dice Loss: 0.9694, Cross Loss: 0.2614\n",
            "100/239, Train_loss: 0.2613540589809418 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [101/239], Dice Loss: 0.7504, Cross Loss: 0.2614\n",
            "101/239, Train_loss: 0.2613523304462433 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [102/239], Dice Loss: 0.8288, Cross Loss: 0.1482\n",
            "102/239, Train_loss: 0.14820092916488647 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [103/239], Dice Loss: 0.9374, Cross Loss: 0.1482\n",
            "103/239, Train_loss: 0.14820444583892822 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [104/239], Dice Loss: 0.8296, Cross Loss: 0.2613\n",
            "104/239, Train_loss: 0.2613410949707031 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [105/239], Dice Loss: 0.8061, Cross Loss: 0.1482\n",
            "105/239, Train_loss: 0.14821070432662964 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [106/239], Dice Loss: 0.9555, Cross Loss: 0.1482\n",
            "106/239, Train_loss: 0.14821089804172516 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [107/239], Dice Loss: 0.7837, Cross Loss: 0.1482\n",
            "107/239, Train_loss: 0.14820508658885956 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [108/239], Dice Loss: 0.8146, Cross Loss: 0.2613\n",
            "108/239, Train_loss: 0.26133406162261963 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [109/239], Dice Loss: 0.8197, Cross Loss: 0.1482\n",
            "109/239, Train_loss: 0.14820343255996704 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [110/239], Dice Loss: 0.9323, Cross Loss: 0.1482\n",
            "110/239, Train_loss: 0.14820361137390137 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [111/239], Dice Loss: 0.9298, Cross Loss: 0.1482\n",
            "111/239, Train_loss: 0.14820381999015808 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [112/239], Dice Loss: 0.8297, Cross Loss: 0.1482\n",
            "112/239, Train_loss: 0.1482035517692566 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [113/239], Dice Loss: 0.8338, Cross Loss: 0.1482\n",
            "113/239, Train_loss: 0.1482006311416626 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [114/239], Dice Loss: 0.7519, Cross Loss: 0.1482\n",
            "114/239, Train_loss: 0.14820055663585663 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [115/239], Dice Loss: 0.9385, Cross Loss: 0.1482\n",
            "115/239, Train_loss: 0.1482030600309372 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [116/239], Dice Loss: 0.9063, Cross Loss: 0.1482\n",
            "116/239, Train_loss: 0.14820268750190735 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [117/239], Dice Loss: 0.9083, Cross Loss: 0.1482\n",
            "117/239, Train_loss: 0.14819937944412231 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [118/239], Dice Loss: 0.9035, Cross Loss: 0.1482\n",
            "118/239, Train_loss: 0.14819912612438202 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [119/239], Dice Loss: 0.9175, Cross Loss: 0.1482\n",
            "119/239, Train_loss: 0.14819982647895813 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [120/239], Dice Loss: 0.9245, Cross Loss: 0.1482\n",
            "120/239, Train_loss: 0.14819873869419098 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [121/239], Dice Loss: 0.7921, Cross Loss: 0.1482\n",
            "121/239, Train_loss: 0.1481996327638626 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [122/239], Dice Loss: 0.7035, Cross Loss: 0.2613\n",
            "122/239, Train_loss: 0.2613477110862732 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [123/239], Dice Loss: 0.9123, Cross Loss: 0.1482\n",
            "123/239, Train_loss: 0.1482047587633133 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [124/239], Dice Loss: 0.5864, Cross Loss: 0.2614\n",
            "124/239, Train_loss: 0.26135170459747314 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [125/239], Dice Loss: 0.8338, Cross Loss: 0.2614\n",
            "125/239, Train_loss: 0.26135802268981934 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [126/239], Dice Loss: 0.9160, Cross Loss: 0.1482\n",
            "126/239, Train_loss: 0.14819951355457306 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [127/239], Dice Loss: 0.9393, Cross Loss: 0.1482\n",
            "127/239, Train_loss: 0.14819779992103577 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [128/239], Dice Loss: 0.8306, Cross Loss: 0.2613\n",
            "128/239, Train_loss: 0.26134270429611206 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [129/239], Dice Loss: 0.7577, Cross Loss: 0.1482\n",
            "129/239, Train_loss: 0.14819739758968353 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [130/239], Dice Loss: 0.6207, Cross Loss: 0.2613\n",
            "130/239, Train_loss: 0.2613461911678314 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [131/239], Dice Loss: 0.8558, Cross Loss: 0.1482\n",
            "131/239, Train_loss: 0.14820027351379395 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [132/239], Dice Loss: 0.9987, Cross Loss: 0.1482\n",
            "132/239, Train_loss: 0.1481986939907074 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [133/239], Dice Loss: 0.7162, Cross Loss: 0.1482\n",
            "133/239, Train_loss: 0.1481989026069641 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [134/239], Dice Loss: 0.8680, Cross Loss: 0.2613\n",
            "134/239, Train_loss: 0.2613475024700165 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [135/239], Dice Loss: 0.6058, Cross Loss: 0.1482\n",
            "135/239, Train_loss: 0.14819882810115814 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [136/239], Dice Loss: 0.9974, Cross Loss: 0.2613\n",
            "136/239, Train_loss: 0.26134559512138367 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [137/239], Dice Loss: 0.5304, Cross Loss: 0.2614\n",
            "137/239, Train_loss: 0.26135799288749695 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [138/239], Dice Loss: 0.8620, Cross Loss: 0.1482\n",
            "138/239, Train_loss: 0.14820128679275513 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [139/239], Dice Loss: 0.8249, Cross Loss: 0.2613\n",
            "139/239, Train_loss: 0.2613428235054016 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [140/239], Dice Loss: 0.9915, Cross Loss: 0.1482\n",
            "140/239, Train_loss: 0.14820225536823273 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [141/239], Dice Loss: 0.8810, Cross Loss: 0.2613\n",
            "141/239, Train_loss: 0.2613447308540344 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [142/239], Dice Loss: 0.8824, Cross Loss: 0.1482\n",
            "142/239, Train_loss: 0.1482025682926178 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [143/239], Dice Loss: 0.9906, Cross Loss: 0.2614\n",
            "143/239, Train_loss: 0.2613533139228821 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [144/239], Dice Loss: 0.8880, Cross Loss: 0.1482\n",
            "144/239, Train_loss: 0.1481996476650238 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [145/239], Dice Loss: 0.6391, Cross Loss: 0.1482\n",
            "145/239, Train_loss: 0.14820131659507751 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [146/239], Dice Loss: 0.8931, Cross Loss: 0.1482\n",
            "146/239, Train_loss: 0.14820125699043274 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [147/239], Dice Loss: 0.9272, Cross Loss: 0.1482\n",
            "147/239, Train_loss: 0.1481994390487671 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [148/239], Dice Loss: 0.9390, Cross Loss: 0.1482\n",
            "148/239, Train_loss: 0.1481991708278656 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [149/239], Dice Loss: 0.5230, Cross Loss: 0.2613\n",
            "149/239, Train_loss: 0.2613489031791687 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [150/239], Dice Loss: 0.9916, Cross Loss: 0.1482\n",
            "150/239, Train_loss: 0.14819814264774323 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [151/239], Dice Loss: 0.6349, Cross Loss: 0.2613\n",
            "151/239, Train_loss: 0.2613472640514374 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [152/239], Dice Loss: 0.9430, Cross Loss: 0.1482\n",
            "152/239, Train_loss: 0.14820009469985962 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [153/239], Dice Loss: 0.8842, Cross Loss: 0.1482\n",
            "153/239, Train_loss: 0.14820033311843872 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [154/239], Dice Loss: 0.6801, Cross Loss: 0.2614\n",
            "154/239, Train_loss: 0.26135560870170593 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [155/239], Dice Loss: 0.8433, Cross Loss: 0.1482\n",
            "155/239, Train_loss: 0.1482010930776596 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [156/239], Dice Loss: 0.7949, Cross Loss: 0.2613\n",
            "156/239, Train_loss: 0.26133573055267334 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [157/239], Dice Loss: 0.8229, Cross Loss: 0.2613\n",
            "157/239, Train_loss: 0.2613287568092346 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [158/239], Dice Loss: 0.7540, Cross Loss: 0.2613\n",
            "158/239, Train_loss: 0.26133963465690613 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [159/239], Dice Loss: 0.9566, Cross Loss: 0.1482\n",
            "159/239, Train_loss: 0.1482040286064148 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [160/239], Dice Loss: 0.7618, Cross Loss: 0.2613\n",
            "160/239, Train_loss: 0.26133859157562256 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [161/239], Dice Loss: 0.8982, Cross Loss: 0.1482\n",
            "161/239, Train_loss: 0.14820453524589539 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [162/239], Dice Loss: 0.9392, Cross Loss: 0.2613\n",
            "162/239, Train_loss: 0.2613493800163269 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [163/239], Dice Loss: 0.9812, Cross Loss: 0.1482\n",
            "163/239, Train_loss: 0.14820362627506256 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [164/239], Dice Loss: 0.9196, Cross Loss: 0.1482\n",
            "164/239, Train_loss: 0.14820386469364166 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [165/239], Dice Loss: 0.7824, Cross Loss: 0.2613\n",
            "165/239, Train_loss: 0.26134416460990906 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [166/239], Dice Loss: 0.9918, Cross Loss: 0.2613\n",
            "166/239, Train_loss: 0.261345237493515 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [167/239], Dice Loss: 0.9050, Cross Loss: 0.2613\n",
            "167/239, Train_loss: 0.2613369822502136 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [168/239], Dice Loss: 0.7438, Cross Loss: 0.2613\n",
            "168/239, Train_loss: 0.26133596897125244 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [169/239], Dice Loss: 0.8576, Cross Loss: 0.2613\n",
            "169/239, Train_loss: 0.2613467872142792 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [170/239], Dice Loss: 0.9527, Cross Loss: 0.2613\n",
            "170/239, Train_loss: 0.2613373100757599 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [171/239], Dice Loss: 0.8712, Cross Loss: 0.2613\n",
            "171/239, Train_loss: 0.2613208293914795 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [172/239], Dice Loss: 0.7605, Cross Loss: 0.1482\n",
            "172/239, Train_loss: 0.14820727705955505 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [173/239], Dice Loss: 0.8376, Cross Loss: 0.2613\n",
            "173/239, Train_loss: 0.26134854555130005 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [174/239], Dice Loss: 0.6194, Cross Loss: 0.2613\n",
            "174/239, Train_loss: 0.26132482290267944 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [175/239], Dice Loss: 0.9634, Cross Loss: 0.2613\n",
            "175/239, Train_loss: 0.26131927967071533 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [176/239], Dice Loss: 0.8369, Cross Loss: 0.1482\n",
            "176/239, Train_loss: 0.14821502566337585 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [177/239], Dice Loss: 0.6645, Cross Loss: 0.1482\n",
            "177/239, Train_loss: 0.14821656048297882 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [178/239], Dice Loss: 0.8068, Cross Loss: 0.2613\n",
            "178/239, Train_loss: 0.2613128125667572 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [179/239], Dice Loss: 0.8789, Cross Loss: 0.1482\n",
            "179/239, Train_loss: 0.1482134312391281 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [180/239], Dice Loss: 0.8281, Cross Loss: 0.1482\n",
            "180/239, Train_loss: 0.14821460843086243 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [181/239], Dice Loss: 0.9243, Cross Loss: 0.2613\n",
            "181/239, Train_loss: 0.26132163405418396 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [182/239], Dice Loss: 0.7254, Cross Loss: 0.1482\n",
            "182/239, Train_loss: 0.1482183188199997 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [183/239], Dice Loss: 0.9363, Cross Loss: 0.2613\n",
            "183/239, Train_loss: 0.26126182079315186 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [184/239], Dice Loss: 0.8890, Cross Loss: 0.1482\n",
            "184/239, Train_loss: 0.14822110533714294 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [185/239], Dice Loss: 0.9132, Cross Loss: 0.1482\n",
            "185/239, Train_loss: 0.14822198450565338 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [186/239], Dice Loss: 0.8062, Cross Loss: 0.1482\n",
            "186/239, Train_loss: 0.14823056757450104 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [187/239], Dice Loss: 0.9800, Cross Loss: 0.1482\n",
            "187/239, Train_loss: 0.1482311338186264 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [188/239], Dice Loss: 0.8572, Cross Loss: 0.2613\n",
            "188/239, Train_loss: 0.26129791140556335 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [189/239], Dice Loss: 0.5932, Cross Loss: 0.1482\n",
            "189/239, Train_loss: 0.14822322130203247 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [190/239], Dice Loss: 0.9740, Cross Loss: 0.1482\n",
            "190/239, Train_loss: 0.14822061359882355 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [191/239], Dice Loss: 0.7260, Cross Loss: 0.2613\n",
            "191/239, Train_loss: 0.2612772583961487 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [192/239], Dice Loss: 0.9055, Cross Loss: 0.1482\n",
            "192/239, Train_loss: 0.14823488891124725 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [193/239], Dice Loss: 0.9549, Cross Loss: 0.2613\n",
            "193/239, Train_loss: 0.26126980781555176 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [194/239], Dice Loss: 0.9901, Cross Loss: 0.1482\n",
            "194/239, Train_loss: 0.14823105931282043 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [195/239], Dice Loss: 0.9968, Cross Loss: 0.1482\n",
            "195/239, Train_loss: 0.14823201298713684 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [196/239], Dice Loss: 0.9973, Cross Loss: 0.2613\n",
            "196/239, Train_loss: 0.26128286123275757 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [197/239], Dice Loss: 0.9981, Cross Loss: 0.1482\n",
            "197/239, Train_loss: 0.14823147654533386 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [198/239], Dice Loss: 0.9979, Cross Loss: 0.1482\n",
            "198/239, Train_loss: 0.14823254942893982 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [199/239], Dice Loss: 0.8358, Cross Loss: 0.1482\n",
            "199/239, Train_loss: 0.14822134375572205 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [200/239], Dice Loss: 0.6247, Cross Loss: 0.1482\n",
            "200/239, Train_loss: 0.14822092652320862 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [201/239], Dice Loss: 0.8393, Cross Loss: 0.2613\n",
            "201/239, Train_loss: 0.26130250096321106 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [202/239], Dice Loss: 0.6866, Cross Loss: 0.2613\n",
            "202/239, Train_loss: 0.26125627756118774 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [203/239], Dice Loss: 0.7775, Cross Loss: 0.1482\n",
            "203/239, Train_loss: 0.14823591709136963 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [204/239], Dice Loss: 0.9900, Cross Loss: 0.1482\n",
            "204/239, Train_loss: 0.14823698997497559 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [205/239], Dice Loss: 0.7772, Cross Loss: 0.2613\n",
            "205/239, Train_loss: 0.2612651586532593 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [206/239], Dice Loss: 0.8488, Cross Loss: 0.1482\n",
            "206/239, Train_loss: 0.14821186661720276 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [207/239], Dice Loss: 0.8529, Cross Loss: 0.1482\n",
            "207/239, Train_loss: 0.1482430100440979 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [208/239], Dice Loss: 0.8572, Cross Loss: 0.1482\n",
            "208/239, Train_loss: 0.14824290573596954 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [209/239], Dice Loss: 0.8360, Cross Loss: 0.2613\n",
            "209/239, Train_loss: 0.2612804174423218 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [210/239], Dice Loss: 0.9997, Cross Loss: 0.1482\n",
            "210/239, Train_loss: 0.14822925627231598 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [211/239], Dice Loss: 0.8785, Cross Loss: 0.1482\n",
            "211/239, Train_loss: 0.1482318490743637 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [212/239], Dice Loss: 0.8668, Cross Loss: 0.1482\n",
            "212/239, Train_loss: 0.14823010563850403 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [213/239], Dice Loss: 0.6414, Cross Loss: 0.2613\n",
            "213/239, Train_loss: 0.26127493381500244 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [214/239], Dice Loss: 0.6730, Cross Loss: 0.2613\n",
            "214/239, Train_loss: 0.26128140091896057 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [215/239], Dice Loss: 0.6211, Cross Loss: 0.1482\n",
            "215/239, Train_loss: 0.14821799099445343 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [216/239], Dice Loss: 0.9057, Cross Loss: 0.1482\n",
            "216/239, Train_loss: 0.14821603894233704 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [217/239], Dice Loss: 0.6166, Cross Loss: 0.2613\n",
            "217/239, Train_loss: 0.2613140642642975 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [218/239], Dice Loss: 0.6331, Cross Loss: 0.2613\n",
            "218/239, Train_loss: 0.26130175590515137 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [219/239], Dice Loss: 0.8728, Cross Loss: 0.1482\n",
            "219/239, Train_loss: 0.14822085201740265 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [220/239], Dice Loss: 0.9899, Cross Loss: 0.1482\n",
            "220/239, Train_loss: 0.14821287989616394 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [221/239], Dice Loss: 0.9871, Cross Loss: 0.1482\n",
            "221/239, Train_loss: 0.14821678400039673 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [222/239], Dice Loss: 0.9969, Cross Loss: 0.1482\n",
            "222/239, Train_loss: 0.14821335673332214 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [223/239], Dice Loss: 0.9861, Cross Loss: 0.1482\n",
            "223/239, Train_loss: 0.14821800589561462 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [224/239], Dice Loss: 0.6054, Cross Loss: 0.1482\n",
            "224/239, Train_loss: 0.14821791648864746 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [225/239], Dice Loss: 0.9554, Cross Loss: 0.1482\n",
            "225/239, Train_loss: 0.14821913838386536 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [226/239], Dice Loss: 0.9979, Cross Loss: 0.1482\n",
            "226/239, Train_loss: 0.14822375774383545 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [227/239], Dice Loss: 1.0000, Cross Loss: 0.1482\n",
            "227/239, Train_loss: 0.1482236534357071 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [228/239], Dice Loss: 0.9992, Cross Loss: 0.1482\n",
            "228/239, Train_loss: 0.14821426570415497 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [229/239], Dice Loss: 0.9980, Cross Loss: 0.1482\n",
            "229/239, Train_loss: 0.14822301268577576 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [230/239], Dice Loss: 0.9944, Cross Loss: 0.1482\n",
            "230/239, Train_loss: 0.14822354912757874 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [231/239], Dice Loss: 0.9858, Cross Loss: 0.1482\n",
            "231/239, Train_loss: 0.1482233703136444 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [232/239], Dice Loss: 0.9740, Cross Loss: 0.2613\n",
            "232/239, Train_loss: 0.2612992227077484 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [233/239], Dice Loss: 0.7634, Cross Loss: 0.1482\n",
            "233/239, Train_loss: 0.1482132077217102 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [234/239], Dice Loss: 0.9993, Cross Loss: 0.1482\n",
            "234/239, Train_loss: 0.14821204543113708 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [235/239], Dice Loss: 0.5587, Cross Loss: 0.2614\n",
            "235/239, Train_loss: 0.2613503634929657 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [236/239], Dice Loss: 0.9964, Cross Loss: 0.2613\n",
            "236/239, Train_loss: 0.2613252103328705 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [237/239], Dice Loss: 1.0000, Cross Loss: 0.1482\n",
            "237/239, Train_loss: 0.1482136845588684 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [238/239], Dice Loss: 0.9456, Cross Loss: 0.1482\n",
            "238/239, Train_loss: 0.14821982383728027 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [239/239], Dice Loss: 0.9988, Cross Loss: 0.2613\n",
            "239/239, Train_loss: 0.26128488779067993 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1861\n",
            "Epoch_metric: tensor(0.1861, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.1860\n",
            "test_dice_epoch: tensor(0.1860, device='cuda:0')\n",
            "current epoch: 4 current mean dice: tensor(0.1860, device='cuda:0')\n",
            "best mean dice: tensor(0.1865, device='cuda:0') at epoch: 1\n",
            "----------\n",
            "epoch 5/100\n",
            "Epoch [5/100], Batch [1/239], Dice Loss: 0.9773, Cross Loss: 0.2613\n",
            "1/239, Train_loss: 0.2612804174423218 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [2/239], Dice Loss: 0.7488, Cross Loss: 0.2613\n",
            "2/239, Train_loss: 0.26127538084983826 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [3/239], Dice Loss: 0.9723, Cross Loss: 0.2613\n",
            "3/239, Train_loss: 0.2612953186035156 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [4/239], Dice Loss: 0.9584, Cross Loss: 0.2613\n",
            "4/239, Train_loss: 0.2612762451171875 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [5/239], Dice Loss: 0.8957, Cross Loss: 0.1482\n",
            "5/239, Train_loss: 0.14820784330368042 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [6/239], Dice Loss: 0.9243, Cross Loss: 0.1482\n",
            "6/239, Train_loss: 0.1482090801000595 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [7/239], Dice Loss: 0.6527, Cross Loss: 0.2613\n",
            "7/239, Train_loss: 0.2613134980201721 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [8/239], Dice Loss: 0.9973, Cross Loss: 0.2613\n",
            "8/239, Train_loss: 0.2612546682357788 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [9/239], Dice Loss: 0.9975, Cross Loss: 0.2613\n",
            "9/239, Train_loss: 0.26128214597702026 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [10/239], Dice Loss: 0.8734, Cross Loss: 0.1482\n",
            "10/239, Train_loss: 0.14824146032333374 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [11/239], Dice Loss: 0.7735, Cross Loss: 0.1482\n",
            "11/239, Train_loss: 0.14822843670845032 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [12/239], Dice Loss: 0.9294, Cross Loss: 0.1483\n",
            "12/239, Train_loss: 0.1482705920934677 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [13/239], Dice Loss: 0.9051, Cross Loss: 0.1483\n",
            "13/239, Train_loss: 0.14825743436813354 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [14/239], Dice Loss: 0.7038, Cross Loss: 0.1483\n",
            "14/239, Train_loss: 0.14827680587768555 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [15/239], Dice Loss: 0.7221, Cross Loss: 0.1483\n",
            "15/239, Train_loss: 0.14828257262706757 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [16/239], Dice Loss: 0.9433, Cross Loss: 0.1483\n",
            "16/239, Train_loss: 0.1483067274093628 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [17/239], Dice Loss: 0.5886, Cross Loss: 0.1483\n",
            "17/239, Train_loss: 0.14829827845096588 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [18/239], Dice Loss: 0.9447, Cross Loss: 0.1482\n",
            "18/239, Train_loss: 0.14824378490447998 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [19/239], Dice Loss: 0.6469, Cross Loss: 0.2612\n",
            "19/239, Train_loss: 0.2612321078777313 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [20/239], Dice Loss: 0.6077, Cross Loss: 0.2613\n",
            "20/239, Train_loss: 0.26131680607795715 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [21/239], Dice Loss: 0.8960, Cross Loss: 0.2612\n",
            "21/239, Train_loss: 0.26124823093414307 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [22/239], Dice Loss: 0.9596, Cross Loss: 0.1483\n",
            "22/239, Train_loss: 0.14826685190200806 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [23/239], Dice Loss: 0.6764, Cross Loss: 0.2613\n",
            "23/239, Train_loss: 0.261316180229187 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [24/239], Dice Loss: 0.8191, Cross Loss: 0.2612\n",
            "24/239, Train_loss: 0.26122623682022095 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [25/239], Dice Loss: 0.9670, Cross Loss: 0.1482\n",
            "25/239, Train_loss: 0.14823326468467712 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [26/239], Dice Loss: 0.9530, Cross Loss: 0.1482\n",
            "26/239, Train_loss: 0.148221954703331 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [27/239], Dice Loss: 0.9649, Cross Loss: 0.1482\n",
            "27/239, Train_loss: 0.1482224315404892 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [28/239], Dice Loss: 0.7459, Cross Loss: 0.1482\n",
            "28/239, Train_loss: 0.14822715520858765 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [29/239], Dice Loss: 0.9102, Cross Loss: 0.1482\n",
            "29/239, Train_loss: 0.14824777841567993 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [30/239], Dice Loss: 0.9808, Cross Loss: 0.1482\n",
            "30/239, Train_loss: 0.14824488759040833 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [31/239], Dice Loss: 0.7595, Cross Loss: 0.1482\n",
            "31/239, Train_loss: 0.14823809266090393 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [32/239], Dice Loss: 0.9353, Cross Loss: 0.1482\n",
            "32/239, Train_loss: 0.14822274446487427 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [33/239], Dice Loss: 0.4252, Cross Loss: 0.2613\n",
            "33/239, Train_loss: 0.2612778842449188 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [34/239], Dice Loss: 0.9965, Cross Loss: 0.1482\n",
            "34/239, Train_loss: 0.14822310209274292 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [35/239], Dice Loss: 0.9194, Cross Loss: 0.2613\n",
            "35/239, Train_loss: 0.261272132396698 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [36/239], Dice Loss: 0.9210, Cross Loss: 0.1482\n",
            "36/239, Train_loss: 0.14823609590530396 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [37/239], Dice Loss: 0.9178, Cross Loss: 0.1482\n",
            "37/239, Train_loss: 0.14821481704711914 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [38/239], Dice Loss: 0.8314, Cross Loss: 0.1482\n",
            "38/239, Train_loss: 0.14822113513946533 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [39/239], Dice Loss: 0.6760, Cross Loss: 0.2613\n",
            "39/239, Train_loss: 0.2612740993499756 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [40/239], Dice Loss: 0.7612, Cross Loss: 0.2613\n",
            "40/239, Train_loss: 0.26126593351364136 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [41/239], Dice Loss: 0.9550, Cross Loss: 0.1482\n",
            "41/239, Train_loss: 0.1482233703136444 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [42/239], Dice Loss: 0.5982, Cross Loss: 0.1482\n",
            "42/239, Train_loss: 0.1482333391904831 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [43/239], Dice Loss: 0.9346, Cross Loss: 0.1482\n",
            "43/239, Train_loss: 0.14823263883590698 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [44/239], Dice Loss: 0.6757, Cross Loss: 0.1482\n",
            "44/239, Train_loss: 0.14821523427963257 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [45/239], Dice Loss: 0.7173, Cross Loss: 0.2613\n",
            "45/239, Train_loss: 0.26132839918136597 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [46/239], Dice Loss: 0.9672, Cross Loss: 0.1482\n",
            "46/239, Train_loss: 0.14820913970470428 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [47/239], Dice Loss: 0.7690, Cross Loss: 0.1482\n",
            "47/239, Train_loss: 0.148216113448143 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [48/239], Dice Loss: 0.8156, Cross Loss: 0.2613\n",
            "48/239, Train_loss: 0.2612649202346802 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [49/239], Dice Loss: 0.8689, Cross Loss: 0.1482\n",
            "49/239, Train_loss: 0.14824168384075165 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [50/239], Dice Loss: 0.8034, Cross Loss: 0.1482\n",
            "50/239, Train_loss: 0.14821529388427734 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [51/239], Dice Loss: 0.6122, Cross Loss: 0.1482\n",
            "51/239, Train_loss: 0.14821463823318481 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [52/239], Dice Loss: 0.7953, Cross Loss: 0.1482\n",
            "52/239, Train_loss: 0.14821377396583557 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [53/239], Dice Loss: 0.7821, Cross Loss: 0.1482\n",
            "53/239, Train_loss: 0.14822006225585938 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [54/239], Dice Loss: 0.7884, Cross Loss: 0.2613\n",
            "54/239, Train_loss: 0.2613111734390259 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [55/239], Dice Loss: 0.5689, Cross Loss: 0.1482\n",
            "55/239, Train_loss: 0.1482173204421997 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [56/239], Dice Loss: 0.9522, Cross Loss: 0.1482\n",
            "56/239, Train_loss: 0.14821690320968628 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [57/239], Dice Loss: 0.7337, Cross Loss: 0.2613\n",
            "57/239, Train_loss: 0.26131731271743774 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [58/239], Dice Loss: 0.9652, Cross Loss: 0.1482\n",
            "58/239, Train_loss: 0.14820627868175507 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [59/239], Dice Loss: 0.9535, Cross Loss: 0.1482\n",
            "59/239, Train_loss: 0.14820539951324463 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [60/239], Dice Loss: 0.7528, Cross Loss: 0.1482\n",
            "60/239, Train_loss: 0.14820684492588043 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [61/239], Dice Loss: 0.9335, Cross Loss: 0.1482\n",
            "61/239, Train_loss: 0.14820566773414612 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [62/239], Dice Loss: 0.6620, Cross Loss: 0.2613\n",
            "62/239, Train_loss: 0.2612994909286499 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [63/239], Dice Loss: 0.8844, Cross Loss: 0.1482\n",
            "63/239, Train_loss: 0.14824092388153076 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [64/239], Dice Loss: 0.8172, Cross Loss: 0.1482\n",
            "64/239, Train_loss: 0.1482141613960266 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [65/239], Dice Loss: 0.6441, Cross Loss: 0.1482\n",
            "65/239, Train_loss: 0.1482054889202118 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [66/239], Dice Loss: 0.6144, Cross Loss: 0.1482\n",
            "66/239, Train_loss: 0.14821551740169525 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [67/239], Dice Loss: 0.9715, Cross Loss: 0.1482\n",
            "67/239, Train_loss: 0.14820393919944763 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [68/239], Dice Loss: 0.7899, Cross Loss: 0.1483\n",
            "68/239, Train_loss: 0.14825031161308289 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [69/239], Dice Loss: 0.9883, Cross Loss: 0.2613\n",
            "69/239, Train_loss: 0.26131677627563477 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [70/239], Dice Loss: 0.9735, Cross Loss: 0.2613\n",
            "70/239, Train_loss: 0.26132380962371826 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [71/239], Dice Loss: 0.5271, Cross Loss: 0.1482\n",
            "71/239, Train_loss: 0.14821162819862366 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [72/239], Dice Loss: 0.9649, Cross Loss: 0.2613\n",
            "72/239, Train_loss: 0.2613294720649719 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [73/239], Dice Loss: 0.9477, Cross Loss: 0.1482\n",
            "73/239, Train_loss: 0.1482202708721161 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [74/239], Dice Loss: 0.8885, Cross Loss: 0.1482\n",
            "74/239, Train_loss: 0.14822031557559967 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [75/239], Dice Loss: 0.9712, Cross Loss: 0.1482\n",
            "75/239, Train_loss: 0.14820696413516998 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [76/239], Dice Loss: 0.8374, Cross Loss: 0.1482\n",
            "76/239, Train_loss: 0.14821356534957886 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [77/239], Dice Loss: 0.9954, Cross Loss: 0.1482\n",
            "77/239, Train_loss: 0.1482158899307251 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [78/239], Dice Loss: 0.9608, Cross Loss: 0.1482\n",
            "78/239, Train_loss: 0.14820979535579681 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [79/239], Dice Loss: 0.9881, Cross Loss: 0.1482\n",
            "79/239, Train_loss: 0.14821437001228333 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [80/239], Dice Loss: 0.7789, Cross Loss: 0.1482\n",
            "80/239, Train_loss: 0.14820167422294617 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [81/239], Dice Loss: 0.9888, Cross Loss: 0.1482\n",
            "81/239, Train_loss: 0.14820310473442078 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [82/239], Dice Loss: 0.4165, Cross Loss: 0.1482\n",
            "82/239, Train_loss: 0.1482010930776596 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [83/239], Dice Loss: 0.9294, Cross Loss: 0.1482\n",
            "83/239, Train_loss: 0.14820078015327454 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [84/239], Dice Loss: 0.9300, Cross Loss: 0.1482\n",
            "84/239, Train_loss: 0.14820298552513123 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [85/239], Dice Loss: 0.7708, Cross Loss: 0.2613\n",
            "85/239, Train_loss: 0.26134347915649414 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [86/239], Dice Loss: 0.7535, Cross Loss: 0.1482\n",
            "86/239, Train_loss: 0.14820127189159393 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [87/239], Dice Loss: 0.9161, Cross Loss: 0.1482\n",
            "87/239, Train_loss: 0.14820101857185364 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [88/239], Dice Loss: 0.9306, Cross Loss: 0.1482\n",
            "88/239, Train_loss: 0.14819972217082977 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [89/239], Dice Loss: 0.9810, Cross Loss: 0.1482\n",
            "89/239, Train_loss: 0.1481994092464447 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [90/239], Dice Loss: 0.9252, Cross Loss: 0.1482\n",
            "90/239, Train_loss: 0.14819905161857605 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [91/239], Dice Loss: 0.6612, Cross Loss: 0.2613\n",
            "91/239, Train_loss: 0.26133057475090027 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [92/239], Dice Loss: 0.9449, Cross Loss: 0.1482\n",
            "92/239, Train_loss: 0.1482064425945282 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [93/239], Dice Loss: 0.9186, Cross Loss: 0.1482\n",
            "93/239, Train_loss: 0.14820601046085358 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [94/239], Dice Loss: 0.8583, Cross Loss: 0.2613\n",
            "94/239, Train_loss: 0.2613479495048523 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [95/239], Dice Loss: 0.8763, Cross Loss: 0.1482\n",
            "95/239, Train_loss: 0.14819841086864471 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [96/239], Dice Loss: 0.9471, Cross Loss: 0.1482\n",
            "96/239, Train_loss: 0.14819841086864471 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [97/239], Dice Loss: 0.8700, Cross Loss: 0.1482\n",
            "97/239, Train_loss: 0.14820140600204468 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [98/239], Dice Loss: 0.8584, Cross Loss: 0.1482\n",
            "98/239, Train_loss: 0.1482025682926178 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [99/239], Dice Loss: 0.8819, Cross Loss: 0.2613\n",
            "99/239, Train_loss: 0.26133257150650024 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [100/239], Dice Loss: 0.9592, Cross Loss: 0.2614\n",
            "100/239, Train_loss: 0.2613563537597656 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [101/239], Dice Loss: 0.6612, Cross Loss: 0.2613\n",
            "101/239, Train_loss: 0.2613494396209717 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [102/239], Dice Loss: 0.8058, Cross Loss: 0.1482\n",
            "102/239, Train_loss: 0.14819779992103577 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [103/239], Dice Loss: 0.8665, Cross Loss: 0.1482\n",
            "103/239, Train_loss: 0.14820744097232819 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [104/239], Dice Loss: 0.8692, Cross Loss: 0.2613\n",
            "104/239, Train_loss: 0.26134711503982544 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [105/239], Dice Loss: 0.8511, Cross Loss: 0.1482\n",
            "105/239, Train_loss: 0.1482202261686325 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [106/239], Dice Loss: 0.9862, Cross Loss: 0.1482\n",
            "106/239, Train_loss: 0.1482209414243698 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [107/239], Dice Loss: 0.7788, Cross Loss: 0.1482\n",
            "107/239, Train_loss: 0.14820769429206848 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [108/239], Dice Loss: 0.8076, Cross Loss: 0.2613\n",
            "108/239, Train_loss: 0.2613290250301361 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [109/239], Dice Loss: 0.7416, Cross Loss: 0.1482\n",
            "109/239, Train_loss: 0.14820167422294617 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [110/239], Dice Loss: 0.9735, Cross Loss: 0.1482\n",
            "110/239, Train_loss: 0.14820167422294617 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [111/239], Dice Loss: 0.9479, Cross Loss: 0.1482\n",
            "111/239, Train_loss: 0.1482016146183014 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [112/239], Dice Loss: 0.8606, Cross Loss: 0.1482\n",
            "112/239, Train_loss: 0.1482016146183014 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [113/239], Dice Loss: 0.8378, Cross Loss: 0.1482\n",
            "113/239, Train_loss: 0.14819839596748352 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [114/239], Dice Loss: 0.7639, Cross Loss: 0.1482\n",
            "114/239, Train_loss: 0.14819824695587158 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [115/239], Dice Loss: 0.9207, Cross Loss: 0.1482\n",
            "115/239, Train_loss: 0.14820607006549835 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [116/239], Dice Loss: 0.8965, Cross Loss: 0.1482\n",
            "116/239, Train_loss: 0.14820577204227448 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [117/239], Dice Loss: 0.9207, Cross Loss: 0.1482\n",
            "117/239, Train_loss: 0.14819678664207458 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [118/239], Dice Loss: 0.8862, Cross Loss: 0.1482\n",
            "118/239, Train_loss: 0.14819656312465668 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [119/239], Dice Loss: 0.9025, Cross Loss: 0.1482\n",
            "119/239, Train_loss: 0.14819806814193726 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [120/239], Dice Loss: 0.9227, Cross Loss: 0.1482\n",
            "120/239, Train_loss: 0.14819632470607758 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [121/239], Dice Loss: 0.7644, Cross Loss: 0.1482\n",
            "121/239, Train_loss: 0.14819687604904175 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [122/239], Dice Loss: 0.6788, Cross Loss: 0.2613\n",
            "122/239, Train_loss: 0.26132723689079285 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [123/239], Dice Loss: 0.9060, Cross Loss: 0.1482\n",
            "123/239, Train_loss: 0.1482100784778595 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [124/239], Dice Loss: 0.5717, Cross Loss: 0.2613\n",
            "124/239, Train_loss: 0.26133307814598083 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [125/239], Dice Loss: 0.8266, Cross Loss: 0.2614\n",
            "125/239, Train_loss: 0.2613620162010193 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [126/239], Dice Loss: 0.9084, Cross Loss: 0.1482\n",
            "126/239, Train_loss: 0.14819709956645966 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [127/239], Dice Loss: 0.9295, Cross Loss: 0.1482\n",
            "127/239, Train_loss: 0.14819519221782684 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [128/239], Dice Loss: 0.8029, Cross Loss: 0.2613\n",
            "128/239, Train_loss: 0.26130399107933044 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [129/239], Dice Loss: 0.8068, Cross Loss: 0.1482\n",
            "129/239, Train_loss: 0.14819470047950745 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [130/239], Dice Loss: 0.6718, Cross Loss: 0.2613\n",
            "130/239, Train_loss: 0.26133739948272705 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [131/239], Dice Loss: 0.8796, Cross Loss: 0.1482\n",
            "131/239, Train_loss: 0.14820519089698792 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [132/239], Dice Loss: 0.9989, Cross Loss: 0.1482\n",
            "132/239, Train_loss: 0.14819684624671936 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [133/239], Dice Loss: 0.7641, Cross Loss: 0.1482\n",
            "133/239, Train_loss: 0.14819711446762085 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [134/239], Dice Loss: 0.8192, Cross Loss: 0.2614\n",
            "134/239, Train_loss: 0.26135069131851196 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [135/239], Dice Loss: 0.6021, Cross Loss: 0.1482\n",
            "135/239, Train_loss: 0.14819757640361786 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [136/239], Dice Loss: 0.9989, Cross Loss: 0.2613\n",
            "136/239, Train_loss: 0.2613295912742615 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [137/239], Dice Loss: 0.5204, Cross Loss: 0.2613\n",
            "137/239, Train_loss: 0.2613433599472046 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [138/239], Dice Loss: 0.8674, Cross Loss: 0.1482\n",
            "138/239, Train_loss: 0.14821353554725647 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [139/239], Dice Loss: 0.5834, Cross Loss: 0.2613\n",
            "139/239, Train_loss: 0.2613222599029541 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [140/239], Dice Loss: 0.9953, Cross Loss: 0.1482\n",
            "140/239, Train_loss: 0.1482134312391281 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [141/239], Dice Loss: 0.8069, Cross Loss: 0.2613\n",
            "141/239, Train_loss: 0.26130184531211853 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [142/239], Dice Loss: 0.9003, Cross Loss: 0.1482\n",
            "142/239, Train_loss: 0.14821016788482666 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [143/239], Dice Loss: 0.9885, Cross Loss: 0.2613\n",
            "143/239, Train_loss: 0.2613399922847748 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [144/239], Dice Loss: 0.8878, Cross Loss: 0.1482\n",
            "144/239, Train_loss: 0.1482039988040924 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [145/239], Dice Loss: 0.6637, Cross Loss: 0.1482\n",
            "145/239, Train_loss: 0.14821234345436096 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [146/239], Dice Loss: 0.9103, Cross Loss: 0.1482\n",
            "146/239, Train_loss: 0.14821411669254303 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [147/239], Dice Loss: 0.9507, Cross Loss: 0.1482\n",
            "147/239, Train_loss: 0.1482059210538864 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [148/239], Dice Loss: 0.9530, Cross Loss: 0.1482\n",
            "148/239, Train_loss: 0.14820563793182373 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [149/239], Dice Loss: 0.5307, Cross Loss: 0.2613\n",
            "149/239, Train_loss: 0.2613459825515747 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [150/239], Dice Loss: 0.9938, Cross Loss: 0.1482\n",
            "150/239, Train_loss: 0.14820000529289246 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [151/239], Dice Loss: 0.5651, Cross Loss: 0.2613\n",
            "151/239, Train_loss: 0.26131802797317505 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [152/239], Dice Loss: 0.9074, Cross Loss: 0.1482\n",
            "152/239, Train_loss: 0.148213192820549 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [153/239], Dice Loss: 0.8481, Cross Loss: 0.1482\n",
            "153/239, Train_loss: 0.1482156366109848 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [154/239], Dice Loss: 0.6936, Cross Loss: 0.2613\n",
            "154/239, Train_loss: 0.2613174617290497 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [155/239], Dice Loss: 0.8178, Cross Loss: 0.1482\n",
            "155/239, Train_loss: 0.14822803437709808 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [156/239], Dice Loss: 0.8100, Cross Loss: 0.2612\n",
            "156/239, Train_loss: 0.26123499870300293 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [157/239], Dice Loss: 0.7968, Cross Loss: 0.2612\n",
            "157/239, Train_loss: 0.26122626662254333 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [158/239], Dice Loss: 0.7118, Cross Loss: 0.2612\n",
            "158/239, Train_loss: 0.2612334191799164 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [159/239], Dice Loss: 0.9656, Cross Loss: 0.1483\n",
            "159/239, Train_loss: 0.14826558530330658 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [160/239], Dice Loss: 0.7443, Cross Loss: 0.2612\n",
            "160/239, Train_loss: 0.26123952865600586 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [161/239], Dice Loss: 0.8729, Cross Loss: 0.1483\n",
            "161/239, Train_loss: 0.14825966954231262 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [162/239], Dice Loss: 0.9087, Cross Loss: 0.2613\n",
            "162/239, Train_loss: 0.2613285183906555 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [163/239], Dice Loss: 0.9855, Cross Loss: 0.1483\n",
            "163/239, Train_loss: 0.1482665240764618 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [164/239], Dice Loss: 0.8809, Cross Loss: 0.1483\n",
            "164/239, Train_loss: 0.14827074110507965 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [165/239], Dice Loss: 0.8636, Cross Loss: 0.2613\n",
            "165/239, Train_loss: 0.2612767815589905 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [166/239], Dice Loss: 0.9910, Cross Loss: 0.2613\n",
            "166/239, Train_loss: 0.2612619400024414 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [167/239], Dice Loss: 0.9592, Cross Loss: 0.2612\n",
            "167/239, Train_loss: 0.2611965537071228 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [168/239], Dice Loss: 0.7904, Cross Loss: 0.2612\n",
            "168/239, Train_loss: 0.261163592338562 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [169/239], Dice Loss: 0.8215, Cross Loss: 0.2612\n",
            "169/239, Train_loss: 0.2611563801765442 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [170/239], Dice Loss: 0.9179, Cross Loss: 0.2611\n",
            "170/239, Train_loss: 0.2611115574836731 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [171/239], Dice Loss: 0.9026, Cross Loss: 0.2610\n",
            "171/239, Train_loss: 0.2609963119029999 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [172/239], Dice Loss: 0.7183, Cross Loss: 0.1484\n",
            "172/239, Train_loss: 0.1484082043170929 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [173/239], Dice Loss: 0.8912, Cross Loss: 0.2610\n",
            "173/239, Train_loss: 0.2610390782356262 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [174/239], Dice Loss: 0.5871, Cross Loss: 0.2605\n",
            "174/239, Train_loss: 0.2605397701263428 Train_dice: tensor(0.2605, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [175/239], Dice Loss: 0.9624, Cross Loss: 0.2602\n",
            "175/239, Train_loss: 0.2601562440395355 Train_dice: tensor(0.2602, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [176/239], Dice Loss: 0.8090, Cross Loss: 0.1490\n",
            "176/239, Train_loss: 0.14903473854064941 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [177/239], Dice Loss: 0.6889, Cross Loss: 0.1492\n",
            "177/239, Train_loss: 0.14919504523277283 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [178/239], Dice Loss: 0.8182, Cross Loss: 0.2592\n",
            "178/239, Train_loss: 0.25922882556915283 Train_dice: tensor(0.2592, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [179/239], Dice Loss: 0.8451, Cross Loss: 0.1494\n",
            "179/239, Train_loss: 0.14938369393348694 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [180/239], Dice Loss: 0.8155, Cross Loss: 0.1495\n",
            "180/239, Train_loss: 0.14948910474777222 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [181/239], Dice Loss: 0.8869, Cross Loss: 0.2595\n",
            "181/239, Train_loss: 0.25949928164482117 Train_dice: tensor(0.2595, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [182/239], Dice Loss: 0.6579, Cross Loss: 0.1497\n",
            "182/239, Train_loss: 0.14970506727695465 Train_dice: tensor(0.1497, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [183/239], Dice Loss: 0.9555, Cross Loss: 0.2590\n",
            "183/239, Train_loss: 0.25903064012527466 Train_dice: tensor(0.2590, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [184/239], Dice Loss: 0.8780, Cross Loss: 0.1498\n",
            "184/239, Train_loss: 0.14983373880386353 Train_dice: tensor(0.1498, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [185/239], Dice Loss: 0.8755, Cross Loss: 0.1498\n",
            "185/239, Train_loss: 0.1498188078403473 Train_dice: tensor(0.1498, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [186/239], Dice Loss: 0.7551, Cross Loss: 0.1498\n",
            "186/239, Train_loss: 0.1497504562139511 Train_dice: tensor(0.1498, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [187/239], Dice Loss: 0.9800, Cross Loss: 0.1494\n",
            "187/239, Train_loss: 0.1494479924440384 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [188/239], Dice Loss: 0.8564, Cross Loss: 0.2597\n",
            "188/239, Train_loss: 0.25968554615974426 Train_dice: tensor(0.2597, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [189/239], Dice Loss: 0.5903, Cross Loss: 0.1490\n",
            "189/239, Train_loss: 0.14897343516349792 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [190/239], Dice Loss: 0.9738, Cross Loss: 0.1488\n",
            "190/239, Train_loss: 0.14884406328201294 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [191/239], Dice Loss: 0.7012, Cross Loss: 0.2600\n",
            "191/239, Train_loss: 0.26004838943481445 Train_dice: tensor(0.2600, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [192/239], Dice Loss: 0.9143, Cross Loss: 0.1487\n",
            "192/239, Train_loss: 0.14874446392059326 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [193/239], Dice Loss: 0.9608, Cross Loss: 0.2604\n",
            "193/239, Train_loss: 0.2603679597377777 Train_dice: tensor(0.2604, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [194/239], Dice Loss: 0.9935, Cross Loss: 0.1487\n",
            "194/239, Train_loss: 0.14865925908088684 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [195/239], Dice Loss: 0.9995, Cross Loss: 0.1486\n",
            "195/239, Train_loss: 0.14863653481006622 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [196/239], Dice Loss: 0.9986, Cross Loss: 0.2606\n",
            "196/239, Train_loss: 0.2605554461479187 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [197/239], Dice Loss: 0.9988, Cross Loss: 0.1486\n",
            "197/239, Train_loss: 0.1485878825187683 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [198/239], Dice Loss: 0.9984, Cross Loss: 0.1486\n",
            "198/239, Train_loss: 0.14857405424118042 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [199/239], Dice Loss: 0.7924, Cross Loss: 0.1485\n",
            "199/239, Train_loss: 0.14853611588478088 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [200/239], Dice Loss: 0.6425, Cross Loss: 0.1485\n",
            "200/239, Train_loss: 0.14850914478302002 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [201/239], Dice Loss: 0.7960, Cross Loss: 0.2608\n",
            "201/239, Train_loss: 0.26076027750968933 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [202/239], Dice Loss: 0.5989, Cross Loss: 0.2606\n",
            "202/239, Train_loss: 0.26056820154190063 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [203/239], Dice Loss: 0.8152, Cross Loss: 0.1485\n",
            "203/239, Train_loss: 0.14851690828800201 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [204/239], Dice Loss: 0.9908, Cross Loss: 0.1485\n",
            "204/239, Train_loss: 0.1485157608985901 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [205/239], Dice Loss: 0.7116, Cross Loss: 0.2607\n",
            "205/239, Train_loss: 0.26071634888648987 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [206/239], Dice Loss: 0.8311, Cross Loss: 0.1485\n",
            "206/239, Train_loss: 0.1484622061252594 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [207/239], Dice Loss: 0.8845, Cross Loss: 0.1485\n",
            "207/239, Train_loss: 0.14851632714271545 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [208/239], Dice Loss: 0.8743, Cross Loss: 0.1485\n",
            "208/239, Train_loss: 0.14851278066635132 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [209/239], Dice Loss: 0.8590, Cross Loss: 0.2607\n",
            "209/239, Train_loss: 0.26065921783447266 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [210/239], Dice Loss: 0.9991, Cross Loss: 0.1485\n",
            "210/239, Train_loss: 0.14852756261825562 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [211/239], Dice Loss: 0.8428, Cross Loss: 0.1485\n",
            "211/239, Train_loss: 0.14846216142177582 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [212/239], Dice Loss: 0.8570, Cross Loss: 0.1485\n",
            "212/239, Train_loss: 0.14845184981822968 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [213/239], Dice Loss: 0.6195, Cross Loss: 0.2608\n",
            "213/239, Train_loss: 0.26077595353126526 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [214/239], Dice Loss: 0.6275, Cross Loss: 0.2608\n",
            "214/239, Train_loss: 0.26078033447265625 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [215/239], Dice Loss: 0.5958, Cross Loss: 0.1484\n",
            "215/239, Train_loss: 0.1484335958957672 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [216/239], Dice Loss: 0.9114, Cross Loss: 0.1484\n",
            "216/239, Train_loss: 0.1484350860118866 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [217/239], Dice Loss: 0.5769, Cross Loss: 0.2608\n",
            "217/239, Train_loss: 0.260841429233551 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [218/239], Dice Loss: 0.6314, Cross Loss: 0.2608\n",
            "218/239, Train_loss: 0.26080140471458435 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [219/239], Dice Loss: 0.8784, Cross Loss: 0.1485\n",
            "219/239, Train_loss: 0.1484641283750534 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [220/239], Dice Loss: 0.9852, Cross Loss: 0.1485\n",
            "220/239, Train_loss: 0.1484634429216385 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [221/239], Dice Loss: 0.9734, Cross Loss: 0.1485\n",
            "221/239, Train_loss: 0.14845652878284454 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [222/239], Dice Loss: 0.9951, Cross Loss: 0.1485\n",
            "222/239, Train_loss: 0.1484660804271698 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [223/239], Dice Loss: 0.9879, Cross Loss: 0.1485\n",
            "223/239, Train_loss: 0.14847305417060852 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [224/239], Dice Loss: 0.5971, Cross Loss: 0.1485\n",
            "224/239, Train_loss: 0.14846190810203552 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [225/239], Dice Loss: 0.9254, Cross Loss: 0.1484\n",
            "225/239, Train_loss: 0.1484294831752777 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [226/239], Dice Loss: 0.9897, Cross Loss: 0.1484\n",
            "226/239, Train_loss: 0.14843079447746277 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [227/239], Dice Loss: 0.9999, Cross Loss: 0.1484\n",
            "227/239, Train_loss: 0.14841191470623016 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [228/239], Dice Loss: 0.9997, Cross Loss: 0.1484\n",
            "228/239, Train_loss: 0.14836445450782776 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [229/239], Dice Loss: 0.9991, Cross Loss: 0.1482\n",
            "229/239, Train_loss: 0.14823441207408905 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [230/239], Dice Loss: 0.9229, Cross Loss: 0.1482\n",
            "230/239, Train_loss: 0.14822730422019958 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [231/239], Dice Loss: 0.8563, Cross Loss: 0.1483\n",
            "231/239, Train_loss: 0.14825475215911865 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [232/239], Dice Loss: 0.9528, Cross Loss: 0.2610\n",
            "232/239, Train_loss: 0.26101019978523254 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [233/239], Dice Loss: 0.7758, Cross Loss: 0.1483\n",
            "233/239, Train_loss: 0.14831504225730896 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [234/239], Dice Loss: 0.9991, Cross Loss: 0.1483\n",
            "234/239, Train_loss: 0.14830604195594788 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [235/239], Dice Loss: 0.5617, Cross Loss: 0.2611\n",
            "235/239, Train_loss: 0.26114022731781006 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [236/239], Dice Loss: 0.9973, Cross Loss: 0.2611\n",
            "236/239, Train_loss: 0.26109880208969116 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [237/239], Dice Loss: 1.0000, Cross Loss: 0.1483\n",
            "237/239, Train_loss: 0.14831221103668213 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [238/239], Dice Loss: 0.9310, Cross Loss: 0.1483\n",
            "238/239, Train_loss: 0.14834779500961304 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [239/239], Dice Loss: 0.9991, Cross Loss: 0.2611\n",
            "239/239, Train_loss: 0.26106008887290955 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1861\n",
            "Epoch_metric: tensor(0.1861, device='cuda:0', grad_fn=<AliasBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "def updateLogs(path, data):\n",
        "    f = open(path,'a')\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "\n",
        "model_dir = './results/lesion/Focal_dice_Loss/'\n",
        "\n",
        "\n",
        "start_from = 1\n",
        "test_interval =1\n",
        "num_epochs = 100\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "\n",
        "save_loss_train = []\n",
        "save_loss_test = []\n",
        "save_metric_train = []\n",
        "save_metric_test = []\n",
        "if (start_from != 1):\n",
        "  save_loss_train, save_metric_train, save_loss_test, save_metric_test= [x.tolist() for x in load_metrices(load_from)]\n",
        "  if(len(save_metric_test)):\n",
        "        best_metric = max(save_metric_test)\n",
        "  best_metric_epoch = -2\n",
        "train_loader, test_loader = data_in\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    train_epoch_loss = 0\n",
        "    train_step = 0\n",
        "    epoch_metric_train = 0\n",
        "\n",
        "    for batch_id, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        train_step += 1\n",
        "        \n",
        "        volume = batch_data[\"image\"]\n",
        "        label = batch_data[\"label\"]\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          volume, label = (volume.to(device), label.to(device))\n",
        "\n",
        "        \n",
        "        # Zero the gradients\n",
        "        optimizer_out.zero_grad()\n",
        "        optimizer_bb.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        mask_output, bb_output = model(volume)\n",
        "        \n",
        "        #Compute loss for segmentation mask\n",
        "        # mask_loss = loss_function(label, mask_output)\n",
        "        \n",
        "       \n",
        "        bb_target = one_hot_tensor[batch_id]\n",
        "        \n",
        "       \n",
        "        bb_target = bb_target.view(1, -1).to(device)\n",
        "        \n",
        "        #print(bb_target.cpu().detach().numpy())\n",
        "        #print(\"True \",encoder.inverse_transform(bb_target))\n",
        "        #print(bb_output.cpu().detach().numpy())\n",
        "        #bb_output.to(device)\n",
        "        #bb = bb_output.detach().numpy()\n",
        "        #print(\"Predicted \", encoder.inverse_transform(bb))\n",
        "       \n",
        "        dsc = dsc_loss( mask_output, label)\n",
        "        cse_loss = cross_entropy_loss(bb_output, bb_target.float())\n",
        "        # Total loss as a combination of mask loss and bounding box classifier loss\n",
        "        #train_loss = criterion( mask_output, label, bb_output, bb_target)\n",
        "        \n",
        "        # Backward pass\n",
        "        dsc.backward( retain_graph=True)\n",
        "        cse_loss.backward()\n",
        "        \n",
        "        # Update weights\n",
        "        optimizer_out.step()\n",
        "        optimizer_bb.step()\n",
        "        \n",
        "        # Print training progress\n",
        "        print('Epoch [{}/{}], Batch [{}/{}], Dice Loss: {:.4f}, Cross Loss: {:.4f}'\n",
        "              .format(epoch+1, num_epochs, batch_id+1, len(train_loader), dsc.item(), cse_loss.item()))\n",
        "        updateLogs(os.path.join(model_dir, \"logs.txt\"), f'Epoch [{epoch+1}/{num_epochs}], Batch [{ batch_id+1}/{ len(train_loader)}],Dice Loss: {dsc.item():.4f}, Cross Loss:  {cse_loss.item():.4f}\\n')\n",
        "        updateLogs(os.path.join(model_dir, \"logs.txt\"), f'True {bb_target.cpu().detach().numpy()}\\n Predicted {bb_output.cpu().detach().numpy()} \\n')\n",
        "\n",
        "\n",
        "        train_epoch_loss += cse_loss.item()\n",
        "        print(\n",
        "                f\"{train_step}/{len(train_loader) // train_loader.batch_size}, \"\n",
        "                f\"Train_loss: {cse_loss.item()}\", end=\" \")\n",
        "\n",
        "        train_metric = cross_entropy_loss(bb_output, bb_target.float())\n",
        "        epoch_metric_train += train_metric\n",
        "        print(f'Train_dice: {train_metric}')\n",
        "\n",
        "    print('-'*20)\n",
        "        \n",
        "    train_epoch_loss /= train_step\n",
        "    print(f'Epoch_loss: {train_epoch_loss:.4f}')\n",
        "    save_loss_train.append(train_epoch_loss)\n",
        "    np.save(os.path.join(model_dir, 'loss_train.npy'), save_loss_train)\n",
        "        \n",
        "    epoch_metric_train /= train_step\n",
        "    print(f'Epoch_metric: {epoch_metric_train}')\n",
        "\n",
        "    save_metric_train.append(epoch_metric_train.cpu().detach().numpy())\n",
        "    np.save(os.path.join(model_dir, 'metric_train.npy'), save_metric_train)\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(\n",
        "            model_dir, \"current_metric_model.pth\"))\n",
        "        \n",
        "    updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"{'-'*20}{epoch+1} \\nEpoch_loss: {train_epoch_loss:.4f}\\nEpoch_metric: {epoch_metric_train}\\n\")\n",
        "\n",
        "    if (epoch + 1) % test_interval == 0:\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          test_epoch_loss = 0\n",
        "          test_metric = 0\n",
        "          epoch_metric_test = 0\n",
        "          test_step = 0\n",
        "          total_loss = 0\n",
        "          for batch_id, batch_data in enumerate(test_loader):\n",
        "              test_step += 1\n",
        "\n",
        "              volume = batch_data[\"image\"]\n",
        "              label = batch_data[\"label\"]\n",
        "              if torch.cuda.is_available():\n",
        "                  volume, label = (volume.to(device), label.to(device))\n",
        "              \n",
        "              # Forward pass\n",
        "              mask_output, bb_output = model(volume)\n",
        "              \n",
        "              bb_target = one_hot_tensor[batch_id]\n",
        "        \n",
        "       \n",
        "              bb_target = bb_target.view(1, -1).to(device)\n",
        "              \n",
        "              # Compute loss for bounding box classifier\n",
        "\n",
        "              test_loss = cross_entropy_loss( bb_output, bb_target.float())\n",
        "              \n",
        "              test_epoch_loss += test_loss.item()\n",
        "              test_metric = cross_entropy_loss( bb_output, bb_target.float())\n",
        "              \n",
        "              epoch_metric_test += test_metric\n",
        "          \n",
        "          # Calculate the average loss across all batches in the test_loader\n",
        "          test_epoch_loss /= test_step\n",
        "          print(f'test_loss_epoch: {test_epoch_loss:.4f}')\n",
        "          save_loss_test.append(test_epoch_loss)\n",
        "          np.save(os.path.join(model_dir, 'loss_test.npy'), save_loss_test)\n",
        "\n",
        "          epoch_metric_test /= test_step\n",
        "          print(f'test_dice_epoch: {epoch_metric_test}')\n",
        "          save_metric_test.append(epoch_metric_test.cpu().detach().numpy())\n",
        "          np.save(os.path.join(model_dir, 'metric_test.npy'), save_metric_test)\n",
        "\n",
        "\n",
        "          if epoch_metric_test > best_metric:\n",
        "              best_metric = epoch_metric_test\n",
        "              best_metric_epoch = epoch + 1\n",
        "              torch.save(model.state_dict(), os.path.join(\n",
        "              model_dir, \"best_metric_model.pth\"))\n",
        "                \n",
        "          print(\n",
        "                    f\"current epoch: {epoch + 1} current mean dice: {epoch_metric_test}\"\n",
        "                    f\"\\nbest mean dice: {best_metric} \"\n",
        "                     \n",
        "                    f\"at epoch: {best_metric_epoch}\"\n",
        "                )\n",
        "          \n",
        "          \n",
        "          updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"{'-'*30}\\ncurrent epoch: {epoch + 1} \\n\"\n",
        "                    f'test_dice_epoch: {epoch_metric_test}\\n'\n",
        "                    f'test_loss_epoch: {test_epoch_loss:.4f}\\n'\n",
        "                    f\"best mean dice: {best_metric} \"\n",
        "                    f\"at epoch: {best_metric_epoch}\\n\")\n",
        "          \n",
        "        updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"train completed, best_metric: {best_metric}\"\n",
        "        f\"at epoch: {best_metric_epoch}\\n\")\n",
        "\n",
        "        update_history([start_from, num_epochs, best_metric, best_metric_epoch],model_dir=model_dir)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl61WEOJ4ZQ9"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "def load_metrices(path):\n",
        "  metrices_dir = path\n",
        "  train_loss = train_metric = test_loss = test_metric =np.array([])\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'loss_train.npy')):\n",
        "    train_loss = np.load(os.path.join(metrices_dir, 'loss_train.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'metric_train.npy')):\n",
        "    train_metric = np.load(os.path.join(metrices_dir, 'metric_train.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'loss_test.npy')):\n",
        "    test_loss = np.load(os.path.join(metrices_dir, 'loss_test.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'metric_test.npy')):\n",
        "    test_metric = np.load(os.path.join(metrices_dir, 'metric_test.npy'))\n",
        "  return train_loss, train_metric, test_loss, test_metric\n",
        "\n",
        "def dice_metric(predicted, target):\n",
        "    '''\n",
        "    In this function we take `predicted` and `target` (label) to calculate the dice coeficient then we use it \n",
        "    to calculate a metric value for the training and the validation.\n",
        "    '''\n",
        "    dice_value = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
        "    value = 1 - dice_value(predicted, target).item()\n",
        "    return value\n",
        "\n",
        "\n",
        "def get_time():\n",
        "  utc_time= datetime.datetime.now(pytz.utc)\n",
        "  local_time = utc_time.astimezone(pytz.timezone('Asia/Colombo'))\n",
        "  return local_time.strftime(\"%Y:%m:%d %H:%M:%S\")\n",
        "\n",
        "def update_history(data,model_dir):\n",
        "  history_file_path = model_dir + \"history.csv\"\n",
        "  if not os.path.exists(history_file_path):\n",
        "    with open(history_file_path,'a') as fd:\n",
        "        fd.write(\",\".join([\"Start\", \"End\", \"Best Matrix\", \"Best M. At\"]))\n",
        "  with open(history_file_path,'a') as fd:\n",
        "      str_data=[str(x) for x in (data + [get_time()])]\n",
        "      fd.write(\"\\n\" + \",\".join(str_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAA2fmxbywsV"
      },
      "source": [
        "## Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmqiWAAdywsW",
        "outputId": "4cfc4d58-7483-4b40-b42e-c8a66adaa19f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(384, 384, 16) (384, 384, 16)\n"
          ]
        }
      ],
      "source": [
        "# first image, label from orignal image\n",
        "imagea =nib.load(data[0][\"image\"]).get_fdata()\n",
        "labela =nib.load(data[0][\"label\"]).get_fdata()\n",
        "print(imagea.shape,labela.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGnnLlngywsW",
        "outputId": "a8f47b40-f866-4daa-f1ec-8eaede514f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 128, 128, 16]) torch.Size([1, 1, 128, 128, 16])\n"
          ]
        }
      ],
      "source": [
        "# first image, label from preprossed image\n",
        "image1= first(train_loader)[\"image\"]\n",
        "label1 = first(train_loader)[\"label\"]\n",
        "print(image1.shape,label1.shape)\n",
        "image11 =image1.get_array()\n",
        "label11 = label1.get_array()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8kWaIwrywsW"
      },
      "outputs": [],
      "source": [
        "# Define a function to visualize the data\n",
        "def explore_3dimage2(layer):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    channel = 2\n",
        "    # plt.imshow(image11[0,0,:,:,layer], cmap='gray');\n",
        "    # plt.title('Explore Layers of Prostate MRI', fontsize=20)\n",
        "    # plt.axis('off')\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
        "\n",
        "    ax[0].imshow(image1[0,0,:,:,layer], cmap='gray')\n",
        "    ax[0].set_title(f\"Image\", fontsize=15)\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(label1[0,0,:,:,layer])\n",
        "    ax[1].axis('off')\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229,
          "referenced_widgets": [
            "8d4ca001c8124085a6d046e89e56720c",
            "0e3c8730bba84534ab76e92089c68f10",
            "c79a410f98b942f5b4ea8277ee05ab01",
            "4b89f553301541c0aad7eec599168ff3",
            "538ec8a1afa44636a3f40e2bf56dcfaa",
            "cd035e61a5054420acc4fa0bdbc3a68a",
            "921e5059901440ec9f40c0a4ebb278dd"
          ]
        },
        "id": "qmJVtNMWywsW",
        "outputId": "2ad740b3-b743-4984-84df-415a9b78c895"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d4ca001c8124085a6d046e89e56720c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=7, description='layer', max=15), Output()), _dom_classes=('widget-intera…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<function __main__.explore_3dimage2(layer)>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Explore the preprocessed image, label\n",
        "interact(explore_3dimage2, layer=(0, 15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEDLevmu6TRU"
      },
      "source": [
        "## 3 attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDxbp9pcLjX3",
        "outputId": "7362c2ad-e474-4341-c77c-70d22ef9f098"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 6])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MulticlassImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MulticlassImageClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(4, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(32)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(64)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm3d(128)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8 * 2, 512)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.relu4 = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = x.view(-1, 128 * 8 * 8 * 2)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the model, loss function and optimizer\n",
        "model = MulticlassImageClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "model(torch.randn(1,4,128,128,16)).shape\n",
        "\n",
        "# # Train the model\n",
        "# for epoch in range(num_epochs):\n",
        "#     for i, (inputs, labels) in enumerate(train_loader):\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         if i % 100 == 0:\n",
        "#             print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# # Test the model\n",
        "# with torch.no_grad():\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for inputs, labels in test_loader:\n",
        "#         outputs = model(inputs)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#     print(f\"Test Accuracy: {(100 * correct / total):.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0oVgMzR_ywr_",
        "2y1omcz8ywsF",
        "QAA2fmxbywsV"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "packages",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e3c8730bba84534ab76e92089c68f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "IntSliderModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": true,
            "description": "layer",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_538ec8a1afa44636a3f40e2bf56dcfaa",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_cd035e61a5054420acc4fa0bdbc3a68a",
            "value": 7
          }
        },
        "4b89f553301541c0aad7eec599168ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "538ec8a1afa44636a3f40e2bf56dcfaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4ca001c8124085a6d046e89e56720c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e3c8730bba84534ab76e92089c68f10",
              "IPY_MODEL_c79a410f98b942f5b4ea8277ee05ab01"
            ],
            "layout": "IPY_MODEL_4b89f553301541c0aad7eec599168ff3"
          }
        },
        "921e5059901440ec9f40c0a4ebb278dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c79a410f98b942f5b4ea8277ee05ab01": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_921e5059901440ec9f40c0a4ebb278dd",
            "msg_id": "",
            "outputs": []
          }
        },
        "cd035e61a5054420acc4fa0bdbc3a68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "SliderStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}