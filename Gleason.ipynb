{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "NkjTDA-cXZLV"
      },
      "source": [
        ">[Load Colab](#scrollTo=0oVgMzR_ywr_)\n",
        "\n",
        ">[initiate](#scrollTo=2y1omcz8ywsF)\n",
        "\n",
        ">[Gleason Score](#scrollTo=diKStuuskvkS)\n",
        "\n",
        ">[Augment](#scrollTo=aqf2izgHRetB)\n",
        "\n",
        ">[Transform](#scrollTo=L3A1UwZUywsS)\n",
        "\n",
        ">[Train](#scrollTo=QA0eNCpVywsW)\n",
        "\n",
        ">[ResNet](#scrollTo=AOE9Lac4k6cy)\n",
        "\n",
        ">[Trash](#scrollTo=txse4NidVs-S)\n",
        "\n",
        ">>[Check](#scrollTo=QAA2fmxbywsV)\n",
        "\n",
        ">>[3 attempt](#scrollTo=oEDLevmu6TRU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93MDd95az4Te",
        "outputId": "b3c08a0c-72eb-4cdd-f088-82b76b11ccb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'PROSTATEx_masks'...\n",
            "remote: Enumerating objects: 5026, done.\u001b[K\n",
            "remote: Counting objects: 100% (376/376), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 5026 (delta 353), reused 195 (delta 179), pack-reused 4650\u001b[K\n",
            "Receiving objects: 100% (5026/5026), 902.72 MiB | 14.32 MiB/s, done.\n",
            "Resolving deltas: 100% (3701/3701), done.\n",
            "Updating files: 100% (2565/2565), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rcuocolo/PROSTATEx_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oVgMzR_ywr_"
      },
      "source": [
        "# Load Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4mUqLGhywsA",
        "outputId": "40e40dac-cc3e-43ae-ce1b-88caceda5f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P_ajbq-ywsC",
        "outputId": "136d078e-c866-4e31-a050-227b11cf4bb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1wCK5vBkYx3cQUsc06EFZG4zaqE0idTPk/Prostate_Lesion\n"
          ]
        }
      ],
      "source": [
        "#for colab\n",
        "%cd \"/content/drive/MyDrive/Prostate_Lesion/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0lVBl5IywsE",
        "outputId": "d0992cdd-492f-44f0-dcbb-3c7edf755256"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting monai\n",
            "  Downloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dicom2nifti\n",
            "  Downloading dicom2nifti-2.4.8-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 4)) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 5)) (3.7.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 6)) (7.7.1)\n",
            "Collecting elasticdeform\n",
            "  Downloading elasticdeform-0.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.3/91.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from -r requirement.txt (line 8)) (0.19.3)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from monai->-r requirement.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.9/dist-packages (from monai->-r requirement.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from dicom2nifti->-r requirement.txt (line 2)) (1.10.1)\n",
            "Collecting python-gdcm\n",
            "  Downloading python_gdcm-3.0.21-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.9/dist-packages (from dicom2nifti->-r requirement.txt (line 2)) (3.0.2)\n",
            "Collecting pydicom>=2.2.0\n",
            "  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirement.txt (line 3)) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirement.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirement.txt (line 5)) (1.0.7)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (5.5.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (3.0.7)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (3.6.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->-r requirement.txt (line 6)) (7.34.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirement.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirement.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirement.txt (line 8)) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->-r requirement.txt (line 8)) (2023.4.12)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r requirement.txt (line 5)) (3.15.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirement.txt (line 6)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirement.txt (line 6)) (6.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (3.0.38)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirement.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8->monai->-r requirement.txt (line 1)) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8->monai->-r requirement.txt (line 1)) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8->monai->-r requirement.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (6.4.8)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.8.3)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.5.6)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (23.2.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (5.8.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (21.3.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (6.5.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (5.3.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirement.txt (line 6)) (0.2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8->monai->-r requirement.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8->monai->-r requirement.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (21.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (4.11.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (6.0.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.2.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (4.9.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.2.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.7.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (23.1.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirement.txt (line 6)) (2.21)\n",
            "Installing collected packages: python-gdcm, pydicom, jedi, elasticdeform, dicom2nifti, torchmetrics, monai\n",
            "Successfully installed dicom2nifti-2.4.8 elasticdeform-0.5.0 jedi-0.18.2 monai-1.1.0 pydicom-2.3.1 python-gdcm-3.0.21 torchmetrics-0.11.4\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirement.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y1omcz8ywsF"
      },
      "source": [
        "# initiate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UbJoro_hywsG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import os\n",
        "from glob import glob\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import dicom2nifti\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from ipywidgets import interact, interactive, IntSlider, ToggleButtons\n",
        "import os\n",
        "\n",
        "from monai.utils import first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MVBHgyXmywsG"
      },
      "outputs": [],
      "source": [
        "HOME_DIR =       \"./\"\n",
        "DATA_DIR =       \"./PROSTATEx_masks/Files/lesions/\"\n",
        "OUT_DIR =        \"./results/gleason/\"\n",
        "SLICED_OUT_DIR = \"./data/sliced/\"\n",
        "AUG_OUT_DIR = \"./data/augmented/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mp9_EMN1trtr"
      },
      "outputs": [],
      "source": [
        "from monai.utils import first\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pasts0BAz-F1"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snhTABh_0DHQ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Prostate_Lesion/PROSTATEx_masks/Files/lesions/Image_list.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeEybDb40LeL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"images/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"images/\")\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"prostates/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"prostates/\")\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"pz_masks/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"pz_masks/\")\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"tz_masks/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"tz_masks/\")\n",
        "if not os.path.exists(SLICED_OUT_DIR + \"labels/\"):\n",
        "    os.makedirs(SLICED_OUT_DIR + \"labels/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KggD8-P0NjS"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for index, row in df.iterrows():\n",
        "    value = row['T2'].split(\"-\")\n",
        "    numb= value[1].split(\"_\")[0]\n",
        "    if int(numb) == 128:\n",
        "        t2 = DATA_DIR+'Images/T2/'+row['T2']+'.nii'\n",
        "    else:\n",
        "        t2 = DATA_DIR+'Images/T2/'+row['T2']+'.nii.gz'\n",
        "    if int(numb) == 25:\n",
        "        adc = DATA_DIR+'Images/ADC/'+row['ADC']+'a.nii.gz'\n",
        "    elif int(numb) == 113:\n",
        "        value = row['ADC'][:-2]+'9'\n",
        "        adc = DATA_DIR+'Images/ADC/'+value+'.nii.gz'\n",
        "    elif int(numb) == 203:\n",
        "        adc = DATA_DIR+'Images/ADC/'+'ProstateX-0203_diffusie-3ProstateX-0203_diffusie-3Scan-4bval_fs_7.niiScan-4bval_fs_7.nii.gz'\n",
        "    else:\n",
        "        adc = DATA_DIR+'Images/ADC/'+row['ADC']+'.nii.gz'\n",
        "   \n",
        "    data.append({ 'T2': t2, 'ADC': adc , 'name': value[0] + '_' +str(numb)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTNBHbJE0SCC"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def load_nifti(image_nifty_file, label_nifty_file):\n",
        "    # load the image and label file, get the image content and return a numpy array for each\n",
        "    image = nib.load(image_nifty_file)\n",
        "    label = nib.load(label_nifty_file)\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "def save_to_json(data, path):\n",
        "  with open(path, 'w') as fp:\n",
        "    json.dump(data, fp)\n",
        "\n",
        "\n",
        "def remove_slices(img,start, end):\n",
        "  imgvol = np.array( img.dataobj )\n",
        "  imgvol = imgvol[ :, :, start:end ]\n",
        "  newimg = nib.Nifti1Image ( imgvol, img.affine )\n",
        "  return newimg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T60QdJfp0XuN"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATA_DIR +'image_list.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ14L86u0bWW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "\n",
        "# specify the directory path\n",
        "image_path = './PROSTATEx_masks/Files/prostate/Images/'\n",
        "prostate_path = './PROSTATEx_masks/Files/prostate/mask_prostate/'\n",
        "pz_path = './PROSTATEx_masks/Files/prostate/mask_pz/'\n",
        "tz_path = './PROSTATEx_masks/Files/prostate/mask_tz/'\n",
        "lesion_path = './PROSTATEx_masks/Files/lesions/Masks/T2/'\n",
        "#t2_path = './PROSTATEx_masks/Files/lesions/Images/T2/'\n",
        "#adc_path = './PROSTATEx_masks/Files/lesions/Masks/ADC/'\n",
        "\n",
        "# get all the file names in the directory\n",
        "t2_images = os.listdir(image_path)\n",
        "prostates = os.listdir(prostate_path)\n",
        "pz_images = os.listdir(pz_path)\n",
        "tz_images = os.listdir(tz_path)\n",
        "t2_lesions = os.listdir(lesion_path)\n",
        "#t2_2 = os.listdir(t2_path)\n",
        "#adc_images = os.listdir(adc_path)\n",
        "\n",
        "data = []\n",
        "for i in range(len(df['T2'])):\n",
        "    t2 = nib.load(image_path + t2_images[i])\n",
        "    prostate = nib.load(prostate_path + prostates[i])\n",
        "    pz = nib.load(pz_path + pz_images[i])\n",
        "    tz = nib.load(tz_path + tz_images[i])\n",
        "    lesion = nib.load(lesion_path + t2_lesions[i])\n",
        "    name = t2_images[i].split('_')[0]\n",
        "\n",
        "    \n",
        "    if t2.shape[2] == lesion.shape[2]:\n",
        "        path = {\n",
        "            'T2': image_path + t2_images[i],\n",
        "            'prostate': prostate_path + prostates[i],\n",
        "            'PZ' : pz_path + pz_images[i],\n",
        "            'TZ': tz_path + tz_images[i],\n",
        "            'label': lesion_path + t2_lesions[i],\n",
        "            'Name':name\n",
        "        }\n",
        "        data.append(path)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBzW3e9M0hfv"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def load_nifti(image_path_1,image_path_2,image_path_3,image_path_4, label_path):\n",
        "    # load the image and label file, get the image content and return a numpy array for each\n",
        "    image1 = nib.load(image_path_1)\n",
        "    image2 = nib.load(image_path_2)\n",
        "    image3 = nib.load(image_path_3)\n",
        "    image4 = nib.load(image_path_4)\n",
        "    label = nib.load(label_path)\n",
        "    \n",
        "    return image1,image2,image3,image4, label\n",
        "\n",
        "def save_to_json(data, path):\n",
        "  with open(path, 'w') as fp:\n",
        "    json.dump(data, fp)\n",
        "\n",
        "\n",
        "def remove_slice(img,  slice_size):\n",
        "    total_slize_size = img.shape[2]\n",
        "    extra_slices = total_slize_size - slice_size\n",
        "    end  = total_slize_size - (extra_slices // 2 )\n",
        "    start = end - slice_size \n",
        "    imgvol = np.array( img.dataobj )\n",
        "    imgvol = imgvol[ :, :, start:end ]\n",
        "    newimg = nib.Nifti1Image ( imgvol, img.affine )\n",
        "    return newimg\n",
        "\n",
        "def create_same_slice_nifti(data, slice_size ,dir):\n",
        "  paths = []\n",
        "  total = len(data)\n",
        "  count = 1\n",
        "  for entry in data:\n",
        "    image, prostate, pz , tz, label = load_nifti(entry[\"T2\"], entry[\"prostate\"], entry[\"PZ\"], entry[\"TZ\"], entry[\"label\"])\n",
        "\n",
        "    total_slize_size = min( image.shape[2], prostate.shape[2], pz.shape[2] , tz.shape[2], label.shape[2])\n",
        "    if(total_slize_size < slice_size): \n",
        "      print(\"ERROR: slice upper limit exceeds\")\n",
        "      continue\n",
        "    \n",
        "\n",
        "    new_image = remove_slice(image,slice_size)\n",
        "    new_prostate = remove_slice(prostate,slice_size)\n",
        "    new_pz = remove_slice(pz,slice_size)\n",
        "    new_tz = remove_slice(tz,slice_size)\n",
        "    new_lbl = remove_slice(label,slice_size)\n",
        "\n",
        "    image_path = dir + 'images/'+ entry['Name']+'.nii.gz'\n",
        "    label_path = dir + 'labels/' + entry['Name']+'.nii.gz'\n",
        "    prostate_path = dir + 'prostates/' + entry['Name']+'.nii.gz'\n",
        "    pz_path = dir + 'pz_masks/' + entry['Name']+'.nii.gz'\n",
        "    tz_path = dir + 'tz_masks/' + entry['Name']+'.nii.gz'\n",
        "    \n",
        "    paths.append({\n",
        "      \"image\":image_path, \n",
        "      'prostate':prostate_path,\n",
        "      'PZ': pz_path,\n",
        "      'TZ' : tz_path,\n",
        "      \"label\":label_path, \n",
        "    })\n",
        "    new_image.to_filename(image_path)\n",
        "    new_prostate.to_filename(prostate_path)\n",
        "    new_pz.to_filename(pz_path)\n",
        "    new_tz.to_filename(tz_path)\n",
        "    new_lbl.to_filename(label_path)\n",
        "    \n",
        "    print(f\"{count}/{total}\")\n",
        "    count += 1\n",
        "\n",
        "  save_to_json({\"path\": paths}, dir + 'config.json')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDlti3Nr0pRO"
      },
      "outputs": [],
      "source": [
        "slice_size = 16\n",
        "create_same_slice_nifti(data, slice_size ,SLICED_OUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diKStuuskvkS"
      },
      "source": [
        "# Gleason Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRfmbmVwSN7v"
      },
      "outputs": [],
      "source": [
        "def get_data_path(path):\n",
        "  f = open( path + 'config.json')\n",
        "  jdata = json.load(f)\n",
        "  f.close()\n",
        "  return jdata[\"path\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbzkY12W3C42"
      },
      "outputs": [],
      "source": [
        "data = get_data_path(SLICED_OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY1lS8ItlF15"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Prostate_Lesion/PROSTATEx_masks/Files/lesions/PROSTATEx_Classes.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88w9rvMClKCe"
      },
      "outputs": [],
      "source": [
        "df['Gleason Grade Group'] = df['Gleason Grade Group'].replace('No biopsy information', '0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM-dJSpulIAQ",
        "outputId": "e339dd57-e6f4-4aa1-c9ac-84461fae5f05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['3', '1', '2', '0', '4', '5'], dtype=object)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Gleason Grade Group'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJvSGhCgud1L",
        "outputId": "daa1f356-d135-4198-c666-eced60c02973"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    187\n",
              "2     41\n",
              "1     36\n",
              "3     20\n",
              "4      8\n",
              "5      7\n",
              "Name: Gleason Grade Group, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Gleason Grade Group'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPebMmiz7qe0",
        "outputId": "dfbec151-fdb5-44c2-8346-4469a77b970d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File name: ProstateX-0170-Finding2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the file path\n",
        "file_path = './data/sliced/images/ProstateX-0170-Finding2.nii.gz'\n",
        "\n",
        "def getFileName(file_path):\n",
        "\n",
        "  # Get the base filename without the extension\n",
        "  file_name = os.path.basename(file_path)\n",
        "  file_name = os.path.splitext(file_name)[0]\n",
        "\n",
        "  # Remove the '.nii' extension\n",
        "  file_name = file_name.replace('.nii', '')\n",
        "\n",
        "  return file_name\n",
        "\n",
        "# Display the filename\n",
        "print('File name:', getFileName(file_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEQehTyy9fz8",
        "outputId": "a623d20a-6594-4cf4-f3bf-4eb487f9cec4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID                        299\n",
              "Clinically Significant      2\n",
              "Gleason Grade Group         6\n",
              "dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt3uI7tLQrXk"
      },
      "outputs": [],
      "source": [
        "def save_to_json(data, path):\n",
        "  with open(path, 'w') as fp:\n",
        "    json.dump(data, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YOege-e3ycH"
      },
      "outputs": [],
      "source": [
        "updated_data = []\n",
        "df['ID'] = df['ID'].str.replace('_','-')\n",
        "\n",
        "\n",
        "for row in data:\n",
        "  name = getFileName(row['image'])\n",
        " \n",
        "  gleason = df[ df['ID'] == name]\n",
        "  row['name'] = name\n",
        "  row['Clinically Significant'] = str(gleason['Clinically Significant'].values[0])\n",
        "  row['Gleason Grade Group'] = gleason['Gleason Grade Group'].values[0]\n",
        "  updated_data.append(row)\n",
        " \n",
        "\n",
        "save_to_json({\"path\": updated_data}, SLICED_OUT_DIR + 'combined_config.json')   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqf2izgHRetB"
      },
      "source": [
        "# Augment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VwWvG8QUSQED"
      },
      "outputs": [],
      "source": [
        "def get_data_path(path):\n",
        "  f = open( path + 'combined_config.json')\n",
        "  jdata = json.load(f)\n",
        "  f.close()\n",
        "  return jdata[\"path\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V0-EQ4EGSBGN"
      },
      "outputs": [],
      "source": [
        "combine_data = data = get_data_path(SLICED_OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4g23I4q6Rhkx"
      },
      "outputs": [],
      "source": [
        "indexes = [[],[],[],[],[],[]]\n",
        "\n",
        "for i, data in enumerate(combine_data):  \n",
        "  indexes[int(data['Gleason Grade Group'])].append(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1htOVKmW-4C"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import random\n",
        "import elasticdeform \n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import time\n",
        "\n",
        "from scipy.ndimage import affine_transform, rotate, zoom\n",
        "from skimage.exposure import adjust_gamma, rescale_intensity\n",
        "from skimage.util import random_noise\n",
        "from scipy.ndimage import map_coordinates\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "\n",
        "def rotated(voxels, mask_1, mask_2, mask_3, lbl, theta = None):\n",
        "    # Rotate volume by a minor angle (+/- 10 degrees: determined by investigation of dataset variability)\n",
        "    if theta is None:\n",
        "        theta = random.randint(-10, 10)\n",
        "    vox_new = rotate(voxels, theta, reshape = False)\n",
        "    mask_1_new = rotate(mask_1, theta, reshape = False)\n",
        "    mask_2_new = rotate(mask_2, theta, reshape = False)\n",
        "    mask_3_new = rotate(mask_3, theta, reshape = False)\n",
        "    lbl_new =  rotate(lbl, theta, reshape = False)\n",
        "\n",
        "    return vox_new, mask_1_new, mask_2_new, mask_3_new, lbl\n",
        "\n",
        "def scale_and_crop(voxels, mask_1, mask_2, mask_3, lbl):\n",
        "    # Scale the volume by a minor size and crop around centre (can also modify for random crop)\n",
        "    o_s = voxels.shape\n",
        "    r_s = [0]*len(o_s)\n",
        "    scale_factor = random.uniform(1, 1.2)\n",
        "    vox_zoom = zoom(voxels, scale_factor, order=1)\n",
        "    mask_1_zoom = zoom(mask_1, scale_factor, order=0)\n",
        "    mask_2_zoom = zoom(mask_2, scale_factor, order=0)\n",
        "    mask_3_zoom = zoom(mask_3, scale_factor, order=0)\n",
        "    lbl_zoom = zoom(lbl, scale_factor, order=0)\n",
        "    new_shape = vox_zoom.shape\n",
        "    # Start with offset\n",
        "    for i in range(len(o_s)):\n",
        "        if new_shape[i] == 1: \n",
        "            r_s[i] = 0\n",
        "            continue\n",
        "        r_c = int(((new_shape[i] - o_s[i]) - 1)/2)\n",
        "        r_s[i] = r_c\n",
        "    r_e = [r_s[i] + o_s[i] for i in list(range(len(o_s)))]\n",
        "    vox_zoom = vox_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    mask_1_zoom = mask_1_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    mask_2_zoom = mask_2_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    mask_3_zoom = mask_3_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    lbl_zoom = lbl_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
        "    return vox_zoom, mask_1_zoom, mask_2_zoom, mask_3_zoom, lbl_zoom\n",
        "\n",
        "def grayscale_variation(voxels,  mask_1, mask_2, mask_3, lbl):\n",
        "    # Introduce a random global increment in gray-level value of volume. \n",
        "    im_min = np.min(voxels)\n",
        "    im_max = np.max(voxels)\n",
        "    mean = np.random.normal(0, 0.1)\n",
        "    smp = np.random.normal(mean, 0.01, size = np.shape(voxels))\n",
        "    voxels = voxels + im_max*smp\n",
        "    voxels[voxels <= im_min] = im_min # Clamp to min value\n",
        "    voxels[voxels > im_max] = im_max  # Clamp to max value\n",
        "    return voxels, mask_1, mask_2, mask_3, lbl\n",
        "\n",
        "\n",
        "def elastic_deformation(voxels, mask_1, mask_2, mask_3, lbl, alpha=None, sigma=None, mode=\"constant\", cval=0, is_random=False): \n",
        "    # Apply elastic deformation/distortion to the wolume\n",
        "    # Adapted from: https://tensorlayer.readthedocs.io/en/stable/_modules/tensorlayer/prepro.html#elastic_transform\n",
        "    if alpha == None:\n",
        "        alpha=voxels.shape[1]*3.\n",
        "    if sigma == None:\n",
        "        sigma=voxels.shape[1]*0.07\n",
        "    if is_random is False:\n",
        "        random_state = np.random.RandomState(None)\n",
        "    else:\n",
        "        random_state = np.random.RandomState(int(time.time()))\n",
        "        \n",
        "    if len(voxels.shape) == 3:\n",
        "        voxels = np.reshape(voxels, (voxels.shape[0], voxels.shape[1], voxels.shape[2], 1) )\n",
        "        mask_1 = np.reshape(mask_1, (mask_1.shape[0], mask_1.shape[1],mask_1.shape[2], 1) )\n",
        "        mask_2 = np.reshape(mask_2, (mask_2.shape[0], mask_2.shape[1],mask_2.shape[2], 1) )\n",
        "        mask_3 = np.reshape(mask_3, (mask_3.shape[0], mask_3.shape[1],mask_3.shape[2], 1) )\n",
        "        lbl = np.reshape( lbl, (lbl.shape[0], lbl.shape[1], lbl.shape[2], 1 ) )\n",
        "        \n",
        "    shape = (voxels.shape[0], voxels.shape[1])\n",
        "    label_shape = (lbl.shape[0], lbl.shape[1])\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n",
        "    x_, y_ = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
        "    indices = np.reshape(x_ + dx, (-1, 1)), np.reshape(y_ + dy, (-1, 1))\n",
        "    \n",
        "    new_voxels = np.zeros(voxels.shape)\n",
        "    new_mask_1 = np.zeros(mask_1.shape)\n",
        "    new_mask_2 = np.zeros(mask_2.shape)\n",
        "    new_mask_3 = np.zeros(mask_3.shape)\n",
        "    new_lbl = np.zeros( lbl.shape)\n",
        "    for i in range(voxels.shape[2]): # apply the same distortion to each slice within the volume\n",
        "        new_voxels[:,:,i,0] = map_coordinates(voxels[:,:,i,0], indices, order=1).reshape(shape)\n",
        "        new_mask_1[:,:,i,0] = map_coordinates(mask_1[:,:,i,0], indices, order=1).reshape(shape)\n",
        "        new_mask_2[:,:,i,0] = map_coordinates(mask_2[:,:,i,0], indices, order=1).reshape(shape)\n",
        "        new_mask_3[:,:,i,0] = map_coordinates(mask_3[:,:,i,0], indices, order=1).reshape(shape)\n",
        "        new_lbl[:,:,i,0] = map_coordinates(lbl[:,:,i,0], indices, order=1).reshape( label_shape)\n",
        "        \n",
        "    return new_voxels.squeeze(), new_mask_1.squeeze(), new_mask_2.squeeze(), new_mask_3.squeeze(), new_lbl.squeeze()\n",
        "\n",
        "\n",
        "    \n",
        "def translate(image, mask_1, mask_2, mask_3, lbl):\n",
        "\n",
        "    translate = (random.uniform(-10, 10), random.uniform(-10, 10))\n",
        "    matrix = np.array([[1, 0, 0, translate[0]], [0, 1, 0, translate[1]], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "\n",
        "    # Translate\n",
        "    img = affine_transform(image, matrix, order=1)\n",
        "    mask_1_new = affine_transform(mask_1, matrix, order=0)\n",
        "    mask_2_new = affine_transform(mask_2, matrix, order=0)\n",
        "    mask_3_new = affine_transform(mask_3, matrix, order=0)\n",
        "    lbl_new = affine_transform( lbl, matrix, order=0)\n",
        "\n",
        "    return img, mask_1_new, mask_2_new, mask_3_new, lbl_new \n",
        "\n",
        "def randomFlip(image, mask_1, mask_2, mask_3, lbl):\n",
        "    flip_axes = [ i for i in range(2) if i!=2 and np.random.choice([0, 1]) == 1]\n",
        "\n",
        "    # Randomly flip the image and label along one or more axes, except for the z-axis\n",
        "    img = np.flip(image, axis=flip_axes)\n",
        "    mask_1_new = np.flip(mask_1, axis=flip_axes)\n",
        "    mask_2_new = np.flip(mask_2, axis=flip_axes)\n",
        "    mask_3_new = np.flip(mask_3, axis=flip_axes)\n",
        "    lbl_new = np.flip( lbl, axis=flip_axes)\n",
        "\n",
        "    return img, mask_1_new, mask_2_new, mask_3_new, lbl_new\n",
        "\n",
        "\n",
        "\n",
        "def shear(image, mask_1, mask_2, mask_3, lbl):\n",
        "\n",
        "    shear = random.uniform(-0.2, 0.2)\n",
        "    matrix = np.array([[1, shear, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
        "\n",
        "    # Shear\n",
        "    img = affine_transform(image, matrix, order=1)\n",
        "    mask_1_new = affine_transform(mask_1, matrix, order=0)\n",
        "    mask_2_new = affine_transform(mask_2, matrix, order=0)\n",
        "    mask_3_new = affine_transform(mask_3, matrix, order=0)\n",
        "    lbl_new = affine_transform(lbl, matrix, order=0)\n",
        "\n",
        "    return img, mask_1_new, mask_2_new, mask_3_new, lbl_new\n",
        "\n",
        "def sample_with_p(p):\n",
        "    # Helper function to return boolean of a sample with given probability p\n",
        "    if random.random() < p:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def get_random_perturbation(voxels, mask1, mask2, mask3, label):\n",
        "    # Generate a random perturbation of the input feature + label\n",
        "    p_rotate = 1\n",
        "    p_scale = 0.6\n",
        "    p_gray = 0.1\n",
        "    p_deform = 0.6\n",
        "    p_translate = 0.8\n",
        "    p_shear = 0.2\n",
        "\n",
        "    voxel = np.array(voxels.dataobj)\n",
        "    mask_1 = np.array(mask1.dataobj)\n",
        "    mask_2 = np.array(mask2.dataobj)\n",
        "    mask_3 = np.array(mask3.dataobj)\n",
        "    lbl = np.array(label.dataobj)\n",
        "\n",
        "\n",
        "    new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = randomFlip(voxel, mask_1, mask_2, mask_3, lbl)\n",
        "   \n",
        "    new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = rotated(new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl )\n",
        "\n",
        "    if sample_with_p(p_scale):\n",
        "       new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = scale_and_crop( new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "    \n",
        "    if sample_with_p(p_gray):\n",
        "       new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = grayscale_variation(new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "    # if sample_with_p(p_deform):\n",
        "    #     new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = elastic_deformation( new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "    \n",
        "    if sample_with_p(p_shear):\n",
        "      new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = shear( new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "    \n",
        "    if sample_with_p(p_translate):\n",
        "      new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl = translate( new_voxels, new_mask_1, new_mask_2, new_mask_3, new_lbl)\n",
        "\n",
        "    new_voxels = nib.Nifti1Image(new_voxels, voxels.affine)\n",
        "    new_mask_1 = nib.Nifti1Image(new_mask_1, mask1.affine)\n",
        "    new_mask_2 = nib.Nifti1Image(new_mask_2, mask2.affine)\n",
        "    new_mask_3 = nib.Nifti1Image(new_mask_3, mask3.affine)\n",
        "    new_lbl = nib.Nifti1Image(new_lbl, label.affine)\n",
        "    \n",
        "    return new_voxels, new_mask_1,  new_mask_2,  new_mask_3, new_lbl\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou1Zs-t0LCDm"
      },
      "outputs": [],
      "source": [
        "def load_nifti(image, mask_prostate, mask_pz, mask_tz, lbl):\n",
        "    # load the image and label file, get the image content and return a numpy array for each\n",
        "    image = nib.load(image)\n",
        "    prostate = nib.load(mask_prostate)\n",
        "    pz = nib.load(mask_pz)\n",
        "    tz = nib.load(mask_tz)\n",
        "    label = nib.load(lbl)\n",
        "    \n",
        "    return image, prostate, pz, tz, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTqUnZZ9DuYO",
        "outputId": "ea17f537-b370-47c7-c996-5bac06bff071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': './data/sliced/images/ProstateX-0000-Finding1.nii.gz',\n",
              " 'prostate': './data/sliced/prostates/ProstateX-0000-Finding1.nii.gz',\n",
              " 'PZ': './data/sliced/pz_masks/ProstateX-0000-Finding1.nii.gz',\n",
              " 'TZ': './data/sliced/tz_masks/ProstateX-0000-Finding1.nii.gz',\n",
              " 'label': './data/sliced/labels/ProstateX-0000-Finding1.nii.gz',\n",
              " 'name': 'ProstateX-0000-Finding1',\n",
              " 'Clinically Significant': 'True',\n",
              " 'Gleason Grade Group': '3'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combine_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1kNEQSqupN_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def augment( entry):\n",
        "   \n",
        "    img, mask_prostate, mask_pz, mask_tz, lbl = load_nifti(entry[\"image\"],entry[\"prostate\"],entry[\"PZ\"],entry[\"TZ\"], entry[\"label\"])\n",
        "    \n",
        "    image, prostate, pz, tz, label = get_random_perturbation(img, mask_prostate, mask_pz, mask_tz, lbl)\n",
        "    name  =  entry['name'] + str(random.randint(1, 1000))\n",
        "    image_path = entry[\"image\"].replace(\"data/sliced/\", \"data/augmented/\" ).replace(entry['name'], name)\n",
        "    prostate_path = entry[\"prostate\"].replace(\"data/sliced/\", \"data/augmented/\" ).replace(entry['name'], name)\n",
        "    pz_path = entry[\"PZ\"].replace(\"data/sliced/\", \"data/augmented/\" ).replace(entry['name'], name)\n",
        "    tz_path = entry[\"TZ\"].replace(\"data/sliced/\", \"data/augmented/\" ).replace(entry['name'], name)\n",
        "    label_path = entry[\"label\"].replace(\"data/sliced/\", \"data/augmented/\").replace(entry['name'], name)\n",
        "\n",
        "    \n",
        "    image.to_filename(image_path )\n",
        "    prostate.to_filename(prostate_path)\n",
        "    pz.to_filename(pz_path )\n",
        "    tz.to_filename(tz_path ) \n",
        "    label.to_filename(label_path)\n",
        "\n",
        "    paths = {\n",
        "        \"image\":image_path, \n",
        "        'prostate': prostate_path,\n",
        "        'PZ': pz_path,\n",
        "        'TZ': tz_path,\n",
        "        \"label\":label_path,\n",
        "        'name': name,\n",
        "        'Clinically Significant': entry['Clinically Significant'],\n",
        "        'Gleason Grade Group' : entry['Gleason Grade Group']\n",
        "\n",
        "    }\n",
        "\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WzGdoeNSJkj",
        "outputId": "0008ef45-4c54-46bb-edd5-44096ad2847e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': './data/sliced/images/ProstateX-0003-Finding1.nii.gz',\n",
              " 'prostate': './data/sliced/prostates/ProstateX-0003-Finding1.nii.gz',\n",
              " 'PZ': './data/sliced/pz_masks/ProstateX-0003-Finding1.nii.gz',\n",
              " 'TZ': './data/sliced/tz_masks/ProstateX-0003-Finding1.nii.gz',\n",
              " 'label': './data/sliced/labels/ProstateX-0003-Finding1.nii.gz',\n",
              " 'name': 'ProstateX-0003-Finding1',\n",
              " 'Clinically Significant': 'False',\n",
              " 'Gleason Grade Group': '0'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indexes[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsRmCR3XfwSg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(AUG_OUT_DIR  ):\n",
        "    os.makedirs(AUG_OUT_DIR )\n",
        "if not os.path.exists(AUG_OUT_DIR + \"/images\"):\n",
        "    os.makedirs(AUG_OUT_DIR +  \"/images\")\n",
        "if not os.path.exists(AUG_OUT_DIR +  \"/prostates\"):\n",
        "    os.makedirs(AUG_OUT_DIR + \"/prostates\")\n",
        "if not os.path.exists(AUG_OUT_DIR +  \"/pz_masks\"):\n",
        "    os.makedirs(AUG_OUT_DIR + \"/pz_masks\")\n",
        "if not os.path.exists(AUG_OUT_DIR + \"/tz_masks\"):\n",
        "    os.makedirs(AUG_OUT_DIR +  \"/tz_masks\")\n",
        "if not os.path.exists(AUG_OUT_DIR + \"/labels\"):\n",
        "    os.makedirs(AUG_OUT_DIR +  \"/labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klv-5EpXPumT"
      },
      "outputs": [],
      "source": [
        "def save_to_json(data, path):\n",
        "  with open(path, 'w') as fp:\n",
        "    json.dump(data, fp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB877yhgUJST",
        "outputId": "6b0da36d-bb4d-4de8-d398-0285261d3277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "149\n",
            "50\n",
            "38\n",
            "15\n",
            "50\n",
            "100\n",
            "15\n",
            "30\n",
            "50\n",
            "150\n",
            "15\n",
            "45\n",
            "50\n",
            "200\n",
            "15\n",
            "60\n",
            "50\n",
            "250\n",
            "15\n",
            "75\n",
            "50\n",
            "300\n",
            "15\n",
            "90\n"
          ]
        }
      ],
      "source": [
        "train_percent = 0.8\n",
        "train_class_size = 50\n",
        "test_class_size = 15\n",
        "\n",
        "train_data = []\n",
        "test_data = []\n",
        "for data in indexes:\n",
        "  train_size = int(train_percent * len(data))\n",
        "  train_set = data[:train_size]\n",
        "  test_set = data[train_size:]\n",
        "\n",
        "  while train_class_size > len(train_set):\n",
        "    rand_num = random.randint(0, len(train_set)-1)\n",
        "    new_data = augment(train_set[rand_num])\n",
        "    train_set.append(new_data)\n",
        "  train_data.extend(train_set[:50]) #train_data.extend(train_set)\n",
        "  print(len(train_set))\n",
        "  print(len(train_data))\n",
        "\n",
        "  while test_class_size > len(test_set):\n",
        "    rand_num = random.randint(0, len(test_set)-1)\n",
        "    new_data = augment(test_set[rand_num])\n",
        "    test_set.append(new_data)\n",
        "\n",
        "  test_data.extend(test_set[:15]) #test_data.extend(test_set)\n",
        "  print(len(test_set))\n",
        "  print(len(test_data))\n",
        "\n",
        "\n",
        "save_to_json({\"path\": train_data}, AUG_OUT_DIR + 'train_config.json') \n",
        "save_to_json({\"path\": test_data}, AUG_OUT_DIR + 'test_config.json')   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3A1UwZUywsS"
      },
      "source": [
        "# Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V-U6kFUWywsS"
      },
      "outputs": [],
      "source": [
        "def get_data_path(path):\n",
        "  f = open( path )\n",
        "  jdata = json.load(f)\n",
        "  f.close()\n",
        "  return jdata[\"path\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0quaw1JsO79R"
      },
      "outputs": [],
      "source": [
        "train_data =  get_data_path(AUG_OUT_DIR + 'train_config.json')\n",
        "test_data = get_data_path(AUG_OUT_DIR + 'test_config.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDqPvFtLywsU",
        "outputId": "475befdd-b4cd-46a1-93dd-8b905a86a95d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300 90\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data),len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MY-TzlbmywsU"
      },
      "outputs": [],
      "source": [
        "pixdim =(1.5, 1.5, 1.0)\n",
        "a_min=0\n",
        "a_max=500\n",
        "spatial_size= [128, 128,16] #[384, 384,18]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6mC0MOUMywsU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    AddChanneld,\n",
        "    LoadImaged,\n",
        "    Resized,\n",
        "    ToTensord,\n",
        "    Spacingd,\n",
        "    Orientationd,\n",
        "    ScaleIntensityRanged,\n",
        "    CropForegroundd,\n",
        "    SqueezeDimd\n",
        ")\n",
        "from monai.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class ConcatImagesd:\n",
        "    def __init__(self, keys):\n",
        "        self.keys = keys\n",
        "\n",
        "    def __call__(self, data):\n",
        "        concat = np.concatenate([np.expand_dims(data[key], axis=0) for key in self.keys], axis=1)\n",
        "        data[self.keys[0]] = concat\n",
        "        for key in self.keys[1:]:\n",
        "            del data[key]\n",
        "        return data\n",
        "\n",
        "\n",
        "def transform(data, a_min, a_max, spatial_size, pixdim):\n",
        "    train_transforms = Compose([\n",
        "        LoadImaged(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\", \"label\"], reader=\"ITKReader\"),\n",
        "        AddChanneld(keys=[\"label\"]),\n",
        "        AddChanneld(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\"]),\n",
        "        Orientationd(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\", \"label\"], axcodes=\"RAS\"),\n",
        "        ScaleIntensityRanged(keys=[\"image\"], a_min=a_min, a_max=a_max, b_min=0.0, b_max=1.0, clip=True),\n",
        "        CropForegroundd(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\", \"label\"], source_key=\"image\"),\n",
        "        Resized(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\", \"label\"], spatial_size=spatial_size),\n",
        "        ConcatImagesd(keys=[\"image\", \"PZ\", \"TZ\", \"prostate\"]),\n",
        "        ToTensord(keys=[\"image\", \"label\"]),\n",
        "        SqueezeDimd(keys=[\"image\"]),\n",
        "    ])\n",
        "\n",
        "    ds = Dataset(data=data, transform=train_transforms)\n",
        "    loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C113p2eCywsV",
        "outputId": "9055f9a9-4825-423a-a3ad-dc29640b938b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/monai/transforms/io/array.py:199: UserWarning: required package for reader ITKReader is not installed, or the version doesn't match requirement.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
            "  warn_deprecated(obj, msg, warning_category)\n"
          ]
        }
      ],
      "source": [
        "#from utils.transform import transform\n",
        "\n",
        "train_loader = transform(train_data, a_min, a_max, spatial_size, pixdim)\n",
        "test_loader = transform(test_data, a_min, a_max, spatial_size, pixdim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "62jsAR9iywsX"
      },
      "outputs": [],
      "source": [
        "model_dir = OUT_DIR \n",
        "data_in = [train_loader, test_loader]\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD3y2qam_2Ig"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cOx1e04Z7FdF"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "def load_metrices(path):\n",
        "  metrices_dir = path\n",
        "  train_loss = train_metric = test_loss = test_metric =np.array([])\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'loss_train.npy')):\n",
        "    train_loss = np.load(os.path.join(metrices_dir, 'loss_train.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'metric_train.npy')):\n",
        "    train_metric = np.load(os.path.join(metrices_dir, 'metric_train.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'loss_test.npy')):\n",
        "    test_loss = np.load(os.path.join(metrices_dir, 'loss_test.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'metric_test.npy')):\n",
        "    test_metric = np.load(os.path.join(metrices_dir, 'metric_test.npy'))\n",
        "  return train_loss, train_metric, test_loss, test_metric\n",
        "\n",
        "def dice_metric(predicted, target):\n",
        "    '''\n",
        "    In this function we take `predicted` and `target` (label) to calculate the dice coeficient then we use it \n",
        "    to calculate a metric value for the training and the validation.\n",
        "    '''\n",
        "    dice_value = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
        "    value = 1 - dice_value(predicted, target).item()\n",
        "    return value\n",
        "\n",
        "\n",
        "def get_time():\n",
        "  utc_time= datetime.datetime.now(pytz.utc)\n",
        "  local_time = utc_time.astimezone(pytz.timezone('Asia/Colombo'))\n",
        "  return local_time.strftime(\"%Y:%m:%d %H:%M:%S\")\n",
        "\n",
        "def update_history(data,model_dir):\n",
        "  history_file_path = model_dir + \"history.csv\"\n",
        "  if not os.path.exists(history_file_path):\n",
        "    with open(history_file_path,'a') as fd:\n",
        "        fd.write(\",\".join([\"Start\", \"End\", \"Best Matrix\", \"Best M. At\", \"Time\"]))\n",
        "  with open(history_file_path,'a') as fd:\n",
        "      str_data=[str(x) for x in (data + [get_time()])]\n",
        "      fd.write(\"\\n\" + \",\".join(str_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pn5a7gZ5JYef"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "\n",
        "# Train the model\n",
        "def updateLogs(path, data):\n",
        "    f = open(path,'a')\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "\n",
        "def train(model, loss, optimizer,  model_dir, num_epochs = 100, start_from=1, test_interval = 5):\n",
        "  best_metric = -1\n",
        "  best_metric_epoch = -1\n",
        "\n",
        "  save_loss_train = []\n",
        "  save_loss_test = []\n",
        "  save_metric_train = []\n",
        "  save_metric_test = []\n",
        "  if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "  if (start_from != 1):\n",
        "    save_loss_train, save_metric_train, save_loss_test, save_metric_test= [x.tolist() for x in load_metrices(model_dir)]\n",
        "\n",
        "    path = model_dir + \"best_metric_model.pth\"\n",
        "\n",
        "    if (os.path.exists(path)):\n",
        "        model.load_state_dict(torch.load(\n",
        "            os.path.join(path)))\n",
        "        print('Model Loaded')\n",
        "    if(len(save_metric_test)):\n",
        "          best_metric = torch.tensor(max(save_metric_test)).to(device)\n",
        "    best_metric_epoch = -2\n",
        "  train_loader, test_loader = data_in\n",
        "\n",
        "\n",
        "  for epoch in range(start_from-1,num_epochs):\n",
        "\n",
        "      print(\"-\" * 10)\n",
        "      print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
        "      model.train()\n",
        "      train_epoch_loss = 0\n",
        "      train_step = 0\n",
        "      epoch_metric_train = 0\n",
        "\n",
        "      for batch_id, batch_data in enumerate(train_loader):\n",
        "          \n",
        "          train_step += 1\n",
        "          \n",
        "          volume = batch_data[\"image\"]\n",
        "          label = batch_data[\"label\"]\n",
        "\n",
        "          if torch.cuda.is_available():\n",
        "            volume, label = (volume.to(device), label.to(device))\n",
        "\n",
        "          \n",
        "          # Zero the gradients\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          # Forward pass\n",
        "          bb_output = model(volume)\n",
        "          \n",
        "          #Compute loss for segmentation mask\n",
        "          # mask_loss = loss_function(label, mask_output)\n",
        "          \n",
        "          bb_target = [0,0,0,0,0,0]\n",
        "          \n",
        "          value =  int(batch_data['Gleason Grade Group'][0])\n",
        "          bb_target[value] = 1\n",
        "          \n",
        "        \n",
        "          bb_target = torch.tensor(bb_target).view(1, -1).to(device)\n",
        "          \n",
        "          \n",
        "          train_loss = loss(bb_output, bb_target.float())\n",
        "          # Backward pass\n",
        "          train_loss.backward()\n",
        "          \n",
        "          # Update weights\n",
        "          optimizer.step()\n",
        "          \n",
        "        # Print training progress\n",
        "          print('Epoch [{}/{}], Batch [{}/{}],  Cross Loss: {:.4f}'\n",
        "                .format(epoch+1, num_epochs, batch_id+1, len(train_loader),  train_loss.item()))\n",
        "          updateLogs(os.path.join(model_dir, \"logs.txt\"), f'Epoch [{epoch+1}/{num_epochs}], Batch [{ batch_id+1}/{ len(train_loader)}], Log Loss:  {train_loss.item():.4f}\\n')\n",
        "          updateLogs(os.path.join(model_dir, \"logs.txt\"), f'True {bb_target.cpu().detach().numpy()}\\n Predicted {bb_output.cpu().detach().numpy()} \\n')\n",
        "\n",
        "\n",
        "          train_epoch_loss += train_loss.item()\n",
        "          print(\n",
        "                  f\"{train_step}/{len(train_loader) // train_loader.batch_size}, \"\n",
        "                  f\"Train_loss: {train_loss.item():.4f}\", end=\" \")\n",
        "\n",
        "          train_metric = loss(bb_output, bb_target.float())\n",
        "          epoch_metric_train += train_metric\n",
        "          print(f'Train_dice: {train_metric.cpu().detach().numpy():.4f}')\n",
        "\n",
        "      print('-'*20)\n",
        "          \n",
        "      train_epoch_loss /= train_step\n",
        "      print(f'Epoch_loss: {train_epoch_loss:.4f}')\n",
        "      save_loss_train.append(train_epoch_loss)\n",
        "      np.save(os.path.join(model_dir, 'loss_train.npy'), save_loss_train)\n",
        "          \n",
        "      epoch_metric_train /= train_step\n",
        "      print(f'Epoch_metric: {epoch_metric_train}')\n",
        "\n",
        "      save_metric_train.append(epoch_metric_train.cpu().detach().numpy())\n",
        "      np.save(os.path.join(model_dir, 'metric_train.npy'), save_metric_train)\n",
        "\n",
        "      torch.save(model.state_dict(), os.path.join(\n",
        "              model_dir, \"current_metric_model.pth\"))\n",
        "          \n",
        "      updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"{'-'*20}{epoch+1} \\nEpoch_loss: {train_epoch_loss:.4f}\\nEpoch_metric: {epoch_metric_train}\\n\")\n",
        "\n",
        "      if (epoch + 1) % test_interval == 0:\n",
        "\n",
        "          model.eval()\n",
        "\n",
        "          with torch.no_grad():\n",
        "            test_epoch_loss = 0\n",
        "            test_metric = 0\n",
        "            epoch_metric_test = 0\n",
        "            test_step = 0\n",
        "            total_loss = 0\n",
        "            for batch_id, batch_data in enumerate(test_loader):\n",
        "                test_step += 1\n",
        "\n",
        "                volume = batch_data[\"image\"]\n",
        "                label = batch_data[\"label\"]\n",
        "                if torch.cuda.is_available():\n",
        "                    volume, label = (volume.to(device), label.to(device))\n",
        "                \n",
        "                # Forward pass\n",
        "              \n",
        "                #mask_output, \n",
        "                bb_output = model(volume)\n",
        "                \n",
        "                bb_target = [0,0,0,0,0,0]\n",
        "          \n",
        "                value =  int(batch_data['Gleason Grade Group'][0])\n",
        "                bb_target[value] = 1\n",
        "              \n",
        "            \n",
        "                bb_target = torch.tensor(bb_target).view(1, -1).to(device)\n",
        "                    \n",
        "                # Compute loss for bounding box classifier\n",
        "\n",
        "                test_loss = loss( bb_output, bb_target.float())\n",
        "                \n",
        "                test_epoch_loss += test_loss.item()\n",
        "                test_metric = loss( bb_output, bb_target.float())\n",
        "                \n",
        "                epoch_metric_test += test_metric\n",
        "            \n",
        "            # Calculate the average loss across all batches in the test_loader\n",
        "            test_epoch_loss /= test_step\n",
        "            print(f'test_loss_epoch: {test_epoch_loss:.4f}')\n",
        "            save_loss_test.append(test_epoch_loss)\n",
        "            np.save(os.path.join(model_dir, 'loss_test.npy'), save_loss_test)\n",
        "\n",
        "            epoch_metric_test /= test_step\n",
        "            print(f'test_dice_epoch: {epoch_metric_test}')\n",
        "            save_metric_test.append(epoch_metric_test.cpu().detach().numpy())\n",
        "            np.save(os.path.join(model_dir, 'metric_test.npy'), save_metric_test)\n",
        "\n",
        "\n",
        "            if epoch_metric_test > best_metric:\n",
        "                best_metric = epoch_metric_test\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), os.path.join(\n",
        "                model_dir, \"best_metric_model.pth\"))\n",
        "                  \n",
        "            print(\n",
        "                      f\"current epoch: {epoch + 1} current mean dice: {epoch_metric_test}\"\n",
        "                      f\"\\nbest mean dice: {best_metric} \"\n",
        "                      \n",
        "                      f\"at epoch: {best_metric_epoch}\"\n",
        "                  )\n",
        "            \n",
        "            \n",
        "            updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"{'-'*30}\\ncurrent epoch: {epoch + 1} \\n\"\n",
        "                      f'test_dice_epoch: {epoch_metric_test}\\n'\n",
        "                      f'test_loss_epoch: {test_epoch_loss:.4f}\\n'\n",
        "                      f\"best mean dice: {best_metric} \"\n",
        "                      f\"at epoch: {best_metric_epoch}\\n\")\n",
        "            \n",
        "          updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"train completed, best_metric: {best_metric}\"\n",
        "          f\"at epoch: {best_metric_epoch}\\n\")\n",
        "          \n",
        "          update_history([start_from, num_epochs, best_metric.cpu().detach().numpy(), best_metric_epoch],model_dir=model_dir)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA0eNCpVywsW"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8MtbPv2_ofG"
      },
      "source": [
        "## MNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqgOBs_pRriF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BB_classifier(nn.Module):\n",
        "    def __init__(self, n_input_channels=256, n_features=64, n_output_channels=6, anchor_stride=2, dim=3):\n",
        "        super(BB_classifier, self).__init__()\n",
        "        self.n_classes = 6\n",
        "        self.dim = dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv3d(n_input_channels, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_output_channels, kernel_size=3, stride=anchor_stride, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "\n",
        "        # Rearrange dimensions based on self.dim\n",
        "        if self.dim == 2:\n",
        "            x = x.permute(0, 2, 3, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "        else:\n",
        "            x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "\n",
        "        # Apply softmax activation\n",
        "        x = nn.functional.softmax(x[0], dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCwhAKSLydMr"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.conv import Conv3d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout3d(p=0.5),\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout3d(p=0.5)\n",
        "        )  \n",
        "\n",
        "class MNetWithBBClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super().__init__()\n",
        "                \n",
        "        self.dconv_down1 = double_conv(4, 64)\n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        self.dconv_down4 = double_conv(256, 512)        \n",
        "\n",
        "        self.maxpool = nn.MaxPool3d((2,2,2), stride=(2,2,2))\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)        \n",
        "        \n",
        "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
        "        \n",
        "        self.conv_last = nn.Conv3d(64,num_classes, 1)\n",
        "\n",
        "        self.e1_e3 = nn.Conv3d(64, 256, kernel_size=3, stride=(2,2,2), padding=1)\n",
        "        self.conv_mid2 = nn.Conv3d(512, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.e2_e4 = nn.Conv3d(128, 512, kernel_size=3, stride=(2,2,2), padding=1)\n",
        "        self.conv_mid1 = nn.Conv3d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.bb_classifier = BB_classifier()\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        \n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        \n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3) \n",
        "        \n",
        "        x = self.dconv_down4(x)\n",
        "\n",
        "        mid = self.e2_e4(conv2)\n",
        "        mid = self.maxpool(mid)\n",
        "     \n",
        "        x = torch.cat([mid,x], dim=1)\n",
        "        x = self.conv_mid1(x)\n",
        "\n",
        "\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        mid = self.e1_e3(conv1)\n",
        "        mid = self.maxpool(mid)\n",
        "        mid = torch.cat([conv3,mid], dim=1)\n",
        "        mid = self.conv_mid2(mid)\n",
        "\n",
        "        x = torch.cat([x, mid], dim=1)\n",
        "        \n",
        "        x = self.dconv_up3(x)\n",
        "        bb_output = self.bb_classifier(x)\n",
        "        # x = self.upsample(x)        \n",
        "        # x = torch.cat([x, conv2], dim=1)       \n",
        "\n",
        "        # x = self.dconv_up2(x)\n",
        "        # x = self.upsample(x)   \n",
        "        # x = torch.cat([x, conv1], dim=1)   \n",
        "        \n",
        "        # x = self.dconv_up1(x)\n",
        "        \n",
        "        # out = self.conv_last(x)\n",
        "        \n",
        "        return bb_output\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBSu46-vNp1r"
      },
      "outputs": [],
      "source": [
        "model_dir = OUT_DIR + \"Smooth_MNet_50/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q64g3RxKNrlG"
      },
      "outputs": [],
      "source": [
        "in_channels = 4  \n",
        "out_channels = 1 \n",
        "num_classes = 6  \n",
        "\n",
        "model =  MNetWithBBClassifier().to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001 ) #,weight_decay=1e-5, amsgrad=True)  # Update with the appropriate learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07Oqk4eQO3Hs"
      },
      "outputs": [],
      "source": [
        "from monai.losses import FocalLoss\n",
        "\n",
        "# Define the Focal Loss function\n",
        "#loss = FocalLoss()\n",
        "#loss = nn.BCELoss()\n",
        "loss = nn.SmoothL1Loss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq2gMx30Pxxq"
      },
      "outputs": [],
      "source": [
        "train(model, loss, optimizer,  model_dir, num_epochs = 100, start_from=1, test_interval = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-z7jx_d_uiD"
      },
      "source": [
        "## UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "abhISL-9nK_f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout3d(p=0.5),\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout3d(p=0.5)\n",
        "        )  \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BB_classifier(nn.Module):\n",
        "    def __init__(self, n_input_channels=256, n_features=64, n_output_channels=6, anchor_stride=2, dim=3):\n",
        "        super(BB_classifier, self).__init__()\n",
        "        self.n_classes = 6\n",
        "        self.dim = dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv3d(n_input_channels, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout3d(p=0.25),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout3d(p=0.25),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout3d(p=0.25)\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.BatchNorm3d(n_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout3d(p=0.25)\n",
        "            nn.Conv3d(n_features, n_output_channels, kernel_size=3, stride=anchor_stride, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "\n",
        "        # Rearrange dimensions based on self.dim\n",
        "        if self.dim == 2:\n",
        "            x = x.permute(0, 2, 3, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "        else:\n",
        "            x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "\n",
        "        # Apply softmax activation\n",
        "        x = nn.functional.softmax(x[0], dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class UNetWithBBClassifier(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dconv_down1 = double_conv(4, 64)\n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        self.dconv_down4 = double_conv(256, 512)\n",
        "\n",
        "        self.maxpool = nn.MaxPool3d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
        "\n",
        "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
        "\n",
        "        self.conv_last = nn.Conv3d(64, 1, 1)\n",
        "\n",
        "        self.bb_classifier = BB_classifier()\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "\n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "\n",
        "        x = self.dconv_down4(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "\n",
        "        x = self.dconv_up3(x)\n",
        "        bb_output = self.bb_classifier(x)\n",
        "        # x = self.upsample(x)\n",
        "        # x = torch.cat([x, conv2], dim=1)\n",
        "       \n",
        "        # #bb_output = self.bb_classifier(x.view(x.size(0), -1))\n",
        "\n",
        "        # x = self.dconv_up2(x)\n",
        "        # x = self.upsample(x)\n",
        "        # x = torch.cat([x, conv1], dim=1)\n",
        "\n",
        "        # x = self.dconv_up1(x)\n",
        "\n",
        "        # out = self.conv_last(x)\n",
        "\n",
        "        return  bb_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZAjEiPoGr4Yk"
      },
      "outputs": [],
      "source": [
        "# Initialize the U-Net model with bounding box classifier and move to GPU if available\n",
        "in_channels = 4  # Update with the appropriate number of input channels for your data\n",
        "out_channels = 1  # Update with the appropriate number of output channels for your data\n",
        "num_classes = 6  # Update with the appropriate number of Gleason score classes for your data\n",
        "model =  UNetWithBBClassifier(in_channels,out_channels,num_classes ).to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5, amsgrad=True)  # Update with the appropriate learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FJ9GNVsn60iU"
      },
      "outputs": [],
      "source": [
        "from monai.losses import FocalLoss\n",
        "\n",
        "# Define the Focal Loss function\n",
        "#loss = FocalLoss()\n",
        "#loss = nn.BCELoss()\n",
        "loss = nn.SmoothL1Loss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IICXmEXbShEm"
      },
      "outputs": [],
      "source": [
        "model_dir = OUT_DIR + \"Smooth_UNet_50_S_50/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jvmNsiASnpE",
        "outputId": "7d894f42-22ed-413c-e6ee-edb128631aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "epoch 1/100\n",
            "Epoch [1/100], Batch [1/300],  Cross Loss: 0.4132\n",
            "1/300, Train_loss: 0.4132 Train_dice: 0.4132\n",
            "Epoch [1/100], Batch [2/300],  Cross Loss: 0.4387\n",
            "2/300, Train_loss: 0.4387 Train_dice: 0.4387\n",
            "Epoch [1/100], Batch [3/300],  Cross Loss: 0.4544\n",
            "3/300, Train_loss: 0.4544 Train_dice: 0.4544\n",
            "Epoch [1/100], Batch [4/300],  Cross Loss: 0.4237\n",
            "4/300, Train_loss: 0.4237 Train_dice: 0.4237\n",
            "Epoch [1/100], Batch [5/300],  Cross Loss: 0.4029\n",
            "5/300, Train_loss: 0.4029 Train_dice: 0.4029\n",
            "Epoch [1/100], Batch [6/300],  Cross Loss: 0.3995\n",
            "6/300, Train_loss: 0.3995 Train_dice: 0.3995\n",
            "Epoch [1/100], Batch [7/300],  Cross Loss: 0.3793\n",
            "7/300, Train_loss: 0.3793 Train_dice: 0.3793\n",
            "Epoch [1/100], Batch [8/300],  Cross Loss: 0.3666\n",
            "8/300, Train_loss: 0.3666 Train_dice: 0.3666\n",
            "Epoch [1/100], Batch [9/300],  Cross Loss: 0.4482\n",
            "9/300, Train_loss: 0.4482 Train_dice: 0.4482\n",
            "Epoch [1/100], Batch [10/300],  Cross Loss: 0.3297\n",
            "10/300, Train_loss: 0.3297 Train_dice: 0.3297\n",
            "Epoch [1/100], Batch [11/300],  Cross Loss: 0.2936\n",
            "11/300, Train_loss: 0.2936 Train_dice: 0.2936\n",
            "Epoch [1/100], Batch [12/300],  Cross Loss: 0.3828\n",
            "12/300, Train_loss: 0.3828 Train_dice: 0.3828\n",
            "Epoch [1/100], Batch [13/300],  Cross Loss: 0.4750\n",
            "13/300, Train_loss: 0.4750 Train_dice: 0.4750\n",
            "Epoch [1/100], Batch [14/300],  Cross Loss: 0.2999\n",
            "14/300, Train_loss: 0.2999 Train_dice: 0.2999\n",
            "Epoch [1/100], Batch [15/300],  Cross Loss: 0.4479\n",
            "15/300, Train_loss: 0.4479 Train_dice: 0.4479\n",
            "Epoch [1/100], Batch [16/300],  Cross Loss: 0.4266\n",
            "16/300, Train_loss: 0.4266 Train_dice: 0.4266\n",
            "Epoch [1/100], Batch [17/300],  Cross Loss: 0.4670\n",
            "17/300, Train_loss: 0.4670 Train_dice: 0.4670\n",
            "Epoch [1/100], Batch [18/300],  Cross Loss: 0.4756\n",
            "18/300, Train_loss: 0.4756 Train_dice: 0.4756\n",
            "Epoch [1/100], Batch [19/300],  Cross Loss: 0.4692\n",
            "19/300, Train_loss: 0.4692 Train_dice: 0.4692\n",
            "Epoch [1/100], Batch [20/300],  Cross Loss: 0.3961\n",
            "20/300, Train_loss: 0.3961 Train_dice: 0.3961\n",
            "Epoch [1/100], Batch [21/300],  Cross Loss: 0.3597\n",
            "21/300, Train_loss: 0.3597 Train_dice: 0.3597\n",
            "Epoch [1/100], Batch [22/300],  Cross Loss: 0.4898\n",
            "22/300, Train_loss: 0.4898 Train_dice: 0.4898\n",
            "Epoch [1/100], Batch [23/300],  Cross Loss: 0.3516\n",
            "23/300, Train_loss: 0.3516 Train_dice: 0.3516\n",
            "Epoch [1/100], Batch [24/300],  Cross Loss: 0.4437\n",
            "24/300, Train_loss: 0.4437 Train_dice: 0.4437\n",
            "Epoch [1/100], Batch [25/300],  Cross Loss: 0.4558\n",
            "25/300, Train_loss: 0.4558 Train_dice: 0.4558\n",
            "Epoch [1/100], Batch [26/300],  Cross Loss: 0.4560\n",
            "26/300, Train_loss: 0.4560 Train_dice: 0.4560\n",
            "Epoch [1/100], Batch [27/300],  Cross Loss: 0.3845\n",
            "27/300, Train_loss: 0.3845 Train_dice: 0.3845\n",
            "Epoch [1/100], Batch [28/300],  Cross Loss: 0.4271\n",
            "28/300, Train_loss: 0.4271 Train_dice: 0.4271\n",
            "Epoch [1/100], Batch [29/300],  Cross Loss: 0.3517\n",
            "29/300, Train_loss: 0.3517 Train_dice: 0.3517\n",
            "Epoch [1/100], Batch [30/300],  Cross Loss: 0.3990\n",
            "30/300, Train_loss: 0.3990 Train_dice: 0.3990\n",
            "Epoch [1/100], Batch [31/300],  Cross Loss: 0.4634\n",
            "31/300, Train_loss: 0.4634 Train_dice: 0.4634\n",
            "Epoch [1/100], Batch [32/300],  Cross Loss: 0.4587\n",
            "32/300, Train_loss: 0.4587 Train_dice: 0.4587\n",
            "Epoch [1/100], Batch [33/300],  Cross Loss: 0.3744\n",
            "33/300, Train_loss: 0.3744 Train_dice: 0.3744\n",
            "Epoch [1/100], Batch [34/300],  Cross Loss: 0.4922\n",
            "34/300, Train_loss: 0.4922 Train_dice: 0.4922\n",
            "Epoch [1/100], Batch [35/300],  Cross Loss: 0.4840\n",
            "35/300, Train_loss: 0.4840 Train_dice: 0.4840\n",
            "Epoch [1/100], Batch [36/300],  Cross Loss: 0.4470\n",
            "36/300, Train_loss: 0.4470 Train_dice: 0.4470\n",
            "Epoch [1/100], Batch [37/300],  Cross Loss: 0.4882\n",
            "37/300, Train_loss: 0.4882 Train_dice: 0.4882\n",
            "Epoch [1/100], Batch [38/300],  Cross Loss: 0.4520\n",
            "38/300, Train_loss: 0.4520 Train_dice: 0.4520\n",
            "Epoch [1/100], Batch [39/300],  Cross Loss: 0.3902\n",
            "39/300, Train_loss: 0.3902 Train_dice: 0.3902\n",
            "Epoch [1/100], Batch [40/300],  Cross Loss: 0.3801\n",
            "40/300, Train_loss: 0.3801 Train_dice: 0.3801\n",
            "Epoch [1/100], Batch [41/300],  Cross Loss: 0.4074\n",
            "41/300, Train_loss: 0.4074 Train_dice: 0.4074\n",
            "Epoch [1/100], Batch [42/300],  Cross Loss: 0.3885\n",
            "42/300, Train_loss: 0.3885 Train_dice: 0.3885\n",
            "Epoch [1/100], Batch [43/300],  Cross Loss: 0.4226\n",
            "43/300, Train_loss: 0.4226 Train_dice: 0.4226\n",
            "Epoch [1/100], Batch [44/300],  Cross Loss: 0.4712\n",
            "44/300, Train_loss: 0.4712 Train_dice: 0.4712\n",
            "Epoch [1/100], Batch [45/300],  Cross Loss: 0.3695\n",
            "45/300, Train_loss: 0.3695 Train_dice: 0.3695\n",
            "Epoch [1/100], Batch [46/300],  Cross Loss: 0.4134\n",
            "46/300, Train_loss: 0.4134 Train_dice: 0.4134\n",
            "Epoch [1/100], Batch [47/300],  Cross Loss: 0.3555\n",
            "47/300, Train_loss: 0.3555 Train_dice: 0.3555\n",
            "Epoch [1/100], Batch [48/300],  Cross Loss: 0.4713\n",
            "48/300, Train_loss: 0.4713 Train_dice: 0.4713\n",
            "Epoch [1/100], Batch [49/300],  Cross Loss: 0.4235\n",
            "49/300, Train_loss: 0.4235 Train_dice: 0.4235\n",
            "Epoch [1/100], Batch [50/300],  Cross Loss: 0.4188\n",
            "50/300, Train_loss: 0.4188 Train_dice: 0.4188\n",
            "Epoch [1/100], Batch [51/300],  Cross Loss: 0.3907\n",
            "51/300, Train_loss: 0.3907 Train_dice: 0.3907\n",
            "Epoch [1/100], Batch [52/300],  Cross Loss: 0.4577\n",
            "52/300, Train_loss: 0.4577 Train_dice: 0.4577\n",
            "Epoch [1/100], Batch [53/300],  Cross Loss: 0.4506\n",
            "53/300, Train_loss: 0.4506 Train_dice: 0.4506\n",
            "Epoch [1/100], Batch [54/300],  Cross Loss: 0.4092\n",
            "54/300, Train_loss: 0.4092 Train_dice: 0.4092\n",
            "Epoch [1/100], Batch [55/300],  Cross Loss: 0.3804\n",
            "55/300, Train_loss: 0.3804 Train_dice: 0.3804\n",
            "Epoch [1/100], Batch [56/300],  Cross Loss: 0.3299\n",
            "56/300, Train_loss: 0.3299 Train_dice: 0.3299\n",
            "Epoch [1/100], Batch [57/300],  Cross Loss: 0.4143\n",
            "57/300, Train_loss: 0.4143 Train_dice: 0.4143\n",
            "Epoch [1/100], Batch [58/300],  Cross Loss: 0.3410\n",
            "58/300, Train_loss: 0.3410 Train_dice: 0.3410\n",
            "Epoch [1/100], Batch [59/300],  Cross Loss: 0.4258\n",
            "59/300, Train_loss: 0.4258 Train_dice: 0.4258\n",
            "Epoch [1/100], Batch [60/300],  Cross Loss: 0.4793\n",
            "60/300, Train_loss: 0.4793 Train_dice: 0.4793\n",
            "Epoch [1/100], Batch [61/300],  Cross Loss: 0.3190\n",
            "61/300, Train_loss: 0.3190 Train_dice: 0.3190\n",
            "Epoch [1/100], Batch [62/300],  Cross Loss: 0.3479\n",
            "62/300, Train_loss: 0.3479 Train_dice: 0.3479\n",
            "Epoch [1/100], Batch [63/300],  Cross Loss: 0.3634\n",
            "63/300, Train_loss: 0.3634 Train_dice: 0.3634\n",
            "Epoch [1/100], Batch [64/300],  Cross Loss: 0.4674\n",
            "64/300, Train_loss: 0.4674 Train_dice: 0.4674\n",
            "Epoch [1/100], Batch [65/300],  Cross Loss: 0.4302\n",
            "65/300, Train_loss: 0.4302 Train_dice: 0.4302\n",
            "Epoch [1/100], Batch [66/300],  Cross Loss: 0.3844\n",
            "66/300, Train_loss: 0.3844 Train_dice: 0.3844\n",
            "Epoch [1/100], Batch [67/300],  Cross Loss: 0.4687\n",
            "67/300, Train_loss: 0.4687 Train_dice: 0.4687\n",
            "Epoch [1/100], Batch [68/300],  Cross Loss: 0.3327\n",
            "68/300, Train_loss: 0.3327 Train_dice: 0.3327\n",
            "Epoch [1/100], Batch [69/300],  Cross Loss: 0.3986\n",
            "69/300, Train_loss: 0.3986 Train_dice: 0.3986\n",
            "Epoch [1/100], Batch [70/300],  Cross Loss: 0.4393\n",
            "70/300, Train_loss: 0.4393 Train_dice: 0.4393\n",
            "Epoch [1/100], Batch [71/300],  Cross Loss: 0.4605\n",
            "71/300, Train_loss: 0.4605 Train_dice: 0.4605\n",
            "Epoch [1/100], Batch [72/300],  Cross Loss: 0.4280\n",
            "72/300, Train_loss: 0.4280 Train_dice: 0.4280\n",
            "Epoch [1/100], Batch [73/300],  Cross Loss: 0.4711\n",
            "73/300, Train_loss: 0.4711 Train_dice: 0.4711\n",
            "Epoch [1/100], Batch [74/300],  Cross Loss: 0.4377\n",
            "74/300, Train_loss: 0.4377 Train_dice: 0.4377\n",
            "Epoch [1/100], Batch [75/300],  Cross Loss: 0.3830\n",
            "75/300, Train_loss: 0.3830 Train_dice: 0.3830\n",
            "Epoch [1/100], Batch [76/300],  Cross Loss: 0.4164\n",
            "76/300, Train_loss: 0.4164 Train_dice: 0.4164\n",
            "Epoch [1/100], Batch [77/300],  Cross Loss: 0.4564\n",
            "77/300, Train_loss: 0.4564 Train_dice: 0.4564\n",
            "Epoch [1/100], Batch [78/300],  Cross Loss: 0.4843\n",
            "78/300, Train_loss: 0.4843 Train_dice: 0.4843\n",
            "Epoch [1/100], Batch [79/300],  Cross Loss: 0.3212\n",
            "79/300, Train_loss: 0.3212 Train_dice: 0.3212\n",
            "Epoch [1/100], Batch [80/300],  Cross Loss: 0.3831\n",
            "80/300, Train_loss: 0.3831 Train_dice: 0.3831\n",
            "Epoch [1/100], Batch [81/300],  Cross Loss: 0.4442\n",
            "81/300, Train_loss: 0.4442 Train_dice: 0.4442\n",
            "Epoch [1/100], Batch [82/300],  Cross Loss: 0.4408\n",
            "82/300, Train_loss: 0.4408 Train_dice: 0.4408\n",
            "Epoch [1/100], Batch [83/300],  Cross Loss: 0.4356\n",
            "83/300, Train_loss: 0.4356 Train_dice: 0.4356\n",
            "Epoch [1/100], Batch [84/300],  Cross Loss: 0.4813\n",
            "84/300, Train_loss: 0.4813 Train_dice: 0.4813\n",
            "Epoch [1/100], Batch [85/300],  Cross Loss: 0.4467\n",
            "85/300, Train_loss: 0.4467 Train_dice: 0.4467\n",
            "Epoch [1/100], Batch [86/300],  Cross Loss: 0.3404\n",
            "86/300, Train_loss: 0.3404 Train_dice: 0.3404\n",
            "Epoch [1/100], Batch [87/300],  Cross Loss: 0.4477\n",
            "87/300, Train_loss: 0.4477 Train_dice: 0.4477\n",
            "Epoch [1/100], Batch [88/300],  Cross Loss: 0.4650\n",
            "88/300, Train_loss: 0.4650 Train_dice: 0.4650\n",
            "Epoch [1/100], Batch [89/300],  Cross Loss: 0.4627\n",
            "89/300, Train_loss: 0.4627 Train_dice: 0.4627\n",
            "Epoch [1/100], Batch [90/300],  Cross Loss: 0.4588\n",
            "90/300, Train_loss: 0.4588 Train_dice: 0.4588\n",
            "Epoch [1/100], Batch [91/300],  Cross Loss: 0.4664\n",
            "91/300, Train_loss: 0.4664 Train_dice: 0.4664\n",
            "Epoch [1/100], Batch [92/300],  Cross Loss: 0.3816\n",
            "92/300, Train_loss: 0.3816 Train_dice: 0.3816\n",
            "Epoch [1/100], Batch [93/300],  Cross Loss: 0.3995\n",
            "93/300, Train_loss: 0.3995 Train_dice: 0.3995\n",
            "Epoch [1/100], Batch [94/300],  Cross Loss: 0.4186\n",
            "94/300, Train_loss: 0.4186 Train_dice: 0.4186\n",
            "Epoch [1/100], Batch [95/300],  Cross Loss: 0.3445\n",
            "95/300, Train_loss: 0.3445 Train_dice: 0.3445\n",
            "Epoch [1/100], Batch [96/300],  Cross Loss: 0.4717\n",
            "96/300, Train_loss: 0.4717 Train_dice: 0.4717\n",
            "Epoch [1/100], Batch [97/300],  Cross Loss: 0.3992\n",
            "97/300, Train_loss: 0.3992 Train_dice: 0.3992\n",
            "Epoch [1/100], Batch [98/300],  Cross Loss: 0.4423\n",
            "98/300, Train_loss: 0.4423 Train_dice: 0.4423\n",
            "Epoch [1/100], Batch [99/300],  Cross Loss: 0.4462\n",
            "99/300, Train_loss: 0.4462 Train_dice: 0.4462\n",
            "Epoch [1/100], Batch [100/300],  Cross Loss: 0.4039\n",
            "100/300, Train_loss: 0.4039 Train_dice: 0.4039\n",
            "Epoch [1/100], Batch [101/300],  Cross Loss: 0.3336\n",
            "101/300, Train_loss: 0.3336 Train_dice: 0.3336\n",
            "Epoch [1/100], Batch [102/300],  Cross Loss: 0.4081\n",
            "102/300, Train_loss: 0.4081 Train_dice: 0.4081\n",
            "Epoch [1/100], Batch [103/300],  Cross Loss: 0.4298\n",
            "103/300, Train_loss: 0.4298 Train_dice: 0.4298\n",
            "Epoch [1/100], Batch [104/300],  Cross Loss: 0.4546\n",
            "104/300, Train_loss: 0.4546 Train_dice: 0.4546\n",
            "Epoch [1/100], Batch [105/300],  Cross Loss: 0.4631\n",
            "105/300, Train_loss: 0.4631 Train_dice: 0.4631\n",
            "Epoch [1/100], Batch [106/300],  Cross Loss: 0.4605\n",
            "106/300, Train_loss: 0.4605 Train_dice: 0.4605\n",
            "Epoch [1/100], Batch [107/300],  Cross Loss: 0.3350\n",
            "107/300, Train_loss: 0.3350 Train_dice: 0.3350\n",
            "Epoch [1/100], Batch [108/300],  Cross Loss: 0.4337\n",
            "108/300, Train_loss: 0.4337 Train_dice: 0.4337\n",
            "Epoch [1/100], Batch [109/300],  Cross Loss: 0.4616\n",
            "109/300, Train_loss: 0.4616 Train_dice: 0.4616\n",
            "Epoch [1/100], Batch [110/300],  Cross Loss: 0.4451\n",
            "110/300, Train_loss: 0.4451 Train_dice: 0.4451\n",
            "Epoch [1/100], Batch [111/300],  Cross Loss: 0.4179\n",
            "111/300, Train_loss: 0.4179 Train_dice: 0.4179\n",
            "Epoch [1/100], Batch [112/300],  Cross Loss: 0.3765\n",
            "112/300, Train_loss: 0.3765 Train_dice: 0.3765\n",
            "Epoch [1/100], Batch [113/300],  Cross Loss: 0.4543\n",
            "113/300, Train_loss: 0.4543 Train_dice: 0.4543\n",
            "Epoch [1/100], Batch [114/300],  Cross Loss: 0.3514\n",
            "114/300, Train_loss: 0.3514 Train_dice: 0.3514\n",
            "Epoch [1/100], Batch [115/300],  Cross Loss: 0.4384\n",
            "115/300, Train_loss: 0.4384 Train_dice: 0.4384\n",
            "Epoch [1/100], Batch [116/300],  Cross Loss: 0.4400\n",
            "116/300, Train_loss: 0.4400 Train_dice: 0.4400\n",
            "Epoch [1/100], Batch [117/300],  Cross Loss: 0.4152\n",
            "117/300, Train_loss: 0.4152 Train_dice: 0.4152\n",
            "Epoch [1/100], Batch [118/300],  Cross Loss: 0.3624\n",
            "118/300, Train_loss: 0.3624 Train_dice: 0.3624\n",
            "Epoch [1/100], Batch [119/300],  Cross Loss: 0.4381\n",
            "119/300, Train_loss: 0.4381 Train_dice: 0.4381\n",
            "Epoch [1/100], Batch [120/300],  Cross Loss: 0.3535\n",
            "120/300, Train_loss: 0.3535 Train_dice: 0.3535\n",
            "Epoch [1/100], Batch [121/300],  Cross Loss: 0.3808\n",
            "121/300, Train_loss: 0.3808 Train_dice: 0.3808\n",
            "Epoch [1/100], Batch [122/300],  Cross Loss: 0.4426\n",
            "122/300, Train_loss: 0.4426 Train_dice: 0.4426\n",
            "Epoch [1/100], Batch [123/300],  Cross Loss: 0.4286\n",
            "123/300, Train_loss: 0.4286 Train_dice: 0.4286\n",
            "Epoch [1/100], Batch [124/300],  Cross Loss: 0.3250\n",
            "124/300, Train_loss: 0.3250 Train_dice: 0.3250\n",
            "Epoch [1/100], Batch [125/300],  Cross Loss: 0.4454\n",
            "125/300, Train_loss: 0.4454 Train_dice: 0.4454\n",
            "Epoch [1/100], Batch [126/300],  Cross Loss: 0.4455\n",
            "126/300, Train_loss: 0.4455 Train_dice: 0.4455\n",
            "Epoch [1/100], Batch [127/300],  Cross Loss: 0.3744\n",
            "127/300, Train_loss: 0.3744 Train_dice: 0.3744\n",
            "Epoch [1/100], Batch [128/300],  Cross Loss: 0.4394\n",
            "128/300, Train_loss: 0.4394 Train_dice: 0.4394\n",
            "Epoch [1/100], Batch [129/300],  Cross Loss: 0.3436\n",
            "129/300, Train_loss: 0.3436 Train_dice: 0.3436\n",
            "Epoch [1/100], Batch [130/300],  Cross Loss: 0.4355\n",
            "130/300, Train_loss: 0.4355 Train_dice: 0.4355\n",
            "Epoch [1/100], Batch [131/300],  Cross Loss: 0.4457\n",
            "131/300, Train_loss: 0.4457 Train_dice: 0.4457\n",
            "Epoch [1/100], Batch [132/300],  Cross Loss: 0.4304\n",
            "132/300, Train_loss: 0.4304 Train_dice: 0.4304\n",
            "Epoch [1/100], Batch [133/300],  Cross Loss: 0.4363\n",
            "133/300, Train_loss: 0.4363 Train_dice: 0.4363\n",
            "Epoch [1/100], Batch [134/300],  Cross Loss: 0.3400\n",
            "134/300, Train_loss: 0.3400 Train_dice: 0.3400\n",
            "Epoch [1/100], Batch [135/300],  Cross Loss: 0.4687\n",
            "135/300, Train_loss: 0.4687 Train_dice: 0.4687\n",
            "Epoch [1/100], Batch [136/300],  Cross Loss: 0.4078\n",
            "136/300, Train_loss: 0.4078 Train_dice: 0.4078\n",
            "Epoch [1/100], Batch [137/300],  Cross Loss: 0.3756\n",
            "137/300, Train_loss: 0.3756 Train_dice: 0.3756\n",
            "Epoch [1/100], Batch [138/300],  Cross Loss: 0.3494\n",
            "138/300, Train_loss: 0.3494 Train_dice: 0.3494\n",
            "Epoch [1/100], Batch [139/300],  Cross Loss: 0.3316\n",
            "139/300, Train_loss: 0.3316 Train_dice: 0.3316\n",
            "Epoch [1/100], Batch [140/300],  Cross Loss: 0.4181\n",
            "140/300, Train_loss: 0.4181 Train_dice: 0.4181\n",
            "Epoch [1/100], Batch [141/300],  Cross Loss: 0.4222\n",
            "141/300, Train_loss: 0.4222 Train_dice: 0.4222\n",
            "Epoch [1/100], Batch [142/300],  Cross Loss: 0.4203\n",
            "142/300, Train_loss: 0.4203 Train_dice: 0.4203\n",
            "Epoch [1/100], Batch [143/300],  Cross Loss: 0.4416\n",
            "143/300, Train_loss: 0.4416 Train_dice: 0.4416\n",
            "Epoch [1/100], Batch [144/300],  Cross Loss: 0.4187\n",
            "144/300, Train_loss: 0.4187 Train_dice: 0.4187\n",
            "Epoch [1/100], Batch [145/300],  Cross Loss: 0.4051\n",
            "145/300, Train_loss: 0.4051 Train_dice: 0.4051\n",
            "Epoch [1/100], Batch [146/300],  Cross Loss: 0.3319\n",
            "146/300, Train_loss: 0.3319 Train_dice: 0.3319\n",
            "Epoch [1/100], Batch [147/300],  Cross Loss: 0.3309\n",
            "147/300, Train_loss: 0.3309 Train_dice: 0.3309\n",
            "Epoch [1/100], Batch [148/300],  Cross Loss: 0.4886\n",
            "148/300, Train_loss: 0.4886 Train_dice: 0.4886\n",
            "Epoch [1/100], Batch [149/300],  Cross Loss: 0.3958\n",
            "149/300, Train_loss: 0.3958 Train_dice: 0.3958\n",
            "Epoch [1/100], Batch [150/300],  Cross Loss: 0.4386\n",
            "150/300, Train_loss: 0.4386 Train_dice: 0.4386\n",
            "Epoch [1/100], Batch [151/300],  Cross Loss: 0.3189\n",
            "151/300, Train_loss: 0.3189 Train_dice: 0.3189\n",
            "Epoch [1/100], Batch [152/300],  Cross Loss: 0.3178\n",
            "152/300, Train_loss: 0.3178 Train_dice: 0.3178\n",
            "Epoch [1/100], Batch [153/300],  Cross Loss: 0.4943\n",
            "153/300, Train_loss: 0.4943 Train_dice: 0.4943\n",
            "Epoch [1/100], Batch [154/300],  Cross Loss: 0.4869\n",
            "154/300, Train_loss: 0.4869 Train_dice: 0.4869\n",
            "Epoch [1/100], Batch [155/300],  Cross Loss: 0.4802\n",
            "155/300, Train_loss: 0.4802 Train_dice: 0.4802\n",
            "Epoch [1/100], Batch [156/300],  Cross Loss: 0.4562\n",
            "156/300, Train_loss: 0.4562 Train_dice: 0.4562\n",
            "Epoch [1/100], Batch [157/300],  Cross Loss: 0.4769\n",
            "157/300, Train_loss: 0.4769 Train_dice: 0.4769\n",
            "Epoch [1/100], Batch [158/300],  Cross Loss: 0.4642\n",
            "158/300, Train_loss: 0.4642 Train_dice: 0.4642\n",
            "Epoch [1/100], Batch [159/300],  Cross Loss: 0.3866\n",
            "159/300, Train_loss: 0.3866 Train_dice: 0.3866\n",
            "Epoch [1/100], Batch [160/300],  Cross Loss: 0.4433\n",
            "160/300, Train_loss: 0.4433 Train_dice: 0.4433\n",
            "Epoch [1/100], Batch [161/300],  Cross Loss: 0.4323\n",
            "161/300, Train_loss: 0.4323 Train_dice: 0.4323\n",
            "Epoch [1/100], Batch [162/300],  Cross Loss: 0.3439\n",
            "162/300, Train_loss: 0.3439 Train_dice: 0.3439\n",
            "Epoch [1/100], Batch [163/300],  Cross Loss: 0.3967\n",
            "163/300, Train_loss: 0.3967 Train_dice: 0.3967\n",
            "Epoch [1/100], Batch [164/300],  Cross Loss: 0.4456\n",
            "164/300, Train_loss: 0.4456 Train_dice: 0.4456\n",
            "Epoch [1/100], Batch [165/300],  Cross Loss: 0.4239\n",
            "165/300, Train_loss: 0.4239 Train_dice: 0.4239\n",
            "Epoch [1/100], Batch [166/300],  Cross Loss: 0.4495\n",
            "166/300, Train_loss: 0.4495 Train_dice: 0.4495\n",
            "Epoch [1/100], Batch [167/300],  Cross Loss: 0.3806\n",
            "167/300, Train_loss: 0.3806 Train_dice: 0.3806\n",
            "Epoch [1/100], Batch [168/300],  Cross Loss: 0.4238\n",
            "168/300, Train_loss: 0.4238 Train_dice: 0.4238\n",
            "Epoch [1/100], Batch [169/300],  Cross Loss: 0.4308\n",
            "169/300, Train_loss: 0.4308 Train_dice: 0.4308\n",
            "Epoch [1/100], Batch [170/300],  Cross Loss: 0.4353\n",
            "170/300, Train_loss: 0.4353 Train_dice: 0.4353\n",
            "Epoch [1/100], Batch [171/300],  Cross Loss: 0.4358\n",
            "171/300, Train_loss: 0.4358 Train_dice: 0.4358\n",
            "Epoch [1/100], Batch [172/300],  Cross Loss: 0.3720\n",
            "172/300, Train_loss: 0.3720 Train_dice: 0.3720\n",
            "Epoch [1/100], Batch [173/300],  Cross Loss: 0.3711\n",
            "173/300, Train_loss: 0.3711 Train_dice: 0.3711\n",
            "Epoch [1/100], Batch [174/300],  Cross Loss: 0.4144\n",
            "174/300, Train_loss: 0.4144 Train_dice: 0.4144\n",
            "Epoch [1/100], Batch [175/300],  Cross Loss: 0.3907\n",
            "175/300, Train_loss: 0.3907 Train_dice: 0.3907\n",
            "Epoch [1/100], Batch [176/300],  Cross Loss: 0.4652\n",
            "176/300, Train_loss: 0.4652 Train_dice: 0.4652\n",
            "Epoch [1/100], Batch [177/300],  Cross Loss: 0.3971\n",
            "177/300, Train_loss: 0.3971 Train_dice: 0.3971\n",
            "Epoch [1/100], Batch [178/300],  Cross Loss: 0.4415\n",
            "178/300, Train_loss: 0.4415 Train_dice: 0.4415\n",
            "Epoch [1/100], Batch [179/300],  Cross Loss: 0.4385\n",
            "179/300, Train_loss: 0.4385 Train_dice: 0.4385\n",
            "Epoch [1/100], Batch [180/300],  Cross Loss: 0.4071\n",
            "180/300, Train_loss: 0.4071 Train_dice: 0.4071\n",
            "Epoch [1/100], Batch [181/300],  Cross Loss: 0.4067\n",
            "181/300, Train_loss: 0.4067 Train_dice: 0.4067\n",
            "Epoch [1/100], Batch [182/300],  Cross Loss: 0.3775\n",
            "182/300, Train_loss: 0.3775 Train_dice: 0.3775\n",
            "Epoch [1/100], Batch [183/300],  Cross Loss: 0.4341\n",
            "183/300, Train_loss: 0.4341 Train_dice: 0.4341\n",
            "Epoch [1/100], Batch [184/300],  Cross Loss: 0.4064\n",
            "184/300, Train_loss: 0.4064 Train_dice: 0.4064\n",
            "Epoch [1/100], Batch [185/300],  Cross Loss: 0.4726\n",
            "185/300, Train_loss: 0.4726 Train_dice: 0.4726\n",
            "Epoch [1/100], Batch [186/300],  Cross Loss: 0.4213\n",
            "186/300, Train_loss: 0.4213 Train_dice: 0.4213\n",
            "Epoch [1/100], Batch [187/300],  Cross Loss: 0.4315\n",
            "187/300, Train_loss: 0.4315 Train_dice: 0.4315\n",
            "Epoch [1/100], Batch [188/300],  Cross Loss: 0.4233\n",
            "188/300, Train_loss: 0.4233 Train_dice: 0.4233\n",
            "Epoch [1/100], Batch [189/300],  Cross Loss: 0.3669\n",
            "189/300, Train_loss: 0.3669 Train_dice: 0.3669\n",
            "Epoch [1/100], Batch [190/300],  Cross Loss: 0.3806\n",
            "190/300, Train_loss: 0.3806 Train_dice: 0.3806\n",
            "Epoch [1/100], Batch [191/300],  Cross Loss: 0.4674\n",
            "191/300, Train_loss: 0.4674 Train_dice: 0.4674\n",
            "Epoch [1/100], Batch [192/300],  Cross Loss: 0.3602\n",
            "192/300, Train_loss: 0.3602 Train_dice: 0.3602\n",
            "Epoch [1/100], Batch [193/300],  Cross Loss: 0.4315\n",
            "193/300, Train_loss: 0.4315 Train_dice: 0.4315\n",
            "Epoch [1/100], Batch [194/300],  Cross Loss: 0.4259\n",
            "194/300, Train_loss: 0.4259 Train_dice: 0.4259\n",
            "Epoch [1/100], Batch [195/300],  Cross Loss: 0.3459\n",
            "195/300, Train_loss: 0.3459 Train_dice: 0.3459\n",
            "Epoch [1/100], Batch [196/300],  Cross Loss: 0.3402\n",
            "196/300, Train_loss: 0.3402 Train_dice: 0.3402\n",
            "Epoch [1/100], Batch [197/300],  Cross Loss: 0.4642\n",
            "197/300, Train_loss: 0.4642 Train_dice: 0.4642\n",
            "Epoch [1/100], Batch [198/300],  Cross Loss: 0.4061\n",
            "198/300, Train_loss: 0.4061 Train_dice: 0.4061\n",
            "Epoch [1/100], Batch [199/300],  Cross Loss: 0.4547\n",
            "199/300, Train_loss: 0.4547 Train_dice: 0.4547\n",
            "Epoch [1/100], Batch [200/300],  Cross Loss: 0.4350\n",
            "200/300, Train_loss: 0.4350 Train_dice: 0.4350\n",
            "Epoch [1/100], Batch [201/300],  Cross Loss: 0.4548\n",
            "201/300, Train_loss: 0.4548 Train_dice: 0.4548\n",
            "Epoch [1/100], Batch [202/300],  Cross Loss: 0.3334\n",
            "202/300, Train_loss: 0.3334 Train_dice: 0.3334\n",
            "Epoch [1/100], Batch [203/300],  Cross Loss: 0.4743\n",
            "203/300, Train_loss: 0.4743 Train_dice: 0.4743\n",
            "Epoch [1/100], Batch [204/300],  Cross Loss: 0.4706\n",
            "204/300, Train_loss: 0.4706 Train_dice: 0.4706\n",
            "Epoch [1/100], Batch [205/300],  Cross Loss: 0.4852\n",
            "205/300, Train_loss: 0.4852 Train_dice: 0.4852\n",
            "Epoch [1/100], Batch [206/300],  Cross Loss: 0.4708\n",
            "206/300, Train_loss: 0.4708 Train_dice: 0.4708\n",
            "Epoch [1/100], Batch [207/300],  Cross Loss: 0.3845\n",
            "207/300, Train_loss: 0.3845 Train_dice: 0.3845\n",
            "Epoch [1/100], Batch [208/300],  Cross Loss: 0.3213\n",
            "208/300, Train_loss: 0.3213 Train_dice: 0.3213\n",
            "Epoch [1/100], Batch [209/300],  Cross Loss: 0.4824\n",
            "209/300, Train_loss: 0.4824 Train_dice: 0.4824\n",
            "Epoch [1/100], Batch [210/300],  Cross Loss: 0.4341\n",
            "210/300, Train_loss: 0.4341 Train_dice: 0.4341\n",
            "Epoch [1/100], Batch [211/300],  Cross Loss: 0.3248\n",
            "211/300, Train_loss: 0.3248 Train_dice: 0.3248\n",
            "Epoch [1/100], Batch [212/300],  Cross Loss: 0.4072\n",
            "212/300, Train_loss: 0.4072 Train_dice: 0.4072\n",
            "Epoch [1/100], Batch [213/300],  Cross Loss: 0.4190\n",
            "213/300, Train_loss: 0.4190 Train_dice: 0.4190\n",
            "Epoch [1/100], Batch [214/300],  Cross Loss: 0.4704\n",
            "214/300, Train_loss: 0.4704 Train_dice: 0.4704\n",
            "Epoch [1/100], Batch [215/300],  Cross Loss: 0.4488\n",
            "215/300, Train_loss: 0.4488 Train_dice: 0.4488\n",
            "Epoch [1/100], Batch [216/300],  Cross Loss: 0.4008\n",
            "216/300, Train_loss: 0.4008 Train_dice: 0.4008\n",
            "Epoch [1/100], Batch [217/300],  Cross Loss: 0.3613\n",
            "217/300, Train_loss: 0.3613 Train_dice: 0.3613\n",
            "Epoch [1/100], Batch [218/300],  Cross Loss: 0.4469\n",
            "218/300, Train_loss: 0.4469 Train_dice: 0.4469\n",
            "Epoch [1/100], Batch [219/300],  Cross Loss: 0.4676\n",
            "219/300, Train_loss: 0.4676 Train_dice: 0.4676\n",
            "Epoch [1/100], Batch [220/300],  Cross Loss: 0.4006\n",
            "220/300, Train_loss: 0.4006 Train_dice: 0.4006\n",
            "Epoch [1/100], Batch [221/300],  Cross Loss: 0.4451\n",
            "221/300, Train_loss: 0.4451 Train_dice: 0.4451\n",
            "Epoch [1/100], Batch [222/300],  Cross Loss: 0.3355\n",
            "222/300, Train_loss: 0.3355 Train_dice: 0.3355\n",
            "Epoch [1/100], Batch [223/300],  Cross Loss: 0.4352\n",
            "223/300, Train_loss: 0.4352 Train_dice: 0.4352\n",
            "Epoch [1/100], Batch [224/300],  Cross Loss: 0.4407\n",
            "224/300, Train_loss: 0.4407 Train_dice: 0.4407\n",
            "Epoch [1/100], Batch [225/300],  Cross Loss: 0.3990\n",
            "225/300, Train_loss: 0.3990 Train_dice: 0.3990\n",
            "Epoch [1/100], Batch [226/300],  Cross Loss: 0.3507\n",
            "226/300, Train_loss: 0.3507 Train_dice: 0.3507\n",
            "Epoch [1/100], Batch [227/300],  Cross Loss: 0.4224\n",
            "227/300, Train_loss: 0.4224 Train_dice: 0.4224\n",
            "Epoch [1/100], Batch [228/300],  Cross Loss: 0.4480\n",
            "228/300, Train_loss: 0.4480 Train_dice: 0.4480\n",
            "Epoch [1/100], Batch [229/300],  Cross Loss: 0.3623\n",
            "229/300, Train_loss: 0.3623 Train_dice: 0.3623\n",
            "Epoch [1/100], Batch [230/300],  Cross Loss: 0.3894\n",
            "230/300, Train_loss: 0.3894 Train_dice: 0.3894\n",
            "Epoch [1/100], Batch [231/300],  Cross Loss: 0.4792\n",
            "231/300, Train_loss: 0.4792 Train_dice: 0.4792\n",
            "Epoch [1/100], Batch [232/300],  Cross Loss: 0.4389\n",
            "232/300, Train_loss: 0.4389 Train_dice: 0.4389\n",
            "Epoch [1/100], Batch [233/300],  Cross Loss: 0.4391\n",
            "233/300, Train_loss: 0.4391 Train_dice: 0.4391\n",
            "Epoch [1/100], Batch [234/300],  Cross Loss: 0.4333\n",
            "234/300, Train_loss: 0.4333 Train_dice: 0.4333\n",
            "Epoch [1/100], Batch [235/300],  Cross Loss: 0.4210\n",
            "235/300, Train_loss: 0.4210 Train_dice: 0.4210\n",
            "Epoch [1/100], Batch [236/300],  Cross Loss: 0.4191\n",
            "236/300, Train_loss: 0.4191 Train_dice: 0.4191\n",
            "Epoch [1/100], Batch [237/300],  Cross Loss: 0.4816\n",
            "237/300, Train_loss: 0.4816 Train_dice: 0.4816\n",
            "Epoch [1/100], Batch [238/300],  Cross Loss: 0.4072\n",
            "238/300, Train_loss: 0.4072 Train_dice: 0.4072\n",
            "Epoch [1/100], Batch [239/300],  Cross Loss: 0.4189\n",
            "239/300, Train_loss: 0.4189 Train_dice: 0.4189\n",
            "Epoch [1/100], Batch [240/300],  Cross Loss: 0.3690\n",
            "240/300, Train_loss: 0.3690 Train_dice: 0.3690\n",
            "Epoch [1/100], Batch [241/300],  Cross Loss: 0.4388\n",
            "241/300, Train_loss: 0.4388 Train_dice: 0.4388\n",
            "Epoch [1/100], Batch [242/300],  Cross Loss: 0.4350\n",
            "242/300, Train_loss: 0.4350 Train_dice: 0.4350\n",
            "Epoch [1/100], Batch [243/300],  Cross Loss: 0.4182\n",
            "243/300, Train_loss: 0.4182 Train_dice: 0.4182\n",
            "Epoch [1/100], Batch [244/300],  Cross Loss: 0.3789\n",
            "244/300, Train_loss: 0.3789 Train_dice: 0.3789\n",
            "Epoch [1/100], Batch [245/300],  Cross Loss: 0.4368\n",
            "245/300, Train_loss: 0.4368 Train_dice: 0.4368\n",
            "Epoch [1/100], Batch [246/300],  Cross Loss: 0.4420\n",
            "246/300, Train_loss: 0.4420 Train_dice: 0.4420\n",
            "Epoch [1/100], Batch [247/300],  Cross Loss: 0.3721\n",
            "247/300, Train_loss: 0.3721 Train_dice: 0.3721\n",
            "Epoch [1/100], Batch [248/300],  Cross Loss: 0.3927\n",
            "248/300, Train_loss: 0.3927 Train_dice: 0.3927\n",
            "Epoch [1/100], Batch [249/300],  Cross Loss: 0.4234\n",
            "249/300, Train_loss: 0.4234 Train_dice: 0.4234\n",
            "Epoch [1/100], Batch [250/300],  Cross Loss: 0.4122\n",
            "250/300, Train_loss: 0.4122 Train_dice: 0.4122\n",
            "Epoch [1/100], Batch [251/300],  Cross Loss: 0.4590\n",
            "251/300, Train_loss: 0.4590 Train_dice: 0.4590\n",
            "Epoch [1/100], Batch [252/300],  Cross Loss: 0.4175\n",
            "252/300, Train_loss: 0.4175 Train_dice: 0.4175\n",
            "Epoch [1/100], Batch [253/300],  Cross Loss: 0.4103\n",
            "253/300, Train_loss: 0.4103 Train_dice: 0.4103\n",
            "Epoch [1/100], Batch [254/300],  Cross Loss: 0.3644\n",
            "254/300, Train_loss: 0.3644 Train_dice: 0.3644\n",
            "Epoch [1/100], Batch [255/300],  Cross Loss: 0.4750\n",
            "255/300, Train_loss: 0.4750 Train_dice: 0.4750\n",
            "Epoch [1/100], Batch [256/300],  Cross Loss: 0.4292\n",
            "256/300, Train_loss: 0.4292 Train_dice: 0.4292\n",
            "Epoch [1/100], Batch [257/300],  Cross Loss: 0.4625\n",
            "257/300, Train_loss: 0.4625 Train_dice: 0.4625\n",
            "Epoch [1/100], Batch [258/300],  Cross Loss: 0.4019\n",
            "258/300, Train_loss: 0.4019 Train_dice: 0.4019\n",
            "Epoch [1/100], Batch [259/300],  Cross Loss: 0.4701\n",
            "259/300, Train_loss: 0.4701 Train_dice: 0.4701\n",
            "Epoch [1/100], Batch [260/300],  Cross Loss: 0.3927\n",
            "260/300, Train_loss: 0.3927 Train_dice: 0.3927\n",
            "Epoch [1/100], Batch [261/300],  Cross Loss: 0.4043\n",
            "261/300, Train_loss: 0.4043 Train_dice: 0.4043\n",
            "Epoch [1/100], Batch [262/300],  Cross Loss: 0.4668\n",
            "262/300, Train_loss: 0.4668 Train_dice: 0.4668\n",
            "Epoch [1/100], Batch [263/300],  Cross Loss: 0.4552\n",
            "263/300, Train_loss: 0.4552 Train_dice: 0.4552\n",
            "Epoch [1/100], Batch [264/300],  Cross Loss: 0.4328\n",
            "264/300, Train_loss: 0.4328 Train_dice: 0.4328\n",
            "Epoch [1/100], Batch [265/300],  Cross Loss: 0.3551\n",
            "265/300, Train_loss: 0.3551 Train_dice: 0.3551\n",
            "Epoch [1/100], Batch [266/300],  Cross Loss: 0.4428\n",
            "266/300, Train_loss: 0.4428 Train_dice: 0.4428\n",
            "Epoch [1/100], Batch [267/300],  Cross Loss: 0.4264\n",
            "267/300, Train_loss: 0.4264 Train_dice: 0.4264\n",
            "Epoch [1/100], Batch [268/300],  Cross Loss: 0.4476\n",
            "268/300, Train_loss: 0.4476 Train_dice: 0.4476\n",
            "Epoch [1/100], Batch [269/300],  Cross Loss: 0.4403\n",
            "269/300, Train_loss: 0.4403 Train_dice: 0.4403\n",
            "Epoch [1/100], Batch [270/300],  Cross Loss: 0.4089\n",
            "270/300, Train_loss: 0.4089 Train_dice: 0.4089\n",
            "Epoch [1/100], Batch [271/300],  Cross Loss: 0.4104\n",
            "271/300, Train_loss: 0.4104 Train_dice: 0.4104\n",
            "Epoch [1/100], Batch [272/300],  Cross Loss: 0.4183\n",
            "272/300, Train_loss: 0.4183 Train_dice: 0.4183\n",
            "Epoch [1/100], Batch [273/300],  Cross Loss: 0.4345\n",
            "273/300, Train_loss: 0.4345 Train_dice: 0.4345\n",
            "Epoch [1/100], Batch [274/300],  Cross Loss: 0.4259\n",
            "274/300, Train_loss: 0.4259 Train_dice: 0.4259\n",
            "Epoch [1/100], Batch [275/300],  Cross Loss: 0.4168\n",
            "275/300, Train_loss: 0.4168 Train_dice: 0.4168\n",
            "Epoch [1/100], Batch [276/300],  Cross Loss: 0.4204\n",
            "276/300, Train_loss: 0.4204 Train_dice: 0.4204\n",
            "Epoch [1/100], Batch [277/300],  Cross Loss: 0.4456\n",
            "277/300, Train_loss: 0.4456 Train_dice: 0.4456\n",
            "Epoch [1/100], Batch [278/300],  Cross Loss: 0.4416\n",
            "278/300, Train_loss: 0.4416 Train_dice: 0.4416\n",
            "Epoch [1/100], Batch [279/300],  Cross Loss: 0.4384\n",
            "279/300, Train_loss: 0.4384 Train_dice: 0.4384\n",
            "Epoch [1/100], Batch [280/300],  Cross Loss: 0.3855\n",
            "280/300, Train_loss: 0.3855 Train_dice: 0.3855\n",
            "Epoch [1/100], Batch [281/300],  Cross Loss: 0.4320\n",
            "281/300, Train_loss: 0.4320 Train_dice: 0.4320\n",
            "Epoch [1/100], Batch [282/300],  Cross Loss: 0.3862\n",
            "282/300, Train_loss: 0.3862 Train_dice: 0.3862\n",
            "Epoch [1/100], Batch [283/300],  Cross Loss: 0.4269\n",
            "283/300, Train_loss: 0.4269 Train_dice: 0.4269\n",
            "Epoch [1/100], Batch [284/300],  Cross Loss: 0.4218\n",
            "284/300, Train_loss: 0.4218 Train_dice: 0.4218\n",
            "Epoch [1/100], Batch [285/300],  Cross Loss: 0.4257\n",
            "285/300, Train_loss: 0.4257 Train_dice: 0.4257\n",
            "Epoch [1/100], Batch [286/300],  Cross Loss: 0.3548\n",
            "286/300, Train_loss: 0.3548 Train_dice: 0.3548\n",
            "Epoch [1/100], Batch [287/300],  Cross Loss: 0.4132\n",
            "287/300, Train_loss: 0.4132 Train_dice: 0.4132\n",
            "Epoch [1/100], Batch [288/300],  Cross Loss: 0.4015\n",
            "288/300, Train_loss: 0.4015 Train_dice: 0.4015\n",
            "Epoch [1/100], Batch [289/300],  Cross Loss: 0.4178\n",
            "289/300, Train_loss: 0.4178 Train_dice: 0.4178\n",
            "Epoch [1/100], Batch [290/300],  Cross Loss: 0.4109\n",
            "290/300, Train_loss: 0.4109 Train_dice: 0.4109\n",
            "Epoch [1/100], Batch [291/300],  Cross Loss: 0.3907\n",
            "291/300, Train_loss: 0.3907 Train_dice: 0.3907\n",
            "Epoch [1/100], Batch [292/300],  Cross Loss: 0.4464\n",
            "292/300, Train_loss: 0.4464 Train_dice: 0.4464\n",
            "Epoch [1/100], Batch [293/300],  Cross Loss: 0.3578\n",
            "293/300, Train_loss: 0.3578 Train_dice: 0.3578\n",
            "Epoch [1/100], Batch [294/300],  Cross Loss: 0.3576\n",
            "294/300, Train_loss: 0.3576 Train_dice: 0.3576\n",
            "Epoch [1/100], Batch [295/300],  Cross Loss: 0.4323\n",
            "295/300, Train_loss: 0.4323 Train_dice: 0.4323\n",
            "Epoch [1/100], Batch [296/300],  Cross Loss: 0.4007\n",
            "296/300, Train_loss: 0.4007 Train_dice: 0.4007\n",
            "Epoch [1/100], Batch [297/300],  Cross Loss: 0.4313\n",
            "297/300, Train_loss: 0.4313 Train_dice: 0.4313\n",
            "Epoch [1/100], Batch [298/300],  Cross Loss: 0.3901\n",
            "298/300, Train_loss: 0.3901 Train_dice: 0.3901\n",
            "Epoch [1/100], Batch [299/300],  Cross Loss: 0.4203\n",
            "299/300, Train_loss: 0.4203 Train_dice: 0.4203\n",
            "Epoch [1/100], Batch [300/300],  Cross Loss: 0.3802\n",
            "300/300, Train_loss: 0.3802 Train_dice: 0.3802\n",
            "--------------------\n",
            "Epoch_loss: 0.4163\n",
            "Epoch_metric: tensor(0.4163, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 2/100\n",
            "Epoch [2/100], Batch [1/300],  Cross Loss: 0.3913\n",
            "1/300, Train_loss: 0.3913 Train_dice: 0.3913\n",
            "Epoch [2/100], Batch [2/300],  Cross Loss: 0.4247\n",
            "2/300, Train_loss: 0.4247 Train_dice: 0.4247\n",
            "Epoch [2/100], Batch [3/300],  Cross Loss: 0.3578\n",
            "3/300, Train_loss: 0.3578 Train_dice: 0.3578\n",
            "Epoch [2/100], Batch [4/300],  Cross Loss: 0.3659\n",
            "4/300, Train_loss: 0.3659 Train_dice: 0.3659\n",
            "Epoch [2/100], Batch [5/300],  Cross Loss: 0.4179\n",
            "5/300, Train_loss: 0.4179 Train_dice: 0.4179\n",
            "Epoch [2/100], Batch [6/300],  Cross Loss: 0.4180\n",
            "6/300, Train_loss: 0.4180 Train_dice: 0.4180\n",
            "Epoch [2/100], Batch [7/300],  Cross Loss: 0.4013\n",
            "7/300, Train_loss: 0.4013 Train_dice: 0.4013\n",
            "Epoch [2/100], Batch [8/300],  Cross Loss: 0.4796\n",
            "8/300, Train_loss: 0.4796 Train_dice: 0.4796\n",
            "Epoch [2/100], Batch [9/300],  Cross Loss: 0.4080\n",
            "9/300, Train_loss: 0.4080 Train_dice: 0.4080\n",
            "Epoch [2/100], Batch [10/300],  Cross Loss: 0.4803\n",
            "10/300, Train_loss: 0.4803 Train_dice: 0.4803\n",
            "Epoch [2/100], Batch [11/300],  Cross Loss: 0.3819\n",
            "11/300, Train_loss: 0.3819 Train_dice: 0.3819\n",
            "Epoch [2/100], Batch [12/300],  Cross Loss: 0.4686\n",
            "12/300, Train_loss: 0.4686 Train_dice: 0.4686\n",
            "Epoch [2/100], Batch [13/300],  Cross Loss: 0.3892\n",
            "13/300, Train_loss: 0.3892 Train_dice: 0.3892\n",
            "Epoch [2/100], Batch [14/300],  Cross Loss: 0.4806\n",
            "14/300, Train_loss: 0.4806 Train_dice: 0.4806\n",
            "Epoch [2/100], Batch [15/300],  Cross Loss: 0.4223\n",
            "15/300, Train_loss: 0.4223 Train_dice: 0.4223\n",
            "Epoch [2/100], Batch [16/300],  Cross Loss: 0.4625\n",
            "16/300, Train_loss: 0.4625 Train_dice: 0.4625\n",
            "Epoch [2/100], Batch [17/300],  Cross Loss: 0.3788\n",
            "17/300, Train_loss: 0.3788 Train_dice: 0.3788\n",
            "Epoch [2/100], Batch [18/300],  Cross Loss: 0.3894\n",
            "18/300, Train_loss: 0.3894 Train_dice: 0.3894\n",
            "Epoch [2/100], Batch [19/300],  Cross Loss: 0.3821\n",
            "19/300, Train_loss: 0.3821 Train_dice: 0.3821\n",
            "Epoch [2/100], Batch [20/300],  Cross Loss: 0.4064\n",
            "20/300, Train_loss: 0.4064 Train_dice: 0.4064\n",
            "Epoch [2/100], Batch [21/300],  Cross Loss: 0.3730\n",
            "21/300, Train_loss: 0.3730 Train_dice: 0.3730\n",
            "Epoch [2/100], Batch [22/300],  Cross Loss: 0.3869\n",
            "22/300, Train_loss: 0.3869 Train_dice: 0.3869\n",
            "Epoch [2/100], Batch [23/300],  Cross Loss: 0.4685\n",
            "23/300, Train_loss: 0.4685 Train_dice: 0.4685\n",
            "Epoch [2/100], Batch [24/300],  Cross Loss: 0.3781\n",
            "24/300, Train_loss: 0.3781 Train_dice: 0.3781\n",
            "Epoch [2/100], Batch [25/300],  Cross Loss: 0.3661\n",
            "25/300, Train_loss: 0.3661 Train_dice: 0.3661\n",
            "Epoch [2/100], Batch [26/300],  Cross Loss: 0.4101\n",
            "26/300, Train_loss: 0.4101 Train_dice: 0.4101\n",
            "Epoch [2/100], Batch [27/300],  Cross Loss: 0.4141\n",
            "27/300, Train_loss: 0.4141 Train_dice: 0.4141\n",
            "Epoch [2/100], Batch [28/300],  Cross Loss: 0.4706\n",
            "28/300, Train_loss: 0.4706 Train_dice: 0.4706\n",
            "Epoch [2/100], Batch [29/300],  Cross Loss: 0.4594\n",
            "29/300, Train_loss: 0.4594 Train_dice: 0.4594\n",
            "Epoch [2/100], Batch [30/300],  Cross Loss: 0.4031\n",
            "30/300, Train_loss: 0.4031 Train_dice: 0.4031\n",
            "Epoch [2/100], Batch [31/300],  Cross Loss: 0.4685\n",
            "31/300, Train_loss: 0.4685 Train_dice: 0.4685\n",
            "Epoch [2/100], Batch [32/300],  Cross Loss: 0.4142\n",
            "32/300, Train_loss: 0.4142 Train_dice: 0.4142\n",
            "Epoch [2/100], Batch [33/300],  Cross Loss: 0.3863\n",
            "33/300, Train_loss: 0.3863 Train_dice: 0.3863\n",
            "Epoch [2/100], Batch [34/300],  Cross Loss: 0.4222\n",
            "34/300, Train_loss: 0.4222 Train_dice: 0.4222\n",
            "Epoch [2/100], Batch [35/300],  Cross Loss: 0.3955\n",
            "35/300, Train_loss: 0.3955 Train_dice: 0.3955\n",
            "Epoch [2/100], Batch [36/300],  Cross Loss: 0.3533\n",
            "36/300, Train_loss: 0.3533 Train_dice: 0.3533\n",
            "Epoch [2/100], Batch [37/300],  Cross Loss: 0.3777\n",
            "37/300, Train_loss: 0.3777 Train_dice: 0.3777\n",
            "Epoch [2/100], Batch [38/300],  Cross Loss: 0.4214\n",
            "38/300, Train_loss: 0.4214 Train_dice: 0.4214\n",
            "Epoch [2/100], Batch [39/300],  Cross Loss: 0.3754\n",
            "39/300, Train_loss: 0.3754 Train_dice: 0.3754\n",
            "Epoch [2/100], Batch [40/300],  Cross Loss: 0.3701\n",
            "40/300, Train_loss: 0.3701 Train_dice: 0.3701\n",
            "Epoch [2/100], Batch [41/300],  Cross Loss: 0.3716\n",
            "41/300, Train_loss: 0.3716 Train_dice: 0.3716\n",
            "Epoch [2/100], Batch [42/300],  Cross Loss: 0.4752\n",
            "42/300, Train_loss: 0.4752 Train_dice: 0.4752\n",
            "Epoch [2/100], Batch [43/300],  Cross Loss: 0.4204\n",
            "43/300, Train_loss: 0.4204 Train_dice: 0.4204\n",
            "Epoch [2/100], Batch [44/300],  Cross Loss: 0.3686\n",
            "44/300, Train_loss: 0.3686 Train_dice: 0.3686\n",
            "Epoch [2/100], Batch [45/300],  Cross Loss: 0.3872\n",
            "45/300, Train_loss: 0.3872 Train_dice: 0.3872\n",
            "Epoch [2/100], Batch [46/300],  Cross Loss: 0.4091\n",
            "46/300, Train_loss: 0.4091 Train_dice: 0.4091\n",
            "Epoch [2/100], Batch [47/300],  Cross Loss: 0.4159\n",
            "47/300, Train_loss: 0.4159 Train_dice: 0.4159\n",
            "Epoch [2/100], Batch [48/300],  Cross Loss: 0.4886\n",
            "48/300, Train_loss: 0.4886 Train_dice: 0.4886\n",
            "Epoch [2/100], Batch [49/300],  Cross Loss: 0.3537\n",
            "49/300, Train_loss: 0.3537 Train_dice: 0.3537\n",
            "Epoch [2/100], Batch [50/300],  Cross Loss: 0.3652\n",
            "50/300, Train_loss: 0.3652 Train_dice: 0.3652\n",
            "Epoch [2/100], Batch [51/300],  Cross Loss: 0.4655\n",
            "51/300, Train_loss: 0.4655 Train_dice: 0.4655\n",
            "Epoch [2/100], Batch [52/300],  Cross Loss: 0.3509\n",
            "52/300, Train_loss: 0.3509 Train_dice: 0.3509\n",
            "Epoch [2/100], Batch [53/300],  Cross Loss: 0.4630\n",
            "53/300, Train_loss: 0.4630 Train_dice: 0.4630\n",
            "Epoch [2/100], Batch [54/300],  Cross Loss: 0.4890\n",
            "54/300, Train_loss: 0.4890 Train_dice: 0.4890\n",
            "Epoch [2/100], Batch [55/300],  Cross Loss: 0.3981\n",
            "55/300, Train_loss: 0.3981 Train_dice: 0.3981\n",
            "Epoch [2/100], Batch [56/300],  Cross Loss: 0.4661\n",
            "56/300, Train_loss: 0.4661 Train_dice: 0.4661\n",
            "Epoch [2/100], Batch [57/300],  Cross Loss: 0.4420\n",
            "57/300, Train_loss: 0.4420 Train_dice: 0.4420\n",
            "Epoch [2/100], Batch [58/300],  Cross Loss: 0.4275\n",
            "58/300, Train_loss: 0.4275 Train_dice: 0.4275\n",
            "Epoch [2/100], Batch [59/300],  Cross Loss: 0.4769\n",
            "59/300, Train_loss: 0.4769 Train_dice: 0.4769\n",
            "Epoch [2/100], Batch [60/300],  Cross Loss: 0.4153\n",
            "60/300, Train_loss: 0.4153 Train_dice: 0.4153\n",
            "Epoch [2/100], Batch [61/300],  Cross Loss: 0.4790\n",
            "61/300, Train_loss: 0.4790 Train_dice: 0.4790\n",
            "Epoch [2/100], Batch [62/300],  Cross Loss: 0.4534\n",
            "62/300, Train_loss: 0.4534 Train_dice: 0.4534\n",
            "Epoch [2/100], Batch [63/300],  Cross Loss: 0.3854\n",
            "63/300, Train_loss: 0.3854 Train_dice: 0.3854\n",
            "Epoch [2/100], Batch [64/300],  Cross Loss: 0.4355\n",
            "64/300, Train_loss: 0.4355 Train_dice: 0.4355\n",
            "Epoch [2/100], Batch [65/300],  Cross Loss: 0.3770\n",
            "65/300, Train_loss: 0.3770 Train_dice: 0.3770\n",
            "Epoch [2/100], Batch [66/300],  Cross Loss: 0.3644\n",
            "66/300, Train_loss: 0.3644 Train_dice: 0.3644\n",
            "Epoch [2/100], Batch [67/300],  Cross Loss: 0.4327\n",
            "67/300, Train_loss: 0.4327 Train_dice: 0.4327\n",
            "Epoch [2/100], Batch [68/300],  Cross Loss: 0.4596\n",
            "68/300, Train_loss: 0.4596 Train_dice: 0.4596\n",
            "Epoch [2/100], Batch [69/300],  Cross Loss: 0.3954\n",
            "69/300, Train_loss: 0.3954 Train_dice: 0.3954\n",
            "Epoch [2/100], Batch [70/300],  Cross Loss: 0.4141\n",
            "70/300, Train_loss: 0.4141 Train_dice: 0.4141\n",
            "Epoch [2/100], Batch [71/300],  Cross Loss: 0.3826\n",
            "71/300, Train_loss: 0.3826 Train_dice: 0.3826\n",
            "Epoch [2/100], Batch [72/300],  Cross Loss: 0.4688\n",
            "72/300, Train_loss: 0.4688 Train_dice: 0.4688\n",
            "Epoch [2/100], Batch [73/300],  Cross Loss: 0.4664\n",
            "73/300, Train_loss: 0.4664 Train_dice: 0.4664\n",
            "Epoch [2/100], Batch [74/300],  Cross Loss: 0.4419\n",
            "74/300, Train_loss: 0.4419 Train_dice: 0.4419\n",
            "Epoch [2/100], Batch [75/300],  Cross Loss: 0.4121\n",
            "75/300, Train_loss: 0.4121 Train_dice: 0.4121\n",
            "Epoch [2/100], Batch [76/300],  Cross Loss: 0.3480\n",
            "76/300, Train_loss: 0.3480 Train_dice: 0.3480\n",
            "Epoch [2/100], Batch [77/300],  Cross Loss: 0.4644\n",
            "77/300, Train_loss: 0.4644 Train_dice: 0.4644\n",
            "Epoch [2/100], Batch [78/300],  Cross Loss: 0.3995\n",
            "78/300, Train_loss: 0.3995 Train_dice: 0.3995\n",
            "Epoch [2/100], Batch [79/300],  Cross Loss: 0.4469\n",
            "79/300, Train_loss: 0.4469 Train_dice: 0.4469\n",
            "Epoch [2/100], Batch [80/300],  Cross Loss: 0.3360\n",
            "80/300, Train_loss: 0.3360 Train_dice: 0.3360\n",
            "Epoch [2/100], Batch [81/300],  Cross Loss: 0.3962\n",
            "81/300, Train_loss: 0.3962 Train_dice: 0.3962\n",
            "Epoch [2/100], Batch [82/300],  Cross Loss: 0.3916\n",
            "82/300, Train_loss: 0.3916 Train_dice: 0.3916\n",
            "Epoch [2/100], Batch [83/300],  Cross Loss: 0.4221\n",
            "83/300, Train_loss: 0.4221 Train_dice: 0.4221\n",
            "Epoch [2/100], Batch [84/300],  Cross Loss: 0.4606\n",
            "84/300, Train_loss: 0.4606 Train_dice: 0.4606\n",
            "Epoch [2/100], Batch [85/300],  Cross Loss: 0.3350\n",
            "85/300, Train_loss: 0.3350 Train_dice: 0.3350\n",
            "Epoch [2/100], Batch [86/300],  Cross Loss: 0.3622\n",
            "86/300, Train_loss: 0.3622 Train_dice: 0.3622\n",
            "Epoch [2/100], Batch [87/300],  Cross Loss: 0.4392\n",
            "87/300, Train_loss: 0.4392 Train_dice: 0.4392\n",
            "Epoch [2/100], Batch [88/300],  Cross Loss: 0.4722\n",
            "88/300, Train_loss: 0.4722 Train_dice: 0.4722\n",
            "Epoch [2/100], Batch [89/300],  Cross Loss: 0.4504\n",
            "89/300, Train_loss: 0.4504 Train_dice: 0.4504\n",
            "Epoch [2/100], Batch [90/300],  Cross Loss: 0.4638\n",
            "90/300, Train_loss: 0.4638 Train_dice: 0.4638\n",
            "Epoch [2/100], Batch [91/300],  Cross Loss: 0.3948\n",
            "91/300, Train_loss: 0.3948 Train_dice: 0.3948\n",
            "Epoch [2/100], Batch [92/300],  Cross Loss: 0.4394\n",
            "92/300, Train_loss: 0.4394 Train_dice: 0.4394\n",
            "Epoch [2/100], Batch [93/300],  Cross Loss: 0.4611\n",
            "93/300, Train_loss: 0.4611 Train_dice: 0.4611\n",
            "Epoch [2/100], Batch [94/300],  Cross Loss: 0.4109\n",
            "94/300, Train_loss: 0.4109 Train_dice: 0.4109\n",
            "Epoch [2/100], Batch [95/300],  Cross Loss: 0.4290\n",
            "95/300, Train_loss: 0.4290 Train_dice: 0.4290\n",
            "Epoch [2/100], Batch [96/300],  Cross Loss: 0.4098\n",
            "96/300, Train_loss: 0.4098 Train_dice: 0.4098\n",
            "Epoch [2/100], Batch [97/300],  Cross Loss: 0.3404\n",
            "97/300, Train_loss: 0.3404 Train_dice: 0.3404\n",
            "Epoch [2/100], Batch [98/300],  Cross Loss: 0.4411\n",
            "98/300, Train_loss: 0.4411 Train_dice: 0.4411\n",
            "Epoch [2/100], Batch [99/300],  Cross Loss: 0.4653\n",
            "99/300, Train_loss: 0.4653 Train_dice: 0.4653\n",
            "Epoch [2/100], Batch [100/300],  Cross Loss: 0.3500\n",
            "100/300, Train_loss: 0.3500 Train_dice: 0.3500\n",
            "Epoch [2/100], Batch [101/300],  Cross Loss: 0.4300\n",
            "101/300, Train_loss: 0.4300 Train_dice: 0.4300\n",
            "Epoch [2/100], Batch [102/300],  Cross Loss: 0.3986\n",
            "102/300, Train_loss: 0.3986 Train_dice: 0.3986\n",
            "Epoch [2/100], Batch [103/300],  Cross Loss: 0.3895\n",
            "103/300, Train_loss: 0.3895 Train_dice: 0.3895\n",
            "Epoch [2/100], Batch [104/300],  Cross Loss: 0.4249\n",
            "104/300, Train_loss: 0.4249 Train_dice: 0.4249\n",
            "Epoch [2/100], Batch [105/300],  Cross Loss: 0.4069\n",
            "105/300, Train_loss: 0.4069 Train_dice: 0.4069\n",
            "Epoch [2/100], Batch [106/300],  Cross Loss: 0.4535\n",
            "106/300, Train_loss: 0.4535 Train_dice: 0.4535\n",
            "Epoch [2/100], Batch [107/300],  Cross Loss: 0.3549\n",
            "107/300, Train_loss: 0.3549 Train_dice: 0.3549\n",
            "Epoch [2/100], Batch [108/300],  Cross Loss: 0.3839\n",
            "108/300, Train_loss: 0.3839 Train_dice: 0.3839\n",
            "Epoch [2/100], Batch [109/300],  Cross Loss: 0.4017\n",
            "109/300, Train_loss: 0.4017 Train_dice: 0.4017\n",
            "Epoch [2/100], Batch [110/300],  Cross Loss: 0.4025\n",
            "110/300, Train_loss: 0.4025 Train_dice: 0.4025\n",
            "Epoch [2/100], Batch [111/300],  Cross Loss: 0.4606\n",
            "111/300, Train_loss: 0.4606 Train_dice: 0.4606\n",
            "Epoch [2/100], Batch [112/300],  Cross Loss: 0.4439\n",
            "112/300, Train_loss: 0.4439 Train_dice: 0.4439\n",
            "Epoch [2/100], Batch [113/300],  Cross Loss: 0.4483\n",
            "113/300, Train_loss: 0.4483 Train_dice: 0.4483\n",
            "Epoch [2/100], Batch [114/300],  Cross Loss: 0.3627\n",
            "114/300, Train_loss: 0.3627 Train_dice: 0.3627\n",
            "Epoch [2/100], Batch [115/300],  Cross Loss: 0.3631\n",
            "115/300, Train_loss: 0.3631 Train_dice: 0.3631\n",
            "Epoch [2/100], Batch [116/300],  Cross Loss: 0.4179\n",
            "116/300, Train_loss: 0.4179 Train_dice: 0.4179\n",
            "Epoch [2/100], Batch [117/300],  Cross Loss: 0.4583\n",
            "117/300, Train_loss: 0.4583 Train_dice: 0.4583\n",
            "Epoch [2/100], Batch [118/300],  Cross Loss: 0.4443\n",
            "118/300, Train_loss: 0.4443 Train_dice: 0.4443\n",
            "Epoch [2/100], Batch [119/300],  Cross Loss: 0.4415\n",
            "119/300, Train_loss: 0.4415 Train_dice: 0.4415\n",
            "Epoch [2/100], Batch [120/300],  Cross Loss: 0.3931\n",
            "120/300, Train_loss: 0.3931 Train_dice: 0.3931\n",
            "Epoch [2/100], Batch [121/300],  Cross Loss: 0.3771\n",
            "121/300, Train_loss: 0.3771 Train_dice: 0.3771\n",
            "Epoch [2/100], Batch [122/300],  Cross Loss: 0.4701\n",
            "122/300, Train_loss: 0.4701 Train_dice: 0.4701\n",
            "Epoch [2/100], Batch [123/300],  Cross Loss: 0.4279\n",
            "123/300, Train_loss: 0.4279 Train_dice: 0.4279\n",
            "Epoch [2/100], Batch [124/300],  Cross Loss: 0.4234\n",
            "124/300, Train_loss: 0.4234 Train_dice: 0.4234\n",
            "Epoch [2/100], Batch [125/300],  Cross Loss: 0.3941\n",
            "125/300, Train_loss: 0.3941 Train_dice: 0.3941\n",
            "Epoch [2/100], Batch [126/300],  Cross Loss: 0.3901\n",
            "126/300, Train_loss: 0.3901 Train_dice: 0.3901\n",
            "Epoch [2/100], Batch [127/300],  Cross Loss: 0.3351\n",
            "127/300, Train_loss: 0.3351 Train_dice: 0.3351\n",
            "Epoch [2/100], Batch [128/300],  Cross Loss: 0.4553\n",
            "128/300, Train_loss: 0.4553 Train_dice: 0.4553\n",
            "Epoch [2/100], Batch [129/300],  Cross Loss: 0.3833\n",
            "129/300, Train_loss: 0.3833 Train_dice: 0.3833\n",
            "Epoch [2/100], Batch [130/300],  Cross Loss: 0.4029\n",
            "130/300, Train_loss: 0.4029 Train_dice: 0.4029\n",
            "Epoch [2/100], Batch [131/300],  Cross Loss: 0.3795\n",
            "131/300, Train_loss: 0.3795 Train_dice: 0.3795\n",
            "Epoch [2/100], Batch [132/300],  Cross Loss: 0.3834\n",
            "132/300, Train_loss: 0.3834 Train_dice: 0.3834\n",
            "Epoch [2/100], Batch [133/300],  Cross Loss: 0.4457\n",
            "133/300, Train_loss: 0.4457 Train_dice: 0.4457\n",
            "Epoch [2/100], Batch [134/300],  Cross Loss: 0.4547\n",
            "134/300, Train_loss: 0.4547 Train_dice: 0.4547\n",
            "Epoch [2/100], Batch [135/300],  Cross Loss: 0.4578\n",
            "135/300, Train_loss: 0.4578 Train_dice: 0.4578\n",
            "Epoch [2/100], Batch [136/300],  Cross Loss: 0.3720\n",
            "136/300, Train_loss: 0.3720 Train_dice: 0.3720\n",
            "Epoch [2/100], Batch [137/300],  Cross Loss: 0.3828\n",
            "137/300, Train_loss: 0.3828 Train_dice: 0.3828\n",
            "Epoch [2/100], Batch [138/300],  Cross Loss: 0.4642\n",
            "138/300, Train_loss: 0.4642 Train_dice: 0.4642\n",
            "Epoch [2/100], Batch [139/300],  Cross Loss: 0.4703\n",
            "139/300, Train_loss: 0.4703 Train_dice: 0.4703\n",
            "Epoch [2/100], Batch [140/300],  Cross Loss: 0.4236\n",
            "140/300, Train_loss: 0.4236 Train_dice: 0.4236\n",
            "Epoch [2/100], Batch [141/300],  Cross Loss: 0.4258\n",
            "141/300, Train_loss: 0.4258 Train_dice: 0.4258\n",
            "Epoch [2/100], Batch [142/300],  Cross Loss: 0.4532\n",
            "142/300, Train_loss: 0.4532 Train_dice: 0.4532\n",
            "Epoch [2/100], Batch [143/300],  Cross Loss: 0.3507\n",
            "143/300, Train_loss: 0.3507 Train_dice: 0.3507\n",
            "Epoch [2/100], Batch [144/300],  Cross Loss: 0.4583\n",
            "144/300, Train_loss: 0.4583 Train_dice: 0.4583\n",
            "Epoch [2/100], Batch [145/300],  Cross Loss: 0.4161\n",
            "145/300, Train_loss: 0.4161 Train_dice: 0.4161\n",
            "Epoch [2/100], Batch [146/300],  Cross Loss: 0.4259\n",
            "146/300, Train_loss: 0.4259 Train_dice: 0.4259\n",
            "Epoch [2/100], Batch [147/300],  Cross Loss: 0.4303\n",
            "147/300, Train_loss: 0.4303 Train_dice: 0.4303\n",
            "Epoch [2/100], Batch [148/300],  Cross Loss: 0.4239\n",
            "148/300, Train_loss: 0.4239 Train_dice: 0.4239\n",
            "Epoch [2/100], Batch [149/300],  Cross Loss: 0.3735\n",
            "149/300, Train_loss: 0.3735 Train_dice: 0.3735\n",
            "Epoch [2/100], Batch [150/300],  Cross Loss: 0.3590\n",
            "150/300, Train_loss: 0.3590 Train_dice: 0.3590\n",
            "Epoch [2/100], Batch [151/300],  Cross Loss: 0.4102\n",
            "151/300, Train_loss: 0.4102 Train_dice: 0.4102\n",
            "Epoch [2/100], Batch [152/300],  Cross Loss: 0.4489\n",
            "152/300, Train_loss: 0.4489 Train_dice: 0.4489\n",
            "Epoch [2/100], Batch [153/300],  Cross Loss: 0.4304\n",
            "153/300, Train_loss: 0.4304 Train_dice: 0.4304\n",
            "Epoch [2/100], Batch [154/300],  Cross Loss: 0.4560\n",
            "154/300, Train_loss: 0.4560 Train_dice: 0.4560\n",
            "Epoch [2/100], Batch [155/300],  Cross Loss: 0.4594\n",
            "155/300, Train_loss: 0.4594 Train_dice: 0.4594\n",
            "Epoch [2/100], Batch [156/300],  Cross Loss: 0.3877\n",
            "156/300, Train_loss: 0.3877 Train_dice: 0.3877\n",
            "Epoch [2/100], Batch [157/300],  Cross Loss: 0.4580\n",
            "157/300, Train_loss: 0.4580 Train_dice: 0.4580\n",
            "Epoch [2/100], Batch [158/300],  Cross Loss: 0.4476\n",
            "158/300, Train_loss: 0.4476 Train_dice: 0.4476\n",
            "Epoch [2/100], Batch [159/300],  Cross Loss: 0.4024\n",
            "159/300, Train_loss: 0.4024 Train_dice: 0.4024\n",
            "Epoch [2/100], Batch [160/300],  Cross Loss: 0.4113\n",
            "160/300, Train_loss: 0.4113 Train_dice: 0.4113\n",
            "Epoch [2/100], Batch [161/300],  Cross Loss: 0.4609\n",
            "161/300, Train_loss: 0.4609 Train_dice: 0.4609\n",
            "Epoch [2/100], Batch [162/300],  Cross Loss: 0.3891\n",
            "162/300, Train_loss: 0.3891 Train_dice: 0.3891\n",
            "Epoch [2/100], Batch [163/300],  Cross Loss: 0.3783\n",
            "163/300, Train_loss: 0.3783 Train_dice: 0.3783\n",
            "Epoch [2/100], Batch [164/300],  Cross Loss: 0.4205\n",
            "164/300, Train_loss: 0.4205 Train_dice: 0.4205\n",
            "Epoch [2/100], Batch [165/300],  Cross Loss: 0.4052\n",
            "165/300, Train_loss: 0.4052 Train_dice: 0.4052\n",
            "Epoch [2/100], Batch [166/300],  Cross Loss: 0.4296\n",
            "166/300, Train_loss: 0.4296 Train_dice: 0.4296\n",
            "Epoch [2/100], Batch [167/300],  Cross Loss: 0.4312\n",
            "167/300, Train_loss: 0.4312 Train_dice: 0.4312\n",
            "Epoch [2/100], Batch [168/300],  Cross Loss: 0.3996\n",
            "168/300, Train_loss: 0.3996 Train_dice: 0.3996\n",
            "Epoch [2/100], Batch [169/300],  Cross Loss: 0.4362\n",
            "169/300, Train_loss: 0.4362 Train_dice: 0.4362\n",
            "Epoch [2/100], Batch [170/300],  Cross Loss: 0.4229\n",
            "170/300, Train_loss: 0.4229 Train_dice: 0.4229\n",
            "Epoch [2/100], Batch [171/300],  Cross Loss: 0.4176\n",
            "171/300, Train_loss: 0.4176 Train_dice: 0.4176\n",
            "Epoch [2/100], Batch [172/300],  Cross Loss: 0.3838\n",
            "172/300, Train_loss: 0.3838 Train_dice: 0.3838\n",
            "Epoch [2/100], Batch [173/300],  Cross Loss: 0.4067\n",
            "173/300, Train_loss: 0.4067 Train_dice: 0.4067\n",
            "Epoch [2/100], Batch [174/300],  Cross Loss: 0.3857\n",
            "174/300, Train_loss: 0.3857 Train_dice: 0.3857\n",
            "Epoch [2/100], Batch [175/300],  Cross Loss: 0.4238\n",
            "175/300, Train_loss: 0.4238 Train_dice: 0.4238\n",
            "Epoch [2/100], Batch [176/300],  Cross Loss: 0.3644\n",
            "176/300, Train_loss: 0.3644 Train_dice: 0.3644\n",
            "Epoch [2/100], Batch [177/300],  Cross Loss: 0.4047\n",
            "177/300, Train_loss: 0.4047 Train_dice: 0.4047\n",
            "Epoch [2/100], Batch [178/300],  Cross Loss: 0.3364\n",
            "178/300, Train_loss: 0.3364 Train_dice: 0.3364\n",
            "Epoch [2/100], Batch [179/300],  Cross Loss: 0.3994\n",
            "179/300, Train_loss: 0.3994 Train_dice: 0.3994\n",
            "Epoch [2/100], Batch [180/300],  Cross Loss: 0.4052\n",
            "180/300, Train_loss: 0.4052 Train_dice: 0.4052\n",
            "Epoch [2/100], Batch [181/300],  Cross Loss: 0.3878\n",
            "181/300, Train_loss: 0.3878 Train_dice: 0.3878\n",
            "Epoch [2/100], Batch [182/300],  Cross Loss: 0.4457\n",
            "182/300, Train_loss: 0.4457 Train_dice: 0.4457\n",
            "Epoch [2/100], Batch [183/300],  Cross Loss: 0.3695\n",
            "183/300, Train_loss: 0.3695 Train_dice: 0.3695\n",
            "Epoch [2/100], Batch [184/300],  Cross Loss: 0.3597\n",
            "184/300, Train_loss: 0.3597 Train_dice: 0.3597\n",
            "Epoch [2/100], Batch [185/300],  Cross Loss: 0.4259\n",
            "185/300, Train_loss: 0.4259 Train_dice: 0.4259\n",
            "Epoch [2/100], Batch [186/300],  Cross Loss: 0.4360\n",
            "186/300, Train_loss: 0.4360 Train_dice: 0.4360\n",
            "Epoch [2/100], Batch [187/300],  Cross Loss: 0.4687\n",
            "187/300, Train_loss: 0.4687 Train_dice: 0.4687\n",
            "Epoch [2/100], Batch [188/300],  Cross Loss: 0.4460\n",
            "188/300, Train_loss: 0.4460 Train_dice: 0.4460\n",
            "Epoch [2/100], Batch [189/300],  Cross Loss: 0.3571\n",
            "189/300, Train_loss: 0.3571 Train_dice: 0.3571\n",
            "Epoch [2/100], Batch [190/300],  Cross Loss: 0.3717\n",
            "190/300, Train_loss: 0.3717 Train_dice: 0.3717\n",
            "Epoch [2/100], Batch [191/300],  Cross Loss: 0.3932\n",
            "191/300, Train_loss: 0.3932 Train_dice: 0.3932\n",
            "Epoch [2/100], Batch [192/300],  Cross Loss: 0.3663\n",
            "192/300, Train_loss: 0.3663 Train_dice: 0.3663\n",
            "Epoch [2/100], Batch [193/300],  Cross Loss: 0.3476\n",
            "193/300, Train_loss: 0.3476 Train_dice: 0.3476\n",
            "Epoch [2/100], Batch [194/300],  Cross Loss: 0.4301\n",
            "194/300, Train_loss: 0.4301 Train_dice: 0.4301\n",
            "Epoch [2/100], Batch [195/300],  Cross Loss: 0.4212\n",
            "195/300, Train_loss: 0.4212 Train_dice: 0.4212\n",
            "Epoch [2/100], Batch [196/300],  Cross Loss: 0.3828\n",
            "196/300, Train_loss: 0.3828 Train_dice: 0.3828\n",
            "Epoch [2/100], Batch [197/300],  Cross Loss: 0.4199\n",
            "197/300, Train_loss: 0.4199 Train_dice: 0.4199\n",
            "Epoch [2/100], Batch [198/300],  Cross Loss: 0.4281\n",
            "198/300, Train_loss: 0.4281 Train_dice: 0.4281\n",
            "Epoch [2/100], Batch [199/300],  Cross Loss: 0.4170\n",
            "199/300, Train_loss: 0.4170 Train_dice: 0.4170\n",
            "Epoch [2/100], Batch [200/300],  Cross Loss: 0.3854\n",
            "200/300, Train_loss: 0.3854 Train_dice: 0.3854\n",
            "Epoch [2/100], Batch [201/300],  Cross Loss: 0.3601\n",
            "201/300, Train_loss: 0.3601 Train_dice: 0.3601\n",
            "Epoch [2/100], Batch [202/300],  Cross Loss: 0.4240\n",
            "202/300, Train_loss: 0.4240 Train_dice: 0.4240\n",
            "Epoch [2/100], Batch [203/300],  Cross Loss: 0.4124\n",
            "203/300, Train_loss: 0.4124 Train_dice: 0.4124\n",
            "Epoch [2/100], Batch [204/300],  Cross Loss: 0.3886\n",
            "204/300, Train_loss: 0.3886 Train_dice: 0.3886\n",
            "Epoch [2/100], Batch [205/300],  Cross Loss: 0.4149\n",
            "205/300, Train_loss: 0.4149 Train_dice: 0.4149\n",
            "Epoch [2/100], Batch [206/300],  Cross Loss: 0.4247\n",
            "206/300, Train_loss: 0.4247 Train_dice: 0.4247\n",
            "Epoch [2/100], Batch [207/300],  Cross Loss: 0.4203\n",
            "207/300, Train_loss: 0.4203 Train_dice: 0.4203\n",
            "Epoch [2/100], Batch [208/300],  Cross Loss: 0.4414\n",
            "208/300, Train_loss: 0.4414 Train_dice: 0.4414\n",
            "Epoch [2/100], Batch [209/300],  Cross Loss: 0.3461\n",
            "209/300, Train_loss: 0.3461 Train_dice: 0.3461\n",
            "Epoch [2/100], Batch [210/300],  Cross Loss: 0.4633\n",
            "210/300, Train_loss: 0.4633 Train_dice: 0.4633\n",
            "Epoch [2/100], Batch [211/300],  Cross Loss: 0.4018\n",
            "211/300, Train_loss: 0.4018 Train_dice: 0.4018\n",
            "Epoch [2/100], Batch [212/300],  Cross Loss: 0.4228\n",
            "212/300, Train_loss: 0.4228 Train_dice: 0.4228\n",
            "Epoch [2/100], Batch [213/300],  Cross Loss: 0.4576\n",
            "213/300, Train_loss: 0.4576 Train_dice: 0.4576\n",
            "Epoch [2/100], Batch [214/300],  Cross Loss: 0.3741\n",
            "214/300, Train_loss: 0.3741 Train_dice: 0.3741\n",
            "Epoch [2/100], Batch [215/300],  Cross Loss: 0.4152\n",
            "215/300, Train_loss: 0.4152 Train_dice: 0.4152\n",
            "Epoch [2/100], Batch [216/300],  Cross Loss: 0.4061\n",
            "216/300, Train_loss: 0.4061 Train_dice: 0.4061\n",
            "Epoch [2/100], Batch [217/300],  Cross Loss: 0.3960\n",
            "217/300, Train_loss: 0.3960 Train_dice: 0.3960\n",
            "Epoch [2/100], Batch [218/300],  Cross Loss: 0.4220\n",
            "218/300, Train_loss: 0.4220 Train_dice: 0.4220\n",
            "Epoch [2/100], Batch [219/300],  Cross Loss: 0.4155\n",
            "219/300, Train_loss: 0.4155 Train_dice: 0.4155\n",
            "Epoch [2/100], Batch [220/300],  Cross Loss: 0.4426\n",
            "220/300, Train_loss: 0.4426 Train_dice: 0.4426\n",
            "Epoch [2/100], Batch [221/300],  Cross Loss: 0.4080\n",
            "221/300, Train_loss: 0.4080 Train_dice: 0.4080\n",
            "Epoch [2/100], Batch [222/300],  Cross Loss: 0.3807\n",
            "222/300, Train_loss: 0.3807 Train_dice: 0.3807\n",
            "Epoch [2/100], Batch [223/300],  Cross Loss: 0.4206\n",
            "223/300, Train_loss: 0.4206 Train_dice: 0.4206\n",
            "Epoch [2/100], Batch [224/300],  Cross Loss: 0.4181\n",
            "224/300, Train_loss: 0.4181 Train_dice: 0.4181\n",
            "Epoch [2/100], Batch [225/300],  Cross Loss: 0.3969\n",
            "225/300, Train_loss: 0.3969 Train_dice: 0.3969\n",
            "Epoch [2/100], Batch [226/300],  Cross Loss: 0.3852\n",
            "226/300, Train_loss: 0.3852 Train_dice: 0.3852\n",
            "Epoch [2/100], Batch [227/300],  Cross Loss: 0.4150\n",
            "227/300, Train_loss: 0.4150 Train_dice: 0.4150\n",
            "Epoch [2/100], Batch [228/300],  Cross Loss: 0.2935\n",
            "228/300, Train_loss: 0.2935 Train_dice: 0.2935\n",
            "Epoch [2/100], Batch [229/300],  Cross Loss: 0.4172\n",
            "229/300, Train_loss: 0.4172 Train_dice: 0.4172\n",
            "Epoch [2/100], Batch [230/300],  Cross Loss: 0.3666\n",
            "230/300, Train_loss: 0.3666 Train_dice: 0.3666\n",
            "Epoch [2/100], Batch [231/300],  Cross Loss: 0.3942\n",
            "231/300, Train_loss: 0.3942 Train_dice: 0.3942\n",
            "Epoch [2/100], Batch [232/300],  Cross Loss: 0.4438\n",
            "232/300, Train_loss: 0.4438 Train_dice: 0.4438\n",
            "Epoch [2/100], Batch [233/300],  Cross Loss: 0.4216\n",
            "233/300, Train_loss: 0.4216 Train_dice: 0.4216\n",
            "Epoch [2/100], Batch [234/300],  Cross Loss: 0.3999\n",
            "234/300, Train_loss: 0.3999 Train_dice: 0.3999\n",
            "Epoch [2/100], Batch [235/300],  Cross Loss: 0.4286\n",
            "235/300, Train_loss: 0.4286 Train_dice: 0.4286\n",
            "Epoch [2/100], Batch [236/300],  Cross Loss: 0.4035\n",
            "236/300, Train_loss: 0.4035 Train_dice: 0.4035\n",
            "Epoch [2/100], Batch [237/300],  Cross Loss: 0.3795\n",
            "237/300, Train_loss: 0.3795 Train_dice: 0.3795\n",
            "Epoch [2/100], Batch [238/300],  Cross Loss: 0.4252\n",
            "238/300, Train_loss: 0.4252 Train_dice: 0.4252\n",
            "Epoch [2/100], Batch [239/300],  Cross Loss: 0.4387\n",
            "239/300, Train_loss: 0.4387 Train_dice: 0.4387\n",
            "Epoch [2/100], Batch [240/300],  Cross Loss: 0.4108\n",
            "240/300, Train_loss: 0.4108 Train_dice: 0.4108\n",
            "Epoch [2/100], Batch [241/300],  Cross Loss: 0.3693\n",
            "241/300, Train_loss: 0.3693 Train_dice: 0.3693\n",
            "Epoch [2/100], Batch [242/300],  Cross Loss: 0.3753\n",
            "242/300, Train_loss: 0.3753 Train_dice: 0.3753\n",
            "Epoch [2/100], Batch [243/300],  Cross Loss: 0.3655\n",
            "243/300, Train_loss: 0.3655 Train_dice: 0.3655\n",
            "Epoch [2/100], Batch [244/300],  Cross Loss: 0.4122\n",
            "244/300, Train_loss: 0.4122 Train_dice: 0.4122\n",
            "Epoch [2/100], Batch [245/300],  Cross Loss: 0.3825\n",
            "245/300, Train_loss: 0.3825 Train_dice: 0.3825\n",
            "Epoch [2/100], Batch [246/300],  Cross Loss: 0.3030\n",
            "246/300, Train_loss: 0.3030 Train_dice: 0.3030\n",
            "Epoch [2/100], Batch [247/300],  Cross Loss: 0.3487\n",
            "247/300, Train_loss: 0.3487 Train_dice: 0.3487\n",
            "Epoch [2/100], Batch [248/300],  Cross Loss: 0.4277\n",
            "248/300, Train_loss: 0.4277 Train_dice: 0.4277\n",
            "Epoch [2/100], Batch [249/300],  Cross Loss: 0.4609\n",
            "249/300, Train_loss: 0.4609 Train_dice: 0.4609\n",
            "Epoch [2/100], Batch [250/300],  Cross Loss: 0.4345\n",
            "250/300, Train_loss: 0.4345 Train_dice: 0.4345\n",
            "Epoch [2/100], Batch [251/300],  Cross Loss: 0.3703\n",
            "251/300, Train_loss: 0.3703 Train_dice: 0.3703\n",
            "Epoch [2/100], Batch [252/300],  Cross Loss: 0.4331\n",
            "252/300, Train_loss: 0.4331 Train_dice: 0.4331\n",
            "Epoch [2/100], Batch [253/300],  Cross Loss: 0.4624\n",
            "253/300, Train_loss: 0.4624 Train_dice: 0.4624\n",
            "Epoch [2/100], Batch [254/300],  Cross Loss: 0.3162\n",
            "254/300, Train_loss: 0.3162 Train_dice: 0.3162\n",
            "Epoch [2/100], Batch [255/300],  Cross Loss: 0.4322\n",
            "255/300, Train_loss: 0.4322 Train_dice: 0.4322\n",
            "Epoch [2/100], Batch [256/300],  Cross Loss: 0.4060\n",
            "256/300, Train_loss: 0.4060 Train_dice: 0.4060\n",
            "Epoch [2/100], Batch [257/300],  Cross Loss: 0.4136\n",
            "257/300, Train_loss: 0.4136 Train_dice: 0.4136\n",
            "Epoch [2/100], Batch [258/300],  Cross Loss: 0.4192\n",
            "258/300, Train_loss: 0.4192 Train_dice: 0.4192\n",
            "Epoch [2/100], Batch [259/300],  Cross Loss: 0.4572\n",
            "259/300, Train_loss: 0.4572 Train_dice: 0.4572\n",
            "Epoch [2/100], Batch [260/300],  Cross Loss: 0.3850\n",
            "260/300, Train_loss: 0.3850 Train_dice: 0.3850\n",
            "Epoch [2/100], Batch [261/300],  Cross Loss: 0.4513\n",
            "261/300, Train_loss: 0.4513 Train_dice: 0.4513\n",
            "Epoch [2/100], Batch [262/300],  Cross Loss: 0.4556\n",
            "262/300, Train_loss: 0.4556 Train_dice: 0.4556\n",
            "Epoch [2/100], Batch [263/300],  Cross Loss: 0.3767\n",
            "263/300, Train_loss: 0.3767 Train_dice: 0.3767\n",
            "Epoch [2/100], Batch [264/300],  Cross Loss: 0.4032\n",
            "264/300, Train_loss: 0.4032 Train_dice: 0.4032\n",
            "Epoch [2/100], Batch [265/300],  Cross Loss: 0.3640\n",
            "265/300, Train_loss: 0.3640 Train_dice: 0.3640\n",
            "Epoch [2/100], Batch [266/300],  Cross Loss: 0.2929\n",
            "266/300, Train_loss: 0.2929 Train_dice: 0.2929\n",
            "Epoch [2/100], Batch [267/300],  Cross Loss: 0.4375\n",
            "267/300, Train_loss: 0.4375 Train_dice: 0.4375\n",
            "Epoch [2/100], Batch [268/300],  Cross Loss: 0.4006\n",
            "268/300, Train_loss: 0.4006 Train_dice: 0.4006\n",
            "Epoch [2/100], Batch [269/300],  Cross Loss: 0.4408\n",
            "269/300, Train_loss: 0.4408 Train_dice: 0.4408\n",
            "Epoch [2/100], Batch [270/300],  Cross Loss: 0.3958\n",
            "270/300, Train_loss: 0.3958 Train_dice: 0.3958\n",
            "Epoch [2/100], Batch [271/300],  Cross Loss: 0.4069\n",
            "271/300, Train_loss: 0.4069 Train_dice: 0.4069\n",
            "Epoch [2/100], Batch [272/300],  Cross Loss: 0.4206\n",
            "272/300, Train_loss: 0.4206 Train_dice: 0.4206\n",
            "Epoch [2/100], Batch [273/300],  Cross Loss: 0.3768\n",
            "273/300, Train_loss: 0.3768 Train_dice: 0.3768\n",
            "Epoch [2/100], Batch [274/300],  Cross Loss: 0.4413\n",
            "274/300, Train_loss: 0.4413 Train_dice: 0.4413\n",
            "Epoch [2/100], Batch [275/300],  Cross Loss: 0.4094\n",
            "275/300, Train_loss: 0.4094 Train_dice: 0.4094\n",
            "Epoch [2/100], Batch [276/300],  Cross Loss: 0.4488\n",
            "276/300, Train_loss: 0.4488 Train_dice: 0.4488\n",
            "Epoch [2/100], Batch [277/300],  Cross Loss: 0.4526\n",
            "277/300, Train_loss: 0.4526 Train_dice: 0.4526\n",
            "Epoch [2/100], Batch [278/300],  Cross Loss: 0.4210\n",
            "278/300, Train_loss: 0.4210 Train_dice: 0.4210\n",
            "Epoch [2/100], Batch [279/300],  Cross Loss: 0.4564\n",
            "279/300, Train_loss: 0.4564 Train_dice: 0.4564\n",
            "Epoch [2/100], Batch [280/300],  Cross Loss: 0.4591\n",
            "280/300, Train_loss: 0.4591 Train_dice: 0.4591\n",
            "Epoch [2/100], Batch [281/300],  Cross Loss: 0.4344\n",
            "281/300, Train_loss: 0.4344 Train_dice: 0.4344\n",
            "Epoch [2/100], Batch [282/300],  Cross Loss: 0.3888\n",
            "282/300, Train_loss: 0.3888 Train_dice: 0.3888\n",
            "Epoch [2/100], Batch [283/300],  Cross Loss: 0.4479\n",
            "283/300, Train_loss: 0.4479 Train_dice: 0.4479\n",
            "Epoch [2/100], Batch [284/300],  Cross Loss: 0.4043\n",
            "284/300, Train_loss: 0.4043 Train_dice: 0.4043\n",
            "Epoch [2/100], Batch [285/300],  Cross Loss: 0.3852\n",
            "285/300, Train_loss: 0.3852 Train_dice: 0.3852\n",
            "Epoch [2/100], Batch [286/300],  Cross Loss: 0.3921\n",
            "286/300, Train_loss: 0.3921 Train_dice: 0.3921\n",
            "Epoch [2/100], Batch [287/300],  Cross Loss: 0.4452\n",
            "287/300, Train_loss: 0.4452 Train_dice: 0.4452\n",
            "Epoch [2/100], Batch [288/300],  Cross Loss: 0.4417\n",
            "288/300, Train_loss: 0.4417 Train_dice: 0.4417\n",
            "Epoch [2/100], Batch [289/300],  Cross Loss: 0.3911\n",
            "289/300, Train_loss: 0.3911 Train_dice: 0.3911\n",
            "Epoch [2/100], Batch [290/300],  Cross Loss: 0.3956\n",
            "290/300, Train_loss: 0.3956 Train_dice: 0.3956\n",
            "Epoch [2/100], Batch [291/300],  Cross Loss: 0.3982\n",
            "291/300, Train_loss: 0.3982 Train_dice: 0.3982\n",
            "Epoch [2/100], Batch [292/300],  Cross Loss: 0.4177\n",
            "292/300, Train_loss: 0.4177 Train_dice: 0.4177\n",
            "Epoch [2/100], Batch [293/300],  Cross Loss: 0.3729\n",
            "293/300, Train_loss: 0.3729 Train_dice: 0.3729\n",
            "Epoch [2/100], Batch [294/300],  Cross Loss: 0.4220\n",
            "294/300, Train_loss: 0.4220 Train_dice: 0.4220\n",
            "Epoch [2/100], Batch [295/300],  Cross Loss: 0.4148\n",
            "295/300, Train_loss: 0.4148 Train_dice: 0.4148\n",
            "Epoch [2/100], Batch [296/300],  Cross Loss: 0.4004\n",
            "296/300, Train_loss: 0.4004 Train_dice: 0.4004\n",
            "Epoch [2/100], Batch [297/300],  Cross Loss: 0.3998\n",
            "297/300, Train_loss: 0.3998 Train_dice: 0.3998\n",
            "Epoch [2/100], Batch [298/300],  Cross Loss: 0.4094\n",
            "298/300, Train_loss: 0.4094 Train_dice: 0.4094\n",
            "Epoch [2/100], Batch [299/300],  Cross Loss: 0.4153\n",
            "299/300, Train_loss: 0.4153 Train_dice: 0.4153\n",
            "Epoch [2/100], Batch [300/300],  Cross Loss: 0.4118\n",
            "300/300, Train_loss: 0.4118 Train_dice: 0.4118\n",
            "--------------------\n",
            "Epoch_loss: 0.4117\n",
            "Epoch_metric: tensor(0.4117, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 3/100\n",
            "Epoch [3/100], Batch [1/300],  Cross Loss: 0.4542\n",
            "1/300, Train_loss: 0.4542 Train_dice: 0.4542\n",
            "Epoch [3/100], Batch [2/300],  Cross Loss: 0.3906\n",
            "2/300, Train_loss: 0.3906 Train_dice: 0.3906\n",
            "Epoch [3/100], Batch [3/300],  Cross Loss: 0.4006\n",
            "3/300, Train_loss: 0.4006 Train_dice: 0.4006\n",
            "Epoch [3/100], Batch [4/300],  Cross Loss: 0.2618\n",
            "4/300, Train_loss: 0.2618 Train_dice: 0.2618\n",
            "Epoch [3/100], Batch [5/300],  Cross Loss: 0.4116\n",
            "5/300, Train_loss: 0.4116 Train_dice: 0.4116\n",
            "Epoch [3/100], Batch [6/300],  Cross Loss: 0.4103\n",
            "6/300, Train_loss: 0.4103 Train_dice: 0.4103\n",
            "Epoch [3/100], Batch [7/300],  Cross Loss: 0.4368\n",
            "7/300, Train_loss: 0.4368 Train_dice: 0.4368\n",
            "Epoch [3/100], Batch [8/300],  Cross Loss: 0.3972\n",
            "8/300, Train_loss: 0.3972 Train_dice: 0.3972\n",
            "Epoch [3/100], Batch [9/300],  Cross Loss: 0.3555\n",
            "9/300, Train_loss: 0.3555 Train_dice: 0.3555\n",
            "Epoch [3/100], Batch [10/300],  Cross Loss: 0.3938\n",
            "10/300, Train_loss: 0.3938 Train_dice: 0.3938\n",
            "Epoch [3/100], Batch [11/300],  Cross Loss: 0.4551\n",
            "11/300, Train_loss: 0.4551 Train_dice: 0.4551\n",
            "Epoch [3/100], Batch [12/300],  Cross Loss: 0.3857\n",
            "12/300, Train_loss: 0.3857 Train_dice: 0.3857\n",
            "Epoch [3/100], Batch [13/300],  Cross Loss: 0.3850\n",
            "13/300, Train_loss: 0.3850 Train_dice: 0.3850\n",
            "Epoch [3/100], Batch [14/300],  Cross Loss: 0.4148\n",
            "14/300, Train_loss: 0.4148 Train_dice: 0.4148\n",
            "Epoch [3/100], Batch [15/300],  Cross Loss: 0.4011\n",
            "15/300, Train_loss: 0.4011 Train_dice: 0.4011\n",
            "Epoch [3/100], Batch [16/300],  Cross Loss: 0.3832\n",
            "16/300, Train_loss: 0.3832 Train_dice: 0.3832\n",
            "Epoch [3/100], Batch [17/300],  Cross Loss: 0.4144\n",
            "17/300, Train_loss: 0.4144 Train_dice: 0.4144\n",
            "Epoch [3/100], Batch [18/300],  Cross Loss: 0.3683\n",
            "18/300, Train_loss: 0.3683 Train_dice: 0.3683\n",
            "Epoch [3/100], Batch [19/300],  Cross Loss: 0.4452\n",
            "19/300, Train_loss: 0.4452 Train_dice: 0.4452\n",
            "Epoch [3/100], Batch [20/300],  Cross Loss: 0.3420\n",
            "20/300, Train_loss: 0.3420 Train_dice: 0.3420\n",
            "Epoch [3/100], Batch [21/300],  Cross Loss: 0.3827\n",
            "21/300, Train_loss: 0.3827 Train_dice: 0.3827\n",
            "Epoch [3/100], Batch [22/300],  Cross Loss: 0.4636\n",
            "22/300, Train_loss: 0.4636 Train_dice: 0.4636\n",
            "Epoch [3/100], Batch [23/300],  Cross Loss: 0.4260\n",
            "23/300, Train_loss: 0.4260 Train_dice: 0.4260\n",
            "Epoch [3/100], Batch [24/300],  Cross Loss: 0.4301\n",
            "24/300, Train_loss: 0.4301 Train_dice: 0.4301\n",
            "Epoch [3/100], Batch [25/300],  Cross Loss: 0.4152\n",
            "25/300, Train_loss: 0.4152 Train_dice: 0.4152\n",
            "Epoch [3/100], Batch [26/300],  Cross Loss: 0.4206\n",
            "26/300, Train_loss: 0.4206 Train_dice: 0.4206\n",
            "Epoch [3/100], Batch [27/300],  Cross Loss: 0.4722\n",
            "27/300, Train_loss: 0.4722 Train_dice: 0.4722\n",
            "Epoch [3/100], Batch [28/300],  Cross Loss: 0.4366\n",
            "28/300, Train_loss: 0.4366 Train_dice: 0.4366\n",
            "Epoch [3/100], Batch [29/300],  Cross Loss: 0.3347\n",
            "29/300, Train_loss: 0.3347 Train_dice: 0.3347\n",
            "Epoch [3/100], Batch [30/300],  Cross Loss: 0.3448\n",
            "30/300, Train_loss: 0.3448 Train_dice: 0.3448\n",
            "Epoch [3/100], Batch [31/300],  Cross Loss: 0.3636\n",
            "31/300, Train_loss: 0.3636 Train_dice: 0.3636\n",
            "Epoch [3/100], Batch [32/300],  Cross Loss: 0.4290\n",
            "32/300, Train_loss: 0.4290 Train_dice: 0.4290\n",
            "Epoch [3/100], Batch [33/300],  Cross Loss: 0.4450\n",
            "33/300, Train_loss: 0.4450 Train_dice: 0.4450\n",
            "Epoch [3/100], Batch [34/300],  Cross Loss: 0.4164\n",
            "34/300, Train_loss: 0.4164 Train_dice: 0.4164\n",
            "Epoch [3/100], Batch [35/300],  Cross Loss: 0.3114\n",
            "35/300, Train_loss: 0.3114 Train_dice: 0.3114\n",
            "Epoch [3/100], Batch [36/300],  Cross Loss: 0.4170\n",
            "36/300, Train_loss: 0.4170 Train_dice: 0.4170\n",
            "Epoch [3/100], Batch [37/300],  Cross Loss: 0.4729\n",
            "37/300, Train_loss: 0.4729 Train_dice: 0.4729\n",
            "Epoch [3/100], Batch [38/300],  Cross Loss: 0.3867\n",
            "38/300, Train_loss: 0.3867 Train_dice: 0.3867\n",
            "Epoch [3/100], Batch [39/300],  Cross Loss: 0.3680\n",
            "39/300, Train_loss: 0.3680 Train_dice: 0.3680\n",
            "Epoch [3/100], Batch [40/300],  Cross Loss: 0.3980\n",
            "40/300, Train_loss: 0.3980 Train_dice: 0.3980\n",
            "Epoch [3/100], Batch [41/300],  Cross Loss: 0.4747\n",
            "41/300, Train_loss: 0.4747 Train_dice: 0.4747\n",
            "Epoch [3/100], Batch [42/300],  Cross Loss: 0.4188\n",
            "42/300, Train_loss: 0.4188 Train_dice: 0.4188\n",
            "Epoch [3/100], Batch [43/300],  Cross Loss: 0.4098\n",
            "43/300, Train_loss: 0.4098 Train_dice: 0.4098\n",
            "Epoch [3/100], Batch [44/300],  Cross Loss: 0.4624\n",
            "44/300, Train_loss: 0.4624 Train_dice: 0.4624\n",
            "Epoch [3/100], Batch [45/300],  Cross Loss: 0.4001\n",
            "45/300, Train_loss: 0.4001 Train_dice: 0.4001\n",
            "Epoch [3/100], Batch [46/300],  Cross Loss: 0.4085\n",
            "46/300, Train_loss: 0.4085 Train_dice: 0.4085\n",
            "Epoch [3/100], Batch [47/300],  Cross Loss: 0.3672\n",
            "47/300, Train_loss: 0.3672 Train_dice: 0.3672\n",
            "Epoch [3/100], Batch [48/300],  Cross Loss: 0.3676\n",
            "48/300, Train_loss: 0.3676 Train_dice: 0.3676\n",
            "Epoch [3/100], Batch [49/300],  Cross Loss: 0.3744\n",
            "49/300, Train_loss: 0.3744 Train_dice: 0.3744\n",
            "Epoch [3/100], Batch [50/300],  Cross Loss: 0.4003\n",
            "50/300, Train_loss: 0.4003 Train_dice: 0.4003\n",
            "Epoch [3/100], Batch [51/300],  Cross Loss: 0.3283\n",
            "51/300, Train_loss: 0.3283 Train_dice: 0.3283\n",
            "Epoch [3/100], Batch [52/300],  Cross Loss: 0.3680\n",
            "52/300, Train_loss: 0.3680 Train_dice: 0.3680\n",
            "Epoch [3/100], Batch [53/300],  Cross Loss: 0.3683\n",
            "53/300, Train_loss: 0.3683 Train_dice: 0.3683\n",
            "Epoch [3/100], Batch [54/300],  Cross Loss: 0.4248\n",
            "54/300, Train_loss: 0.4248 Train_dice: 0.4248\n",
            "Epoch [3/100], Batch [55/300],  Cross Loss: 0.4064\n",
            "55/300, Train_loss: 0.4064 Train_dice: 0.4064\n",
            "Epoch [3/100], Batch [56/300],  Cross Loss: 0.4180\n",
            "56/300, Train_loss: 0.4180 Train_dice: 0.4180\n",
            "Epoch [3/100], Batch [57/300],  Cross Loss: 0.4464\n",
            "57/300, Train_loss: 0.4464 Train_dice: 0.4464\n",
            "Epoch [3/100], Batch [58/300],  Cross Loss: 0.3235\n",
            "58/300, Train_loss: 0.3235 Train_dice: 0.3235\n",
            "Epoch [3/100], Batch [59/300],  Cross Loss: 0.3639\n",
            "59/300, Train_loss: 0.3639 Train_dice: 0.3639\n",
            "Epoch [3/100], Batch [60/300],  Cross Loss: 0.3976\n",
            "60/300, Train_loss: 0.3976 Train_dice: 0.3976\n",
            "Epoch [3/100], Batch [61/300],  Cross Loss: 0.4055\n",
            "61/300, Train_loss: 0.4055 Train_dice: 0.4055\n",
            "Epoch [3/100], Batch [62/300],  Cross Loss: 0.3938\n",
            "62/300, Train_loss: 0.3938 Train_dice: 0.3938\n",
            "Epoch [3/100], Batch [63/300],  Cross Loss: 0.4114\n",
            "63/300, Train_loss: 0.4114 Train_dice: 0.4114\n",
            "Epoch [3/100], Batch [64/300],  Cross Loss: 0.3544\n",
            "64/300, Train_loss: 0.3544 Train_dice: 0.3544\n",
            "Epoch [3/100], Batch [65/300],  Cross Loss: 0.4605\n",
            "65/300, Train_loss: 0.4605 Train_dice: 0.4605\n",
            "Epoch [3/100], Batch [66/300],  Cross Loss: 0.3504\n",
            "66/300, Train_loss: 0.3504 Train_dice: 0.3504\n",
            "Epoch [3/100], Batch [67/300],  Cross Loss: 0.3985\n",
            "67/300, Train_loss: 0.3985 Train_dice: 0.3985\n",
            "Epoch [3/100], Batch [68/300],  Cross Loss: 0.4065\n",
            "68/300, Train_loss: 0.4065 Train_dice: 0.4065\n",
            "Epoch [3/100], Batch [69/300],  Cross Loss: 0.3704\n",
            "69/300, Train_loss: 0.3704 Train_dice: 0.3704\n",
            "Epoch [3/100], Batch [70/300],  Cross Loss: 0.3798\n",
            "70/300, Train_loss: 0.3798 Train_dice: 0.3798\n",
            "Epoch [3/100], Batch [71/300],  Cross Loss: 0.3967\n",
            "71/300, Train_loss: 0.3967 Train_dice: 0.3967\n",
            "Epoch [3/100], Batch [72/300],  Cross Loss: 0.2943\n",
            "72/300, Train_loss: 0.2943 Train_dice: 0.2943\n",
            "Epoch [3/100], Batch [73/300],  Cross Loss: 0.4403\n",
            "73/300, Train_loss: 0.4403 Train_dice: 0.4403\n",
            "Epoch [3/100], Batch [74/300],  Cross Loss: 0.3859\n",
            "74/300, Train_loss: 0.3859 Train_dice: 0.3859\n",
            "Epoch [3/100], Batch [75/300],  Cross Loss: 0.4365\n",
            "75/300, Train_loss: 0.4365 Train_dice: 0.4365\n",
            "Epoch [3/100], Batch [76/300],  Cross Loss: 0.3879\n",
            "76/300, Train_loss: 0.3879 Train_dice: 0.3879\n",
            "Epoch [3/100], Batch [77/300],  Cross Loss: 0.4496\n",
            "77/300, Train_loss: 0.4496 Train_dice: 0.4496\n",
            "Epoch [3/100], Batch [78/300],  Cross Loss: 0.4671\n",
            "78/300, Train_loss: 0.4671 Train_dice: 0.4671\n",
            "Epoch [3/100], Batch [79/300],  Cross Loss: 0.3691\n",
            "79/300, Train_loss: 0.3691 Train_dice: 0.3691\n",
            "Epoch [3/100], Batch [80/300],  Cross Loss: 0.3898\n",
            "80/300, Train_loss: 0.3898 Train_dice: 0.3898\n",
            "Epoch [3/100], Batch [81/300],  Cross Loss: 0.2702\n",
            "81/300, Train_loss: 0.2702 Train_dice: 0.2702\n",
            "Epoch [3/100], Batch [82/300],  Cross Loss: 0.3901\n",
            "82/300, Train_loss: 0.3901 Train_dice: 0.3901\n",
            "Epoch [3/100], Batch [83/300],  Cross Loss: 0.4158\n",
            "83/300, Train_loss: 0.4158 Train_dice: 0.4158\n",
            "Epoch [3/100], Batch [84/300],  Cross Loss: 0.3659\n",
            "84/300, Train_loss: 0.3659 Train_dice: 0.3659\n",
            "Epoch [3/100], Batch [85/300],  Cross Loss: 0.3844\n",
            "85/300, Train_loss: 0.3844 Train_dice: 0.3844\n",
            "Epoch [3/100], Batch [86/300],  Cross Loss: 0.3751\n",
            "86/300, Train_loss: 0.3751 Train_dice: 0.3751\n",
            "Epoch [3/100], Batch [87/300],  Cross Loss: 0.4012\n",
            "87/300, Train_loss: 0.4012 Train_dice: 0.4012\n",
            "Epoch [3/100], Batch [88/300],  Cross Loss: 0.4465\n",
            "88/300, Train_loss: 0.4465 Train_dice: 0.4465\n",
            "Epoch [3/100], Batch [89/300],  Cross Loss: 0.4005\n",
            "89/300, Train_loss: 0.4005 Train_dice: 0.4005\n",
            "Epoch [3/100], Batch [90/300],  Cross Loss: 0.2457\n",
            "90/300, Train_loss: 0.2457 Train_dice: 0.2457\n",
            "Epoch [3/100], Batch [91/300],  Cross Loss: 0.4692\n",
            "91/300, Train_loss: 0.4692 Train_dice: 0.4692\n",
            "Epoch [3/100], Batch [92/300],  Cross Loss: 0.3659\n",
            "92/300, Train_loss: 0.3659 Train_dice: 0.3659\n",
            "Epoch [3/100], Batch [93/300],  Cross Loss: 0.4215\n",
            "93/300, Train_loss: 0.4215 Train_dice: 0.4215\n",
            "Epoch [3/100], Batch [94/300],  Cross Loss: 0.2272\n",
            "94/300, Train_loss: 0.2272 Train_dice: 0.2272\n",
            "Epoch [3/100], Batch [95/300],  Cross Loss: 0.3856\n",
            "95/300, Train_loss: 0.3856 Train_dice: 0.3856\n",
            "Epoch [3/100], Batch [96/300],  Cross Loss: 0.4307\n",
            "96/300, Train_loss: 0.4307 Train_dice: 0.4307\n",
            "Epoch [3/100], Batch [97/300],  Cross Loss: 0.4268\n",
            "97/300, Train_loss: 0.4268 Train_dice: 0.4268\n",
            "Epoch [3/100], Batch [98/300],  Cross Loss: 0.4614\n",
            "98/300, Train_loss: 0.4614 Train_dice: 0.4614\n",
            "Epoch [3/100], Batch [99/300],  Cross Loss: 0.4668\n",
            "99/300, Train_loss: 0.4668 Train_dice: 0.4668\n",
            "Epoch [3/100], Batch [100/300],  Cross Loss: 0.4925\n",
            "100/300, Train_loss: 0.4925 Train_dice: 0.4925\n",
            "Epoch [3/100], Batch [101/300],  Cross Loss: 0.3926\n",
            "101/300, Train_loss: 0.3926 Train_dice: 0.3926\n",
            "Epoch [3/100], Batch [102/300],  Cross Loss: 0.4309\n",
            "102/300, Train_loss: 0.4309 Train_dice: 0.4309\n",
            "Epoch [3/100], Batch [103/300],  Cross Loss: 0.4569\n",
            "103/300, Train_loss: 0.4569 Train_dice: 0.4569\n",
            "Epoch [3/100], Batch [104/300],  Cross Loss: 0.3698\n",
            "104/300, Train_loss: 0.3698 Train_dice: 0.3698\n",
            "Epoch [3/100], Batch [105/300],  Cross Loss: 0.4488\n",
            "105/300, Train_loss: 0.4488 Train_dice: 0.4488\n",
            "Epoch [3/100], Batch [106/300],  Cross Loss: 0.4139\n",
            "106/300, Train_loss: 0.4139 Train_dice: 0.4139\n",
            "Epoch [3/100], Batch [107/300],  Cross Loss: 0.3724\n",
            "107/300, Train_loss: 0.3724 Train_dice: 0.3724\n",
            "Epoch [3/100], Batch [108/300],  Cross Loss: 0.4605\n",
            "108/300, Train_loss: 0.4605 Train_dice: 0.4605\n",
            "Epoch [3/100], Batch [109/300],  Cross Loss: 0.4449\n",
            "109/300, Train_loss: 0.4449 Train_dice: 0.4449\n",
            "Epoch [3/100], Batch [110/300],  Cross Loss: 0.3647\n",
            "110/300, Train_loss: 0.3647 Train_dice: 0.3647\n",
            "Epoch [3/100], Batch [111/300],  Cross Loss: 0.3748\n",
            "111/300, Train_loss: 0.3748 Train_dice: 0.3748\n",
            "Epoch [3/100], Batch [112/300],  Cross Loss: 0.4319\n",
            "112/300, Train_loss: 0.4319 Train_dice: 0.4319\n",
            "Epoch [3/100], Batch [113/300],  Cross Loss: 0.4325\n",
            "113/300, Train_loss: 0.4325 Train_dice: 0.4325\n",
            "Epoch [3/100], Batch [114/300],  Cross Loss: 0.4186\n",
            "114/300, Train_loss: 0.4186 Train_dice: 0.4186\n",
            "Epoch [3/100], Batch [115/300],  Cross Loss: 0.4261\n",
            "115/300, Train_loss: 0.4261 Train_dice: 0.4261\n",
            "Epoch [3/100], Batch [116/300],  Cross Loss: 0.4168\n",
            "116/300, Train_loss: 0.4168 Train_dice: 0.4168\n",
            "Epoch [3/100], Batch [117/300],  Cross Loss: 0.4315\n",
            "117/300, Train_loss: 0.4315 Train_dice: 0.4315\n",
            "Epoch [3/100], Batch [118/300],  Cross Loss: 0.4647\n",
            "118/300, Train_loss: 0.4647 Train_dice: 0.4647\n",
            "Epoch [3/100], Batch [119/300],  Cross Loss: 0.4413\n",
            "119/300, Train_loss: 0.4413 Train_dice: 0.4413\n",
            "Epoch [3/100], Batch [120/300],  Cross Loss: 0.4308\n",
            "120/300, Train_loss: 0.4308 Train_dice: 0.4308\n",
            "Epoch [3/100], Batch [121/300],  Cross Loss: 0.4066\n",
            "121/300, Train_loss: 0.4066 Train_dice: 0.4066\n",
            "Epoch [3/100], Batch [122/300],  Cross Loss: 0.3574\n",
            "122/300, Train_loss: 0.3574 Train_dice: 0.3574\n",
            "Epoch [3/100], Batch [123/300],  Cross Loss: 0.4000\n",
            "123/300, Train_loss: 0.4000 Train_dice: 0.4000\n",
            "Epoch [3/100], Batch [124/300],  Cross Loss: 0.3674\n",
            "124/300, Train_loss: 0.3674 Train_dice: 0.3674\n",
            "Epoch [3/100], Batch [125/300],  Cross Loss: 0.3675\n",
            "125/300, Train_loss: 0.3675 Train_dice: 0.3675\n",
            "Epoch [3/100], Batch [126/300],  Cross Loss: 0.4247\n",
            "126/300, Train_loss: 0.4247 Train_dice: 0.4247\n",
            "Epoch [3/100], Batch [127/300],  Cross Loss: 0.4403\n",
            "127/300, Train_loss: 0.4403 Train_dice: 0.4403\n",
            "Epoch [3/100], Batch [128/300],  Cross Loss: 0.4030\n",
            "128/300, Train_loss: 0.4030 Train_dice: 0.4030\n",
            "Epoch [3/100], Batch [129/300],  Cross Loss: 0.3860\n",
            "129/300, Train_loss: 0.3860 Train_dice: 0.3860\n",
            "Epoch [3/100], Batch [130/300],  Cross Loss: 0.4386\n",
            "130/300, Train_loss: 0.4386 Train_dice: 0.4386\n",
            "Epoch [3/100], Batch [131/300],  Cross Loss: 0.3883\n",
            "131/300, Train_loss: 0.3883 Train_dice: 0.3883\n",
            "Epoch [3/100], Batch [132/300],  Cross Loss: 0.4536\n",
            "132/300, Train_loss: 0.4536 Train_dice: 0.4536\n",
            "Epoch [3/100], Batch [133/300],  Cross Loss: 0.4243\n",
            "133/300, Train_loss: 0.4243 Train_dice: 0.4243\n",
            "Epoch [3/100], Batch [134/300],  Cross Loss: 0.4773\n",
            "134/300, Train_loss: 0.4773 Train_dice: 0.4773\n",
            "Epoch [3/100], Batch [135/300],  Cross Loss: 0.3698\n",
            "135/300, Train_loss: 0.3698 Train_dice: 0.3698\n",
            "Epoch [3/100], Batch [136/300],  Cross Loss: 0.4219\n",
            "136/300, Train_loss: 0.4219 Train_dice: 0.4219\n",
            "Epoch [3/100], Batch [137/300],  Cross Loss: 0.3662\n",
            "137/300, Train_loss: 0.3662 Train_dice: 0.3662\n",
            "Epoch [3/100], Batch [138/300],  Cross Loss: 0.3716\n",
            "138/300, Train_loss: 0.3716 Train_dice: 0.3716\n",
            "Epoch [3/100], Batch [139/300],  Cross Loss: 0.2452\n",
            "139/300, Train_loss: 0.2452 Train_dice: 0.2452\n",
            "Epoch [3/100], Batch [140/300],  Cross Loss: 0.3737\n",
            "140/300, Train_loss: 0.3737 Train_dice: 0.3737\n",
            "Epoch [3/100], Batch [141/300],  Cross Loss: 0.4190\n",
            "141/300, Train_loss: 0.4190 Train_dice: 0.4190\n",
            "Epoch [3/100], Batch [142/300],  Cross Loss: 0.3755\n",
            "142/300, Train_loss: 0.3755 Train_dice: 0.3755\n",
            "Epoch [3/100], Batch [143/300],  Cross Loss: 0.4850\n",
            "143/300, Train_loss: 0.4850 Train_dice: 0.4850\n",
            "Epoch [3/100], Batch [144/300],  Cross Loss: 0.4133\n",
            "144/300, Train_loss: 0.4133 Train_dice: 0.4133\n",
            "Epoch [3/100], Batch [145/300],  Cross Loss: 0.4142\n",
            "145/300, Train_loss: 0.4142 Train_dice: 0.4142\n",
            "Epoch [3/100], Batch [146/300],  Cross Loss: 0.4607\n",
            "146/300, Train_loss: 0.4607 Train_dice: 0.4607\n",
            "Epoch [3/100], Batch [147/300],  Cross Loss: 0.4213\n",
            "147/300, Train_loss: 0.4213 Train_dice: 0.4213\n",
            "Epoch [3/100], Batch [148/300],  Cross Loss: 0.4722\n",
            "148/300, Train_loss: 0.4722 Train_dice: 0.4722\n",
            "Epoch [3/100], Batch [149/300],  Cross Loss: 0.4621\n",
            "149/300, Train_loss: 0.4621 Train_dice: 0.4621\n",
            "Epoch [3/100], Batch [150/300],  Cross Loss: 0.3544\n",
            "150/300, Train_loss: 0.3544 Train_dice: 0.3544\n",
            "Epoch [3/100], Batch [151/300],  Cross Loss: 0.4181\n",
            "151/300, Train_loss: 0.4181 Train_dice: 0.4181\n",
            "Epoch [3/100], Batch [152/300],  Cross Loss: 0.3872\n",
            "152/300, Train_loss: 0.3872 Train_dice: 0.3872\n",
            "Epoch [3/100], Batch [153/300],  Cross Loss: 0.4637\n",
            "153/300, Train_loss: 0.4637 Train_dice: 0.4637\n",
            "Epoch [3/100], Batch [154/300],  Cross Loss: 0.3428\n",
            "154/300, Train_loss: 0.3428 Train_dice: 0.3428\n",
            "Epoch [3/100], Batch [155/300],  Cross Loss: 0.3901\n",
            "155/300, Train_loss: 0.3901 Train_dice: 0.3901\n",
            "Epoch [3/100], Batch [156/300],  Cross Loss: 0.4186\n",
            "156/300, Train_loss: 0.4186 Train_dice: 0.4186\n",
            "Epoch [3/100], Batch [157/300],  Cross Loss: 0.2397\n",
            "157/300, Train_loss: 0.2397 Train_dice: 0.2397\n",
            "Epoch [3/100], Batch [158/300],  Cross Loss: 0.3557\n",
            "158/300, Train_loss: 0.3557 Train_dice: 0.3557\n",
            "Epoch [3/100], Batch [159/300],  Cross Loss: 0.4247\n",
            "159/300, Train_loss: 0.4247 Train_dice: 0.4247\n",
            "Epoch [3/100], Batch [160/300],  Cross Loss: 0.3356\n",
            "160/300, Train_loss: 0.3356 Train_dice: 0.3356\n",
            "Epoch [3/100], Batch [161/300],  Cross Loss: 0.3776\n",
            "161/300, Train_loss: 0.3776 Train_dice: 0.3776\n",
            "Epoch [3/100], Batch [162/300],  Cross Loss: 0.3690\n",
            "162/300, Train_loss: 0.3690 Train_dice: 0.3690\n",
            "Epoch [3/100], Batch [163/300],  Cross Loss: 0.4806\n",
            "163/300, Train_loss: 0.4806 Train_dice: 0.4806\n",
            "Epoch [3/100], Batch [164/300],  Cross Loss: 0.3251\n",
            "164/300, Train_loss: 0.3251 Train_dice: 0.3251\n",
            "Epoch [3/100], Batch [165/300],  Cross Loss: 0.3911\n",
            "165/300, Train_loss: 0.3911 Train_dice: 0.3911\n",
            "Epoch [3/100], Batch [166/300],  Cross Loss: 0.3478\n",
            "166/300, Train_loss: 0.3478 Train_dice: 0.3478\n",
            "Epoch [3/100], Batch [167/300],  Cross Loss: 0.4340\n",
            "167/300, Train_loss: 0.4340 Train_dice: 0.4340\n",
            "Epoch [3/100], Batch [168/300],  Cross Loss: 0.4248\n",
            "168/300, Train_loss: 0.4248 Train_dice: 0.4248\n",
            "Epoch [3/100], Batch [169/300],  Cross Loss: 0.2110\n",
            "169/300, Train_loss: 0.2110 Train_dice: 0.2110\n",
            "Epoch [3/100], Batch [170/300],  Cross Loss: 0.4019\n",
            "170/300, Train_loss: 0.4019 Train_dice: 0.4019\n",
            "Epoch [3/100], Batch [171/300],  Cross Loss: 0.4468\n",
            "171/300, Train_loss: 0.4468 Train_dice: 0.4468\n",
            "Epoch [3/100], Batch [172/300],  Cross Loss: 0.4294\n",
            "172/300, Train_loss: 0.4294 Train_dice: 0.4294\n",
            "Epoch [3/100], Batch [173/300],  Cross Loss: 0.4446\n",
            "173/300, Train_loss: 0.4446 Train_dice: 0.4446\n",
            "Epoch [3/100], Batch [174/300],  Cross Loss: 0.4297\n",
            "174/300, Train_loss: 0.4297 Train_dice: 0.4297\n",
            "Epoch [3/100], Batch [175/300],  Cross Loss: 0.3949\n",
            "175/300, Train_loss: 0.3949 Train_dice: 0.3949\n",
            "Epoch [3/100], Batch [176/300],  Cross Loss: 0.3607\n",
            "176/300, Train_loss: 0.3607 Train_dice: 0.3607\n",
            "Epoch [3/100], Batch [177/300],  Cross Loss: 0.4291\n",
            "177/300, Train_loss: 0.4291 Train_dice: 0.4291\n",
            "Epoch [3/100], Batch [178/300],  Cross Loss: 0.4572\n",
            "178/300, Train_loss: 0.4572 Train_dice: 0.4572\n",
            "Epoch [3/100], Batch [179/300],  Cross Loss: 0.3744\n",
            "179/300, Train_loss: 0.3744 Train_dice: 0.3744\n",
            "Epoch [3/100], Batch [180/300],  Cross Loss: 0.3948\n",
            "180/300, Train_loss: 0.3948 Train_dice: 0.3948\n",
            "Epoch [3/100], Batch [181/300],  Cross Loss: 0.2910\n",
            "181/300, Train_loss: 0.2910 Train_dice: 0.2910\n",
            "Epoch [3/100], Batch [182/300],  Cross Loss: 0.4355\n",
            "182/300, Train_loss: 0.4355 Train_dice: 0.4355\n",
            "Epoch [3/100], Batch [183/300],  Cross Loss: 0.4260\n",
            "183/300, Train_loss: 0.4260 Train_dice: 0.4260\n",
            "Epoch [3/100], Batch [184/300],  Cross Loss: 0.4097\n",
            "184/300, Train_loss: 0.4097 Train_dice: 0.4097\n",
            "Epoch [3/100], Batch [185/300],  Cross Loss: 0.2889\n",
            "185/300, Train_loss: 0.2889 Train_dice: 0.2889\n",
            "Epoch [3/100], Batch [186/300],  Cross Loss: 0.3916\n",
            "186/300, Train_loss: 0.3916 Train_dice: 0.3916\n",
            "Epoch [3/100], Batch [187/300],  Cross Loss: 0.3152\n",
            "187/300, Train_loss: 0.3152 Train_dice: 0.3152\n",
            "Epoch [3/100], Batch [188/300],  Cross Loss: 0.3247\n",
            "188/300, Train_loss: 0.3247 Train_dice: 0.3247\n",
            "Epoch [3/100], Batch [189/300],  Cross Loss: 0.3938\n",
            "189/300, Train_loss: 0.3938 Train_dice: 0.3938\n",
            "Epoch [3/100], Batch [190/300],  Cross Loss: 0.3521\n",
            "190/300, Train_loss: 0.3521 Train_dice: 0.3521\n",
            "Epoch [3/100], Batch [191/300],  Cross Loss: 0.4277\n",
            "191/300, Train_loss: 0.4277 Train_dice: 0.4277\n",
            "Epoch [3/100], Batch [192/300],  Cross Loss: 0.4862\n",
            "192/300, Train_loss: 0.4862 Train_dice: 0.4862\n",
            "Epoch [3/100], Batch [193/300],  Cross Loss: 0.3666\n",
            "193/300, Train_loss: 0.3666 Train_dice: 0.3666\n",
            "Epoch [3/100], Batch [194/300],  Cross Loss: 0.3498\n",
            "194/300, Train_loss: 0.3498 Train_dice: 0.3498\n",
            "Epoch [3/100], Batch [195/300],  Cross Loss: 0.4403\n",
            "195/300, Train_loss: 0.4403 Train_dice: 0.4403\n",
            "Epoch [3/100], Batch [196/300],  Cross Loss: 0.4345\n",
            "196/300, Train_loss: 0.4345 Train_dice: 0.4345\n",
            "Epoch [3/100], Batch [197/300],  Cross Loss: 0.4500\n",
            "197/300, Train_loss: 0.4500 Train_dice: 0.4500\n",
            "Epoch [3/100], Batch [198/300],  Cross Loss: 0.4523\n",
            "198/300, Train_loss: 0.4523 Train_dice: 0.4523\n",
            "Epoch [3/100], Batch [199/300],  Cross Loss: 0.4898\n",
            "199/300, Train_loss: 0.4898 Train_dice: 0.4898\n",
            "Epoch [3/100], Batch [200/300],  Cross Loss: 0.4205\n",
            "200/300, Train_loss: 0.4205 Train_dice: 0.4205\n",
            "Epoch [3/100], Batch [201/300],  Cross Loss: 0.3947\n",
            "201/300, Train_loss: 0.3947 Train_dice: 0.3947\n",
            "Epoch [3/100], Batch [202/300],  Cross Loss: 0.3133\n",
            "202/300, Train_loss: 0.3133 Train_dice: 0.3133\n",
            "Epoch [3/100], Batch [203/300],  Cross Loss: 0.4316\n",
            "203/300, Train_loss: 0.4316 Train_dice: 0.4316\n",
            "Epoch [3/100], Batch [204/300],  Cross Loss: 0.3791\n",
            "204/300, Train_loss: 0.3791 Train_dice: 0.3791\n",
            "Epoch [3/100], Batch [205/300],  Cross Loss: 0.3784\n",
            "205/300, Train_loss: 0.3784 Train_dice: 0.3784\n",
            "Epoch [3/100], Batch [206/300],  Cross Loss: 0.4203\n",
            "206/300, Train_loss: 0.4203 Train_dice: 0.4203\n",
            "Epoch [3/100], Batch [207/300],  Cross Loss: 0.4544\n",
            "207/300, Train_loss: 0.4544 Train_dice: 0.4544\n",
            "Epoch [3/100], Batch [208/300],  Cross Loss: 0.2715\n",
            "208/300, Train_loss: 0.2715 Train_dice: 0.2715\n",
            "Epoch [3/100], Batch [209/300],  Cross Loss: 0.3828\n",
            "209/300, Train_loss: 0.3828 Train_dice: 0.3828\n",
            "Epoch [3/100], Batch [210/300],  Cross Loss: 0.3947\n",
            "210/300, Train_loss: 0.3947 Train_dice: 0.3947\n",
            "Epoch [3/100], Batch [211/300],  Cross Loss: 0.4366\n",
            "211/300, Train_loss: 0.4366 Train_dice: 0.4366\n",
            "Epoch [3/100], Batch [212/300],  Cross Loss: 0.4492\n",
            "212/300, Train_loss: 0.4492 Train_dice: 0.4492\n",
            "Epoch [3/100], Batch [213/300],  Cross Loss: 0.4379\n",
            "213/300, Train_loss: 0.4379 Train_dice: 0.4379\n",
            "Epoch [3/100], Batch [214/300],  Cross Loss: 0.4470\n",
            "214/300, Train_loss: 0.4470 Train_dice: 0.4470\n",
            "Epoch [3/100], Batch [215/300],  Cross Loss: 0.4328\n",
            "215/300, Train_loss: 0.4328 Train_dice: 0.4328\n",
            "Epoch [3/100], Batch [216/300],  Cross Loss: 0.4039\n",
            "216/300, Train_loss: 0.4039 Train_dice: 0.4039\n",
            "Epoch [3/100], Batch [217/300],  Cross Loss: 0.4314\n",
            "217/300, Train_loss: 0.4314 Train_dice: 0.4314\n",
            "Epoch [3/100], Batch [218/300],  Cross Loss: 0.4029\n",
            "218/300, Train_loss: 0.4029 Train_dice: 0.4029\n",
            "Epoch [3/100], Batch [219/300],  Cross Loss: 0.3721\n",
            "219/300, Train_loss: 0.3721 Train_dice: 0.3721\n",
            "Epoch [3/100], Batch [220/300],  Cross Loss: 0.3937\n",
            "220/300, Train_loss: 0.3937 Train_dice: 0.3937\n",
            "Epoch [3/100], Batch [221/300],  Cross Loss: 0.4688\n",
            "221/300, Train_loss: 0.4688 Train_dice: 0.4688\n",
            "Epoch [3/100], Batch [222/300],  Cross Loss: 0.4627\n",
            "222/300, Train_loss: 0.4627 Train_dice: 0.4627\n",
            "Epoch [3/100], Batch [223/300],  Cross Loss: 0.4148\n",
            "223/300, Train_loss: 0.4148 Train_dice: 0.4148\n",
            "Epoch [3/100], Batch [224/300],  Cross Loss: 0.3179\n",
            "224/300, Train_loss: 0.3179 Train_dice: 0.3179\n",
            "Epoch [3/100], Batch [225/300],  Cross Loss: 0.4205\n",
            "225/300, Train_loss: 0.4205 Train_dice: 0.4205\n",
            "Epoch [3/100], Batch [226/300],  Cross Loss: 0.4224\n",
            "226/300, Train_loss: 0.4224 Train_dice: 0.4224\n",
            "Epoch [3/100], Batch [227/300],  Cross Loss: 0.3748\n",
            "227/300, Train_loss: 0.3748 Train_dice: 0.3748\n",
            "Epoch [3/100], Batch [228/300],  Cross Loss: 0.2338\n",
            "228/300, Train_loss: 0.2338 Train_dice: 0.2338\n",
            "Epoch [3/100], Batch [229/300],  Cross Loss: 0.3901\n",
            "229/300, Train_loss: 0.3901 Train_dice: 0.3901\n",
            "Epoch [3/100], Batch [230/300],  Cross Loss: 0.4362\n",
            "230/300, Train_loss: 0.4362 Train_dice: 0.4362\n",
            "Epoch [3/100], Batch [231/300],  Cross Loss: 0.3537\n",
            "231/300, Train_loss: 0.3537 Train_dice: 0.3537\n",
            "Epoch [3/100], Batch [232/300],  Cross Loss: 0.4089\n",
            "232/300, Train_loss: 0.4089 Train_dice: 0.4089\n",
            "Epoch [3/100], Batch [233/300],  Cross Loss: 0.4213\n",
            "233/300, Train_loss: 0.4213 Train_dice: 0.4213\n",
            "Epoch [3/100], Batch [234/300],  Cross Loss: 0.3981\n",
            "234/300, Train_loss: 0.3981 Train_dice: 0.3981\n",
            "Epoch [3/100], Batch [235/300],  Cross Loss: 0.3551\n",
            "235/300, Train_loss: 0.3551 Train_dice: 0.3551\n",
            "Epoch [3/100], Batch [236/300],  Cross Loss: 0.3115\n",
            "236/300, Train_loss: 0.3115 Train_dice: 0.3115\n",
            "Epoch [3/100], Batch [237/300],  Cross Loss: 0.4958\n",
            "237/300, Train_loss: 0.4958 Train_dice: 0.4958\n",
            "Epoch [3/100], Batch [238/300],  Cross Loss: 0.4173\n",
            "238/300, Train_loss: 0.4173 Train_dice: 0.4173\n",
            "Epoch [3/100], Batch [239/300],  Cross Loss: 0.4595\n",
            "239/300, Train_loss: 0.4595 Train_dice: 0.4595\n",
            "Epoch [3/100], Batch [240/300],  Cross Loss: 0.4777\n",
            "240/300, Train_loss: 0.4777 Train_dice: 0.4777\n",
            "Epoch [3/100], Batch [241/300],  Cross Loss: 0.4395\n",
            "241/300, Train_loss: 0.4395 Train_dice: 0.4395\n",
            "Epoch [3/100], Batch [242/300],  Cross Loss: 0.3767\n",
            "242/300, Train_loss: 0.3767 Train_dice: 0.3767\n",
            "Epoch [3/100], Batch [243/300],  Cross Loss: 0.4606\n",
            "243/300, Train_loss: 0.4606 Train_dice: 0.4606\n",
            "Epoch [3/100], Batch [244/300],  Cross Loss: 0.3704\n",
            "244/300, Train_loss: 0.3704 Train_dice: 0.3704\n",
            "Epoch [3/100], Batch [245/300],  Cross Loss: 0.3460\n",
            "245/300, Train_loss: 0.3460 Train_dice: 0.3460\n",
            "Epoch [3/100], Batch [246/300],  Cross Loss: 0.4098\n",
            "246/300, Train_loss: 0.4098 Train_dice: 0.4098\n",
            "Epoch [3/100], Batch [247/300],  Cross Loss: 0.4108\n",
            "247/300, Train_loss: 0.4108 Train_dice: 0.4108\n",
            "Epoch [3/100], Batch [248/300],  Cross Loss: 0.3747\n",
            "248/300, Train_loss: 0.3747 Train_dice: 0.3747\n",
            "Epoch [3/100], Batch [249/300],  Cross Loss: 0.4302\n",
            "249/300, Train_loss: 0.4302 Train_dice: 0.4302\n",
            "Epoch [3/100], Batch [250/300],  Cross Loss: 0.4083\n",
            "250/300, Train_loss: 0.4083 Train_dice: 0.4083\n",
            "Epoch [3/100], Batch [251/300],  Cross Loss: 0.3518\n",
            "251/300, Train_loss: 0.3518 Train_dice: 0.3518\n",
            "Epoch [3/100], Batch [252/300],  Cross Loss: 0.4000\n",
            "252/300, Train_loss: 0.4000 Train_dice: 0.4000\n",
            "Epoch [3/100], Batch [253/300],  Cross Loss: 0.4240\n",
            "253/300, Train_loss: 0.4240 Train_dice: 0.4240\n",
            "Epoch [3/100], Batch [254/300],  Cross Loss: 0.4022\n",
            "254/300, Train_loss: 0.4022 Train_dice: 0.4022\n",
            "Epoch [3/100], Batch [255/300],  Cross Loss: 0.3937\n",
            "255/300, Train_loss: 0.3937 Train_dice: 0.3937\n",
            "Epoch [3/100], Batch [256/300],  Cross Loss: 0.4732\n",
            "256/300, Train_loss: 0.4732 Train_dice: 0.4732\n",
            "Epoch [3/100], Batch [257/300],  Cross Loss: 0.4286\n",
            "257/300, Train_loss: 0.4286 Train_dice: 0.4286\n",
            "Epoch [3/100], Batch [258/300],  Cross Loss: 0.3857\n",
            "258/300, Train_loss: 0.3857 Train_dice: 0.3857\n",
            "Epoch [3/100], Batch [259/300],  Cross Loss: 0.4057\n",
            "259/300, Train_loss: 0.4057 Train_dice: 0.4057\n",
            "Epoch [3/100], Batch [260/300],  Cross Loss: 0.4006\n",
            "260/300, Train_loss: 0.4006 Train_dice: 0.4006\n",
            "Epoch [3/100], Batch [261/300],  Cross Loss: 0.3815\n",
            "261/300, Train_loss: 0.3815 Train_dice: 0.3815\n",
            "Epoch [3/100], Batch [262/300],  Cross Loss: 0.4052\n",
            "262/300, Train_loss: 0.4052 Train_dice: 0.4052\n",
            "Epoch [3/100], Batch [263/300],  Cross Loss: 0.3604\n",
            "263/300, Train_loss: 0.3604 Train_dice: 0.3604\n",
            "Epoch [3/100], Batch [264/300],  Cross Loss: 0.3913\n",
            "264/300, Train_loss: 0.3913 Train_dice: 0.3913\n",
            "Epoch [3/100], Batch [265/300],  Cross Loss: 0.4699\n",
            "265/300, Train_loss: 0.4699 Train_dice: 0.4699\n",
            "Epoch [3/100], Batch [266/300],  Cross Loss: 0.4554\n",
            "266/300, Train_loss: 0.4554 Train_dice: 0.4554\n",
            "Epoch [3/100], Batch [267/300],  Cross Loss: 0.3783\n",
            "267/300, Train_loss: 0.3783 Train_dice: 0.3783\n",
            "Epoch [3/100], Batch [268/300],  Cross Loss: 0.4633\n",
            "268/300, Train_loss: 0.4633 Train_dice: 0.4633\n",
            "Epoch [3/100], Batch [269/300],  Cross Loss: 0.3948\n",
            "269/300, Train_loss: 0.3948 Train_dice: 0.3948\n",
            "Epoch [3/100], Batch [270/300],  Cross Loss: 0.3511\n",
            "270/300, Train_loss: 0.3511 Train_dice: 0.3511\n",
            "Epoch [3/100], Batch [271/300],  Cross Loss: 0.3724\n",
            "271/300, Train_loss: 0.3724 Train_dice: 0.3724\n",
            "Epoch [3/100], Batch [272/300],  Cross Loss: 0.4326\n",
            "272/300, Train_loss: 0.4326 Train_dice: 0.4326\n",
            "Epoch [3/100], Batch [273/300],  Cross Loss: 0.3793\n",
            "273/300, Train_loss: 0.3793 Train_dice: 0.3793\n",
            "Epoch [3/100], Batch [274/300],  Cross Loss: 0.3781\n",
            "274/300, Train_loss: 0.3781 Train_dice: 0.3781\n",
            "Epoch [3/100], Batch [275/300],  Cross Loss: 0.4109\n",
            "275/300, Train_loss: 0.4109 Train_dice: 0.4109\n",
            "Epoch [3/100], Batch [276/300],  Cross Loss: 0.4071\n",
            "276/300, Train_loss: 0.4071 Train_dice: 0.4071\n",
            "Epoch [3/100], Batch [277/300],  Cross Loss: 0.4384\n",
            "277/300, Train_loss: 0.4384 Train_dice: 0.4384\n",
            "Epoch [3/100], Batch [278/300],  Cross Loss: 0.4531\n",
            "278/300, Train_loss: 0.4531 Train_dice: 0.4531\n",
            "Epoch [3/100], Batch [279/300],  Cross Loss: 0.3720\n",
            "279/300, Train_loss: 0.3720 Train_dice: 0.3720\n",
            "Epoch [3/100], Batch [280/300],  Cross Loss: 0.3227\n",
            "280/300, Train_loss: 0.3227 Train_dice: 0.3227\n",
            "Epoch [3/100], Batch [281/300],  Cross Loss: 0.3542\n",
            "281/300, Train_loss: 0.3542 Train_dice: 0.3542\n",
            "Epoch [3/100], Batch [282/300],  Cross Loss: 0.4661\n",
            "282/300, Train_loss: 0.4661 Train_dice: 0.4661\n",
            "Epoch [3/100], Batch [283/300],  Cross Loss: 0.4263\n",
            "283/300, Train_loss: 0.4263 Train_dice: 0.4263\n",
            "Epoch [3/100], Batch [284/300],  Cross Loss: 0.2419\n",
            "284/300, Train_loss: 0.2419 Train_dice: 0.2419\n",
            "Epoch [3/100], Batch [285/300],  Cross Loss: 0.4670\n",
            "285/300, Train_loss: 0.4670 Train_dice: 0.4670\n",
            "Epoch [3/100], Batch [286/300],  Cross Loss: 0.5142\n",
            "286/300, Train_loss: 0.5142 Train_dice: 0.5142\n",
            "Epoch [3/100], Batch [287/300],  Cross Loss: 0.4603\n",
            "287/300, Train_loss: 0.4603 Train_dice: 0.4603\n",
            "Epoch [3/100], Batch [288/300],  Cross Loss: 0.4154\n",
            "288/300, Train_loss: 0.4154 Train_dice: 0.4154\n",
            "Epoch [3/100], Batch [289/300],  Cross Loss: 0.3954\n",
            "289/300, Train_loss: 0.3954 Train_dice: 0.3954\n",
            "Epoch [3/100], Batch [290/300],  Cross Loss: 0.4206\n",
            "290/300, Train_loss: 0.4206 Train_dice: 0.4206\n",
            "Epoch [3/100], Batch [291/300],  Cross Loss: 0.3442\n",
            "291/300, Train_loss: 0.3442 Train_dice: 0.3442\n",
            "Epoch [3/100], Batch [292/300],  Cross Loss: 0.4476\n",
            "292/300, Train_loss: 0.4476 Train_dice: 0.4476\n",
            "Epoch [3/100], Batch [293/300],  Cross Loss: 0.4675\n",
            "293/300, Train_loss: 0.4675 Train_dice: 0.4675\n",
            "Epoch [3/100], Batch [294/300],  Cross Loss: 0.3994\n",
            "294/300, Train_loss: 0.3994 Train_dice: 0.3994\n",
            "Epoch [3/100], Batch [295/300],  Cross Loss: 0.4013\n",
            "295/300, Train_loss: 0.4013 Train_dice: 0.4013\n",
            "Epoch [3/100], Batch [296/300],  Cross Loss: 0.3332\n",
            "296/300, Train_loss: 0.3332 Train_dice: 0.3332\n",
            "Epoch [3/100], Batch [297/300],  Cross Loss: 0.3680\n",
            "297/300, Train_loss: 0.3680 Train_dice: 0.3680\n",
            "Epoch [3/100], Batch [298/300],  Cross Loss: 0.4111\n",
            "298/300, Train_loss: 0.4111 Train_dice: 0.4111\n",
            "Epoch [3/100], Batch [299/300],  Cross Loss: 0.4152\n",
            "299/300, Train_loss: 0.4152 Train_dice: 0.4152\n",
            "Epoch [3/100], Batch [300/300],  Cross Loss: 0.3941\n",
            "300/300, Train_loss: 0.3941 Train_dice: 0.3941\n",
            "--------------------\n",
            "Epoch_loss: 0.4011\n",
            "Epoch_metric: tensor(0.4011, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 4/100\n",
            "Epoch [4/100], Batch [1/300],  Cross Loss: 0.4158\n",
            "1/300, Train_loss: 0.4158 Train_dice: 0.4158\n",
            "Epoch [4/100], Batch [2/300],  Cross Loss: 0.4374\n",
            "2/300, Train_loss: 0.4374 Train_dice: 0.4374\n",
            "Epoch [4/100], Batch [3/300],  Cross Loss: 0.4426\n",
            "3/300, Train_loss: 0.4426 Train_dice: 0.4426\n",
            "Epoch [4/100], Batch [4/300],  Cross Loss: 0.4375\n",
            "4/300, Train_loss: 0.4375 Train_dice: 0.4375\n",
            "Epoch [4/100], Batch [5/300],  Cross Loss: 0.3729\n",
            "5/300, Train_loss: 0.3729 Train_dice: 0.3729\n",
            "Epoch [4/100], Batch [6/300],  Cross Loss: 0.4133\n",
            "6/300, Train_loss: 0.4133 Train_dice: 0.4133\n",
            "Epoch [4/100], Batch [7/300],  Cross Loss: 0.4335\n",
            "7/300, Train_loss: 0.4335 Train_dice: 0.4335\n",
            "Epoch [4/100], Batch [8/300],  Cross Loss: 0.3840\n",
            "8/300, Train_loss: 0.3840 Train_dice: 0.3840\n",
            "Epoch [4/100], Batch [9/300],  Cross Loss: 0.3404\n",
            "9/300, Train_loss: 0.3404 Train_dice: 0.3404\n",
            "Epoch [4/100], Batch [10/300],  Cross Loss: 0.4371\n",
            "10/300, Train_loss: 0.4371 Train_dice: 0.4371\n",
            "Epoch [4/100], Batch [11/300],  Cross Loss: 0.4063\n",
            "11/300, Train_loss: 0.4063 Train_dice: 0.4063\n",
            "Epoch [4/100], Batch [12/300],  Cross Loss: 0.3916\n",
            "12/300, Train_loss: 0.3916 Train_dice: 0.3916\n",
            "Epoch [4/100], Batch [13/300],  Cross Loss: 0.3827\n",
            "13/300, Train_loss: 0.3827 Train_dice: 0.3827\n",
            "Epoch [4/100], Batch [14/300],  Cross Loss: 0.3782\n",
            "14/300, Train_loss: 0.3782 Train_dice: 0.3782\n",
            "Epoch [4/100], Batch [15/300],  Cross Loss: 0.2929\n",
            "15/300, Train_loss: 0.2929 Train_dice: 0.2929\n",
            "Epoch [4/100], Batch [16/300],  Cross Loss: 0.3712\n",
            "16/300, Train_loss: 0.3712 Train_dice: 0.3712\n",
            "Epoch [4/100], Batch [17/300],  Cross Loss: 0.4333\n",
            "17/300, Train_loss: 0.4333 Train_dice: 0.4333\n",
            "Epoch [4/100], Batch [18/300],  Cross Loss: 0.4278\n",
            "18/300, Train_loss: 0.4278 Train_dice: 0.4278\n",
            "Epoch [4/100], Batch [19/300],  Cross Loss: 0.3062\n",
            "19/300, Train_loss: 0.3062 Train_dice: 0.3062\n",
            "Epoch [4/100], Batch [20/300],  Cross Loss: 0.4344\n",
            "20/300, Train_loss: 0.4344 Train_dice: 0.4344\n",
            "Epoch [4/100], Batch [21/300],  Cross Loss: 0.3972\n",
            "21/300, Train_loss: 0.3972 Train_dice: 0.3972\n",
            "Epoch [4/100], Batch [22/300],  Cross Loss: 0.3541\n",
            "22/300, Train_loss: 0.3541 Train_dice: 0.3541\n",
            "Epoch [4/100], Batch [23/300],  Cross Loss: 0.3598\n",
            "23/300, Train_loss: 0.3598 Train_dice: 0.3598\n",
            "Epoch [4/100], Batch [24/300],  Cross Loss: 0.3645\n",
            "24/300, Train_loss: 0.3645 Train_dice: 0.3645\n",
            "Epoch [4/100], Batch [25/300],  Cross Loss: 0.4427\n",
            "25/300, Train_loss: 0.4427 Train_dice: 0.4427\n",
            "Epoch [4/100], Batch [26/300],  Cross Loss: 0.3598\n",
            "26/300, Train_loss: 0.3598 Train_dice: 0.3598\n",
            "Epoch [4/100], Batch [27/300],  Cross Loss: 0.3904\n",
            "27/300, Train_loss: 0.3904 Train_dice: 0.3904\n",
            "Epoch [4/100], Batch [28/300],  Cross Loss: 0.3683\n",
            "28/300, Train_loss: 0.3683 Train_dice: 0.3683\n",
            "Epoch [4/100], Batch [29/300],  Cross Loss: 0.4407\n",
            "29/300, Train_loss: 0.4407 Train_dice: 0.4407\n",
            "Epoch [4/100], Batch [30/300],  Cross Loss: 0.3497\n",
            "30/300, Train_loss: 0.3497 Train_dice: 0.3497\n",
            "Epoch [4/100], Batch [31/300],  Cross Loss: 0.3723\n",
            "31/300, Train_loss: 0.3723 Train_dice: 0.3723\n",
            "Epoch [4/100], Batch [32/300],  Cross Loss: 0.3549\n",
            "32/300, Train_loss: 0.3549 Train_dice: 0.3549\n",
            "Epoch [4/100], Batch [33/300],  Cross Loss: 0.3724\n",
            "33/300, Train_loss: 0.3724 Train_dice: 0.3724\n",
            "Epoch [4/100], Batch [34/300],  Cross Loss: 0.4038\n",
            "34/300, Train_loss: 0.4038 Train_dice: 0.4038\n",
            "Epoch [4/100], Batch [35/300],  Cross Loss: 0.4152\n",
            "35/300, Train_loss: 0.4152 Train_dice: 0.4152\n",
            "Epoch [4/100], Batch [36/300],  Cross Loss: 0.4684\n",
            "36/300, Train_loss: 0.4684 Train_dice: 0.4684\n",
            "Epoch [4/100], Batch [37/300],  Cross Loss: 0.3842\n",
            "37/300, Train_loss: 0.3842 Train_dice: 0.3842\n",
            "Epoch [4/100], Batch [38/300],  Cross Loss: 0.3612\n",
            "38/300, Train_loss: 0.3612 Train_dice: 0.3612\n",
            "Epoch [4/100], Batch [39/300],  Cross Loss: 0.4952\n",
            "39/300, Train_loss: 0.4952 Train_dice: 0.4952\n",
            "Epoch [4/100], Batch [40/300],  Cross Loss: 0.4591\n",
            "40/300, Train_loss: 0.4591 Train_dice: 0.4591\n",
            "Epoch [4/100], Batch [41/300],  Cross Loss: 0.2274\n",
            "41/300, Train_loss: 0.2274 Train_dice: 0.2274\n",
            "Epoch [4/100], Batch [42/300],  Cross Loss: 0.3258\n",
            "42/300, Train_loss: 0.3258 Train_dice: 0.3258\n",
            "Epoch [4/100], Batch [43/300],  Cross Loss: 0.3716\n",
            "43/300, Train_loss: 0.3716 Train_dice: 0.3716\n",
            "Epoch [4/100], Batch [44/300],  Cross Loss: 0.3719\n",
            "44/300, Train_loss: 0.3719 Train_dice: 0.3719\n",
            "Epoch [4/100], Batch [45/300],  Cross Loss: 0.1864\n",
            "45/300, Train_loss: 0.1864 Train_dice: 0.1864\n",
            "Epoch [4/100], Batch [46/300],  Cross Loss: 0.4969\n",
            "46/300, Train_loss: 0.4969 Train_dice: 0.4969\n",
            "Epoch [4/100], Batch [47/300],  Cross Loss: 0.4544\n",
            "47/300, Train_loss: 0.4544 Train_dice: 0.4544\n",
            "Epoch [4/100], Batch [48/300],  Cross Loss: 0.3865\n",
            "48/300, Train_loss: 0.3865 Train_dice: 0.3865\n",
            "Epoch [4/100], Batch [49/300],  Cross Loss: 0.3404\n",
            "49/300, Train_loss: 0.3404 Train_dice: 0.3404\n",
            "Epoch [4/100], Batch [50/300],  Cross Loss: 0.4545\n",
            "50/300, Train_loss: 0.4545 Train_dice: 0.4545\n",
            "Epoch [4/100], Batch [51/300],  Cross Loss: 0.4507\n",
            "51/300, Train_loss: 0.4507 Train_dice: 0.4507\n",
            "Epoch [4/100], Batch [52/300],  Cross Loss: 0.3239\n",
            "52/300, Train_loss: 0.3239 Train_dice: 0.3239\n",
            "Epoch [4/100], Batch [53/300],  Cross Loss: 0.2473\n",
            "53/300, Train_loss: 0.2473 Train_dice: 0.2473\n",
            "Epoch [4/100], Batch [54/300],  Cross Loss: 0.4209\n",
            "54/300, Train_loss: 0.4209 Train_dice: 0.4209\n",
            "Epoch [4/100], Batch [55/300],  Cross Loss: 0.5238\n",
            "55/300, Train_loss: 0.5238 Train_dice: 0.5238\n",
            "Epoch [4/100], Batch [56/300],  Cross Loss: 0.4459\n",
            "56/300, Train_loss: 0.4459 Train_dice: 0.4459\n",
            "Epoch [4/100], Batch [57/300],  Cross Loss: 0.4735\n",
            "57/300, Train_loss: 0.4735 Train_dice: 0.4735\n",
            "Epoch [4/100], Batch [58/300],  Cross Loss: 0.4371\n",
            "58/300, Train_loss: 0.4371 Train_dice: 0.4371\n",
            "Epoch [4/100], Batch [59/300],  Cross Loss: 0.3422\n",
            "59/300, Train_loss: 0.3422 Train_dice: 0.3422\n",
            "Epoch [4/100], Batch [60/300],  Cross Loss: 0.3723\n",
            "60/300, Train_loss: 0.3723 Train_dice: 0.3723\n",
            "Epoch [4/100], Batch [61/300],  Cross Loss: 0.2010\n",
            "61/300, Train_loss: 0.2010 Train_dice: 0.2010\n",
            "Epoch [4/100], Batch [62/300],  Cross Loss: 0.4775\n",
            "62/300, Train_loss: 0.4775 Train_dice: 0.4775\n",
            "Epoch [4/100], Batch [63/300],  Cross Loss: 0.4215\n",
            "63/300, Train_loss: 0.4215 Train_dice: 0.4215\n",
            "Epoch [4/100], Batch [64/300],  Cross Loss: 0.4544\n",
            "64/300, Train_loss: 0.4544 Train_dice: 0.4544\n",
            "Epoch [4/100], Batch [65/300],  Cross Loss: 0.4426\n",
            "65/300, Train_loss: 0.4426 Train_dice: 0.4426\n",
            "Epoch [4/100], Batch [66/300],  Cross Loss: 0.3755\n",
            "66/300, Train_loss: 0.3755 Train_dice: 0.3755\n",
            "Epoch [4/100], Batch [67/300],  Cross Loss: 0.1578\n",
            "67/300, Train_loss: 0.1578 Train_dice: 0.1578\n",
            "Epoch [4/100], Batch [68/300],  Cross Loss: 0.4526\n",
            "68/300, Train_loss: 0.4526 Train_dice: 0.4526\n",
            "Epoch [4/100], Batch [69/300],  Cross Loss: 0.3787\n",
            "69/300, Train_loss: 0.3787 Train_dice: 0.3787\n",
            "Epoch [4/100], Batch [70/300],  Cross Loss: 0.4310\n",
            "70/300, Train_loss: 0.4310 Train_dice: 0.4310\n",
            "Epoch [4/100], Batch [71/300],  Cross Loss: 0.4052\n",
            "71/300, Train_loss: 0.4052 Train_dice: 0.4052\n",
            "Epoch [4/100], Batch [72/300],  Cross Loss: 0.4398\n",
            "72/300, Train_loss: 0.4398 Train_dice: 0.4398\n",
            "Epoch [4/100], Batch [73/300],  Cross Loss: 0.4411\n",
            "73/300, Train_loss: 0.4411 Train_dice: 0.4411\n",
            "Epoch [4/100], Batch [74/300],  Cross Loss: 0.4335\n",
            "74/300, Train_loss: 0.4335 Train_dice: 0.4335\n",
            "Epoch [4/100], Batch [75/300],  Cross Loss: 0.4783\n",
            "75/300, Train_loss: 0.4783 Train_dice: 0.4783\n",
            "Epoch [4/100], Batch [76/300],  Cross Loss: 0.4389\n",
            "76/300, Train_loss: 0.4389 Train_dice: 0.4389\n",
            "Epoch [4/100], Batch [77/300],  Cross Loss: 0.2215\n",
            "77/300, Train_loss: 0.2215 Train_dice: 0.2215\n",
            "Epoch [4/100], Batch [78/300],  Cross Loss: 0.4339\n",
            "78/300, Train_loss: 0.4339 Train_dice: 0.4339\n",
            "Epoch [4/100], Batch [79/300],  Cross Loss: 0.4078\n",
            "79/300, Train_loss: 0.4078 Train_dice: 0.4078\n",
            "Epoch [4/100], Batch [80/300],  Cross Loss: 0.3914\n",
            "80/300, Train_loss: 0.3914 Train_dice: 0.3914\n",
            "Epoch [4/100], Batch [81/300],  Cross Loss: 0.3884\n",
            "81/300, Train_loss: 0.3884 Train_dice: 0.3884\n",
            "Epoch [4/100], Batch [82/300],  Cross Loss: 0.2524\n",
            "82/300, Train_loss: 0.2524 Train_dice: 0.2524\n",
            "Epoch [4/100], Batch [83/300],  Cross Loss: 0.4211\n",
            "83/300, Train_loss: 0.4211 Train_dice: 0.4211\n",
            "Epoch [4/100], Batch [84/300],  Cross Loss: 0.4277\n",
            "84/300, Train_loss: 0.4277 Train_dice: 0.4277\n",
            "Epoch [4/100], Batch [85/300],  Cross Loss: 0.3667\n",
            "85/300, Train_loss: 0.3667 Train_dice: 0.3667\n",
            "Epoch [4/100], Batch [86/300],  Cross Loss: 0.3879\n",
            "86/300, Train_loss: 0.3879 Train_dice: 0.3879\n",
            "Epoch [4/100], Batch [87/300],  Cross Loss: 0.3639\n",
            "87/300, Train_loss: 0.3639 Train_dice: 0.3639\n",
            "Epoch [4/100], Batch [88/300],  Cross Loss: 0.3482\n",
            "88/300, Train_loss: 0.3482 Train_dice: 0.3482\n",
            "Epoch [4/100], Batch [89/300],  Cross Loss: 0.4065\n",
            "89/300, Train_loss: 0.4065 Train_dice: 0.4065\n",
            "Epoch [4/100], Batch [90/300],  Cross Loss: 0.4190\n",
            "90/300, Train_loss: 0.4190 Train_dice: 0.4190\n",
            "Epoch [4/100], Batch [91/300],  Cross Loss: 0.3652\n",
            "91/300, Train_loss: 0.3652 Train_dice: 0.3652\n",
            "Epoch [4/100], Batch [92/300],  Cross Loss: 0.3929\n",
            "92/300, Train_loss: 0.3929 Train_dice: 0.3929\n",
            "Epoch [4/100], Batch [93/300],  Cross Loss: 0.4197\n",
            "93/300, Train_loss: 0.4197 Train_dice: 0.4197\n",
            "Epoch [4/100], Batch [94/300],  Cross Loss: 0.2826\n",
            "94/300, Train_loss: 0.2826 Train_dice: 0.2826\n",
            "Epoch [4/100], Batch [95/300],  Cross Loss: 0.3852\n",
            "95/300, Train_loss: 0.3852 Train_dice: 0.3852\n",
            "Epoch [4/100], Batch [96/300],  Cross Loss: 0.4151\n",
            "96/300, Train_loss: 0.4151 Train_dice: 0.4151\n",
            "Epoch [4/100], Batch [97/300],  Cross Loss: 0.4057\n",
            "97/300, Train_loss: 0.4057 Train_dice: 0.4057\n",
            "Epoch [4/100], Batch [98/300],  Cross Loss: 0.3897\n",
            "98/300, Train_loss: 0.3897 Train_dice: 0.3897\n",
            "Epoch [4/100], Batch [99/300],  Cross Loss: 0.4075\n",
            "99/300, Train_loss: 0.4075 Train_dice: 0.4075\n",
            "Epoch [4/100], Batch [100/300],  Cross Loss: 0.4409\n",
            "100/300, Train_loss: 0.4409 Train_dice: 0.4409\n",
            "Epoch [4/100], Batch [101/300],  Cross Loss: 0.4848\n",
            "101/300, Train_loss: 0.4848 Train_dice: 0.4848\n",
            "Epoch [4/100], Batch [102/300],  Cross Loss: 0.4012\n",
            "102/300, Train_loss: 0.4012 Train_dice: 0.4012\n",
            "Epoch [4/100], Batch [103/300],  Cross Loss: 0.4664\n",
            "103/300, Train_loss: 0.4664 Train_dice: 0.4664\n",
            "Epoch [4/100], Batch [104/300],  Cross Loss: 0.4864\n",
            "104/300, Train_loss: 0.4864 Train_dice: 0.4864\n",
            "Epoch [4/100], Batch [105/300],  Cross Loss: 0.3726\n",
            "105/300, Train_loss: 0.3726 Train_dice: 0.3726\n",
            "Epoch [4/100], Batch [106/300],  Cross Loss: 0.5079\n",
            "106/300, Train_loss: 0.5079 Train_dice: 0.5079\n",
            "Epoch [4/100], Batch [107/300],  Cross Loss: 0.4918\n",
            "107/300, Train_loss: 0.4918 Train_dice: 0.4918\n",
            "Epoch [4/100], Batch [108/300],  Cross Loss: 0.4091\n",
            "108/300, Train_loss: 0.4091 Train_dice: 0.4091\n",
            "Epoch [4/100], Batch [109/300],  Cross Loss: 0.4135\n",
            "109/300, Train_loss: 0.4135 Train_dice: 0.4135\n",
            "Epoch [4/100], Batch [110/300],  Cross Loss: 0.4249\n",
            "110/300, Train_loss: 0.4249 Train_dice: 0.4249\n",
            "Epoch [4/100], Batch [111/300],  Cross Loss: 0.4087\n",
            "111/300, Train_loss: 0.4087 Train_dice: 0.4087\n",
            "Epoch [4/100], Batch [112/300],  Cross Loss: 0.2619\n",
            "112/300, Train_loss: 0.2619 Train_dice: 0.2619\n",
            "Epoch [4/100], Batch [113/300],  Cross Loss: 0.4103\n",
            "113/300, Train_loss: 0.4103 Train_dice: 0.4103\n",
            "Epoch [4/100], Batch [114/300],  Cross Loss: 0.3605\n",
            "114/300, Train_loss: 0.3605 Train_dice: 0.3605\n",
            "Epoch [4/100], Batch [115/300],  Cross Loss: 0.3733\n",
            "115/300, Train_loss: 0.3733 Train_dice: 0.3733\n",
            "Epoch [4/100], Batch [116/300],  Cross Loss: 0.4815\n",
            "116/300, Train_loss: 0.4815 Train_dice: 0.4815\n",
            "Epoch [4/100], Batch [117/300],  Cross Loss: 0.3393\n",
            "117/300, Train_loss: 0.3393 Train_dice: 0.3393\n",
            "Epoch [4/100], Batch [118/300],  Cross Loss: 0.4455\n",
            "118/300, Train_loss: 0.4455 Train_dice: 0.4455\n",
            "Epoch [4/100], Batch [119/300],  Cross Loss: 0.3583\n",
            "119/300, Train_loss: 0.3583 Train_dice: 0.3583\n",
            "Epoch [4/100], Batch [120/300],  Cross Loss: 0.3162\n",
            "120/300, Train_loss: 0.3162 Train_dice: 0.3162\n",
            "Epoch [4/100], Batch [121/300],  Cross Loss: 0.3399\n",
            "121/300, Train_loss: 0.3399 Train_dice: 0.3399\n",
            "Epoch [4/100], Batch [122/300],  Cross Loss: 0.4585\n",
            "122/300, Train_loss: 0.4585 Train_dice: 0.4585\n",
            "Epoch [4/100], Batch [123/300],  Cross Loss: 0.4703\n",
            "123/300, Train_loss: 0.4703 Train_dice: 0.4703\n",
            "Epoch [4/100], Batch [124/300],  Cross Loss: 0.4241\n",
            "124/300, Train_loss: 0.4241 Train_dice: 0.4241\n",
            "Epoch [4/100], Batch [125/300],  Cross Loss: 0.4099\n",
            "125/300, Train_loss: 0.4099 Train_dice: 0.4099\n",
            "Epoch [4/100], Batch [126/300],  Cross Loss: 0.3381\n",
            "126/300, Train_loss: 0.3381 Train_dice: 0.3381\n",
            "Epoch [4/100], Batch [127/300],  Cross Loss: 0.3972\n",
            "127/300, Train_loss: 0.3972 Train_dice: 0.3972\n",
            "Epoch [4/100], Batch [128/300],  Cross Loss: 0.4128\n",
            "128/300, Train_loss: 0.4128 Train_dice: 0.4128\n",
            "Epoch [4/100], Batch [129/300],  Cross Loss: 0.3300\n",
            "129/300, Train_loss: 0.3300 Train_dice: 0.3300\n",
            "Epoch [4/100], Batch [130/300],  Cross Loss: 0.3799\n",
            "130/300, Train_loss: 0.3799 Train_dice: 0.3799\n",
            "Epoch [4/100], Batch [131/300],  Cross Loss: 0.3728\n",
            "131/300, Train_loss: 0.3728 Train_dice: 0.3728\n",
            "Epoch [4/100], Batch [132/300],  Cross Loss: 0.4065\n",
            "132/300, Train_loss: 0.4065 Train_dice: 0.4065\n",
            "Epoch [4/100], Batch [133/300],  Cross Loss: 0.3183\n",
            "133/300, Train_loss: 0.3183 Train_dice: 0.3183\n",
            "Epoch [4/100], Batch [134/300],  Cross Loss: 0.4561\n",
            "134/300, Train_loss: 0.4561 Train_dice: 0.4561\n",
            "Epoch [4/100], Batch [135/300],  Cross Loss: 0.3107\n",
            "135/300, Train_loss: 0.3107 Train_dice: 0.3107\n",
            "Epoch [4/100], Batch [136/300],  Cross Loss: 0.3963\n",
            "136/300, Train_loss: 0.3963 Train_dice: 0.3963\n",
            "Epoch [4/100], Batch [137/300],  Cross Loss: 0.3893\n",
            "137/300, Train_loss: 0.3893 Train_dice: 0.3893\n",
            "Epoch [4/100], Batch [138/300],  Cross Loss: 0.3486\n",
            "138/300, Train_loss: 0.3486 Train_dice: 0.3486\n",
            "Epoch [4/100], Batch [139/300],  Cross Loss: 0.4142\n",
            "139/300, Train_loss: 0.4142 Train_dice: 0.4142\n",
            "Epoch [4/100], Batch [140/300],  Cross Loss: 0.4741\n",
            "140/300, Train_loss: 0.4741 Train_dice: 0.4741\n",
            "Epoch [4/100], Batch [141/300],  Cross Loss: 0.3607\n",
            "141/300, Train_loss: 0.3607 Train_dice: 0.3607\n",
            "Epoch [4/100], Batch [142/300],  Cross Loss: 0.3507\n",
            "142/300, Train_loss: 0.3507 Train_dice: 0.3507\n",
            "Epoch [4/100], Batch [143/300],  Cross Loss: 0.3042\n",
            "143/300, Train_loss: 0.3042 Train_dice: 0.3042\n",
            "Epoch [4/100], Batch [144/300],  Cross Loss: 0.4286\n",
            "144/300, Train_loss: 0.4286 Train_dice: 0.4286\n",
            "Epoch [4/100], Batch [145/300],  Cross Loss: 0.2760\n",
            "145/300, Train_loss: 0.2760 Train_dice: 0.2760\n",
            "Epoch [4/100], Batch [146/300],  Cross Loss: 0.4450\n",
            "146/300, Train_loss: 0.4450 Train_dice: 0.4450\n",
            "Epoch [4/100], Batch [147/300],  Cross Loss: 0.3516\n",
            "147/300, Train_loss: 0.3516 Train_dice: 0.3516\n",
            "Epoch [4/100], Batch [148/300],  Cross Loss: 0.4419\n",
            "148/300, Train_loss: 0.4419 Train_dice: 0.4419\n",
            "Epoch [4/100], Batch [149/300],  Cross Loss: 0.4250\n",
            "149/300, Train_loss: 0.4250 Train_dice: 0.4250\n",
            "Epoch [4/100], Batch [150/300],  Cross Loss: 0.3303\n",
            "150/300, Train_loss: 0.3303 Train_dice: 0.3303\n",
            "Epoch [4/100], Batch [151/300],  Cross Loss: 0.4167\n",
            "151/300, Train_loss: 0.4167 Train_dice: 0.4167\n",
            "Epoch [4/100], Batch [152/300],  Cross Loss: 0.4671\n",
            "152/300, Train_loss: 0.4671 Train_dice: 0.4671\n",
            "Epoch [4/100], Batch [153/300],  Cross Loss: 0.4561\n",
            "153/300, Train_loss: 0.4561 Train_dice: 0.4561\n",
            "Epoch [4/100], Batch [154/300],  Cross Loss: 0.3397\n",
            "154/300, Train_loss: 0.3397 Train_dice: 0.3397\n",
            "Epoch [4/100], Batch [155/300],  Cross Loss: 0.4184\n",
            "155/300, Train_loss: 0.4184 Train_dice: 0.4184\n",
            "Epoch [4/100], Batch [156/300],  Cross Loss: 0.4011\n",
            "156/300, Train_loss: 0.4011 Train_dice: 0.4011\n",
            "Epoch [4/100], Batch [157/300],  Cross Loss: 0.4756\n",
            "157/300, Train_loss: 0.4756 Train_dice: 0.4756\n",
            "Epoch [4/100], Batch [158/300],  Cross Loss: 0.4087\n",
            "158/300, Train_loss: 0.4087 Train_dice: 0.4087\n",
            "Epoch [4/100], Batch [159/300],  Cross Loss: 0.4383\n",
            "159/300, Train_loss: 0.4383 Train_dice: 0.4383\n",
            "Epoch [4/100], Batch [160/300],  Cross Loss: 0.4457\n",
            "160/300, Train_loss: 0.4457 Train_dice: 0.4457\n",
            "Epoch [4/100], Batch [161/300],  Cross Loss: 0.3884\n",
            "161/300, Train_loss: 0.3884 Train_dice: 0.3884\n",
            "Epoch [4/100], Batch [162/300],  Cross Loss: 0.4250\n",
            "162/300, Train_loss: 0.4250 Train_dice: 0.4250\n",
            "Epoch [4/100], Batch [163/300],  Cross Loss: 0.4654\n",
            "163/300, Train_loss: 0.4654 Train_dice: 0.4654\n",
            "Epoch [4/100], Batch [164/300],  Cross Loss: 0.4079\n",
            "164/300, Train_loss: 0.4079 Train_dice: 0.4079\n",
            "Epoch [4/100], Batch [165/300],  Cross Loss: 0.3763\n",
            "165/300, Train_loss: 0.3763 Train_dice: 0.3763\n",
            "Epoch [4/100], Batch [166/300],  Cross Loss: 0.2959\n",
            "166/300, Train_loss: 0.2959 Train_dice: 0.2959\n",
            "Epoch [4/100], Batch [167/300],  Cross Loss: 0.4838\n",
            "167/300, Train_loss: 0.4838 Train_dice: 0.4838\n",
            "Epoch [4/100], Batch [168/300],  Cross Loss: 0.4102\n",
            "168/300, Train_loss: 0.4102 Train_dice: 0.4102\n",
            "Epoch [4/100], Batch [169/300],  Cross Loss: 0.3690\n",
            "169/300, Train_loss: 0.3690 Train_dice: 0.3690\n",
            "Epoch [4/100], Batch [170/300],  Cross Loss: 0.4253\n",
            "170/300, Train_loss: 0.4253 Train_dice: 0.4253\n",
            "Epoch [4/100], Batch [171/300],  Cross Loss: 0.4370\n",
            "171/300, Train_loss: 0.4370 Train_dice: 0.4370\n",
            "Epoch [4/100], Batch [172/300],  Cross Loss: 0.2547\n",
            "172/300, Train_loss: 0.2547 Train_dice: 0.2547\n",
            "Epoch [4/100], Batch [173/300],  Cross Loss: 0.2067\n",
            "173/300, Train_loss: 0.2067 Train_dice: 0.2067\n",
            "Epoch [4/100], Batch [174/300],  Cross Loss: 0.3583\n",
            "174/300, Train_loss: 0.3583 Train_dice: 0.3583\n",
            "Epoch [4/100], Batch [175/300],  Cross Loss: 0.3389\n",
            "175/300, Train_loss: 0.3389 Train_dice: 0.3389\n",
            "Epoch [4/100], Batch [176/300],  Cross Loss: 0.3870\n",
            "176/300, Train_loss: 0.3870 Train_dice: 0.3870\n",
            "Epoch [4/100], Batch [177/300],  Cross Loss: 0.3132\n",
            "177/300, Train_loss: 0.3132 Train_dice: 0.3132\n",
            "Epoch [4/100], Batch [178/300],  Cross Loss: 0.3291\n",
            "178/300, Train_loss: 0.3291 Train_dice: 0.3291\n",
            "Epoch [4/100], Batch [179/300],  Cross Loss: 0.3608\n",
            "179/300, Train_loss: 0.3608 Train_dice: 0.3608\n",
            "Epoch [4/100], Batch [180/300],  Cross Loss: 0.3601\n",
            "180/300, Train_loss: 0.3601 Train_dice: 0.3601\n",
            "Epoch [4/100], Batch [181/300],  Cross Loss: 0.4329\n",
            "181/300, Train_loss: 0.4329 Train_dice: 0.4329\n",
            "Epoch [4/100], Batch [182/300],  Cross Loss: 0.2154\n",
            "182/300, Train_loss: 0.2154 Train_dice: 0.2154\n",
            "Epoch [4/100], Batch [183/300],  Cross Loss: 0.3746\n",
            "183/300, Train_loss: 0.3746 Train_dice: 0.3746\n",
            "Epoch [4/100], Batch [184/300],  Cross Loss: 0.4480\n",
            "184/300, Train_loss: 0.4480 Train_dice: 0.4480\n",
            "Epoch [4/100], Batch [185/300],  Cross Loss: 0.4166\n",
            "185/300, Train_loss: 0.4166 Train_dice: 0.4166\n",
            "Epoch [4/100], Batch [186/300],  Cross Loss: 0.3946\n",
            "186/300, Train_loss: 0.3946 Train_dice: 0.3946\n",
            "Epoch [4/100], Batch [187/300],  Cross Loss: 0.4068\n",
            "187/300, Train_loss: 0.4068 Train_dice: 0.4068\n",
            "Epoch [4/100], Batch [188/300],  Cross Loss: 0.4310\n",
            "188/300, Train_loss: 0.4310 Train_dice: 0.4310\n",
            "Epoch [4/100], Batch [189/300],  Cross Loss: 0.4302\n",
            "189/300, Train_loss: 0.4302 Train_dice: 0.4302\n",
            "Epoch [4/100], Batch [190/300],  Cross Loss: 0.3532\n",
            "190/300, Train_loss: 0.3532 Train_dice: 0.3532\n",
            "Epoch [4/100], Batch [191/300],  Cross Loss: 0.4247\n",
            "191/300, Train_loss: 0.4247 Train_dice: 0.4247\n",
            "Epoch [4/100], Batch [192/300],  Cross Loss: 0.4270\n",
            "192/300, Train_loss: 0.4270 Train_dice: 0.4270\n",
            "Epoch [4/100], Batch [193/300],  Cross Loss: 0.4275\n",
            "193/300, Train_loss: 0.4275 Train_dice: 0.4275\n",
            "Epoch [4/100], Batch [194/300],  Cross Loss: 0.3161\n",
            "194/300, Train_loss: 0.3161 Train_dice: 0.3161\n",
            "Epoch [4/100], Batch [195/300],  Cross Loss: 0.4214\n",
            "195/300, Train_loss: 0.4214 Train_dice: 0.4214\n",
            "Epoch [4/100], Batch [196/300],  Cross Loss: 0.4066\n",
            "196/300, Train_loss: 0.4066 Train_dice: 0.4066\n",
            "Epoch [4/100], Batch [197/300],  Cross Loss: 0.4140\n",
            "197/300, Train_loss: 0.4140 Train_dice: 0.4140\n",
            "Epoch [4/100], Batch [198/300],  Cross Loss: 0.3685\n",
            "198/300, Train_loss: 0.3685 Train_dice: 0.3685\n",
            "Epoch [4/100], Batch [199/300],  Cross Loss: 0.4065\n",
            "199/300, Train_loss: 0.4065 Train_dice: 0.4065\n",
            "Epoch [4/100], Batch [200/300],  Cross Loss: 0.3839\n",
            "200/300, Train_loss: 0.3839 Train_dice: 0.3839\n",
            "Epoch [4/100], Batch [201/300],  Cross Loss: 0.3352\n",
            "201/300, Train_loss: 0.3352 Train_dice: 0.3352\n",
            "Epoch [4/100], Batch [202/300],  Cross Loss: 0.3929\n",
            "202/300, Train_loss: 0.3929 Train_dice: 0.3929\n",
            "Epoch [4/100], Batch [203/300],  Cross Loss: 0.4024\n",
            "203/300, Train_loss: 0.4024 Train_dice: 0.4024\n",
            "Epoch [4/100], Batch [204/300],  Cross Loss: 0.4515\n",
            "204/300, Train_loss: 0.4515 Train_dice: 0.4515\n",
            "Epoch [4/100], Batch [205/300],  Cross Loss: 0.3634\n",
            "205/300, Train_loss: 0.3634 Train_dice: 0.3634\n",
            "Epoch [4/100], Batch [206/300],  Cross Loss: 0.2512\n",
            "206/300, Train_loss: 0.2512 Train_dice: 0.2512\n",
            "Epoch [4/100], Batch [207/300],  Cross Loss: 0.4251\n",
            "207/300, Train_loss: 0.4251 Train_dice: 0.4251\n",
            "Epoch [4/100], Batch [208/300],  Cross Loss: 0.4479\n",
            "208/300, Train_loss: 0.4479 Train_dice: 0.4479\n",
            "Epoch [4/100], Batch [209/300],  Cross Loss: 0.4260\n",
            "209/300, Train_loss: 0.4260 Train_dice: 0.4260\n",
            "Epoch [4/100], Batch [210/300],  Cross Loss: 0.3944\n",
            "210/300, Train_loss: 0.3944 Train_dice: 0.3944\n",
            "Epoch [4/100], Batch [211/300],  Cross Loss: 0.3987\n",
            "211/300, Train_loss: 0.3987 Train_dice: 0.3987\n",
            "Epoch [4/100], Batch [212/300],  Cross Loss: 0.4277\n",
            "212/300, Train_loss: 0.4277 Train_dice: 0.4277\n",
            "Epoch [4/100], Batch [213/300],  Cross Loss: 0.4211\n",
            "213/300, Train_loss: 0.4211 Train_dice: 0.4211\n",
            "Epoch [4/100], Batch [214/300],  Cross Loss: 0.4153\n",
            "214/300, Train_loss: 0.4153 Train_dice: 0.4153\n",
            "Epoch [4/100], Batch [215/300],  Cross Loss: 0.4126\n",
            "215/300, Train_loss: 0.4126 Train_dice: 0.4126\n",
            "Epoch [4/100], Batch [216/300],  Cross Loss: 0.2834\n",
            "216/300, Train_loss: 0.2834 Train_dice: 0.2834\n",
            "Epoch [4/100], Batch [217/300],  Cross Loss: 0.4222\n",
            "217/300, Train_loss: 0.4222 Train_dice: 0.4222\n",
            "Epoch [4/100], Batch [218/300],  Cross Loss: 0.3572\n",
            "218/300, Train_loss: 0.3572 Train_dice: 0.3572\n",
            "Epoch [4/100], Batch [219/300],  Cross Loss: 0.4836\n",
            "219/300, Train_loss: 0.4836 Train_dice: 0.4836\n",
            "Epoch [4/100], Batch [220/300],  Cross Loss: 0.3895\n",
            "220/300, Train_loss: 0.3895 Train_dice: 0.3895\n",
            "Epoch [4/100], Batch [221/300],  Cross Loss: 0.3878\n",
            "221/300, Train_loss: 0.3878 Train_dice: 0.3878\n",
            "Epoch [4/100], Batch [222/300],  Cross Loss: 0.3406\n",
            "222/300, Train_loss: 0.3406 Train_dice: 0.3406\n",
            "Epoch [4/100], Batch [223/300],  Cross Loss: 0.3332\n",
            "223/300, Train_loss: 0.3332 Train_dice: 0.3332\n",
            "Epoch [4/100], Batch [224/300],  Cross Loss: 0.3926\n",
            "224/300, Train_loss: 0.3926 Train_dice: 0.3926\n",
            "Epoch [4/100], Batch [225/300],  Cross Loss: 0.4576\n",
            "225/300, Train_loss: 0.4576 Train_dice: 0.4576\n",
            "Epoch [4/100], Batch [226/300],  Cross Loss: 0.4834\n",
            "226/300, Train_loss: 0.4834 Train_dice: 0.4834\n",
            "Epoch [4/100], Batch [227/300],  Cross Loss: 0.4025\n",
            "227/300, Train_loss: 0.4025 Train_dice: 0.4025\n",
            "Epoch [4/100], Batch [228/300],  Cross Loss: 0.3983\n",
            "228/300, Train_loss: 0.3983 Train_dice: 0.3983\n",
            "Epoch [4/100], Batch [229/300],  Cross Loss: 0.5381\n",
            "229/300, Train_loss: 0.5381 Train_dice: 0.5381\n",
            "Epoch [4/100], Batch [230/300],  Cross Loss: 0.4177\n",
            "230/300, Train_loss: 0.4177 Train_dice: 0.4177\n",
            "Epoch [4/100], Batch [231/300],  Cross Loss: 0.3911\n",
            "231/300, Train_loss: 0.3911 Train_dice: 0.3911\n",
            "Epoch [4/100], Batch [232/300],  Cross Loss: 0.3810\n",
            "232/300, Train_loss: 0.3810 Train_dice: 0.3810\n",
            "Epoch [4/100], Batch [233/300],  Cross Loss: 0.3725\n",
            "233/300, Train_loss: 0.3725 Train_dice: 0.3725\n",
            "Epoch [4/100], Batch [234/300],  Cross Loss: 0.3804\n",
            "234/300, Train_loss: 0.3804 Train_dice: 0.3804\n",
            "Epoch [4/100], Batch [235/300],  Cross Loss: 0.3863\n",
            "235/300, Train_loss: 0.3863 Train_dice: 0.3863\n",
            "Epoch [4/100], Batch [236/300],  Cross Loss: 0.4344\n",
            "236/300, Train_loss: 0.4344 Train_dice: 0.4344\n",
            "Epoch [4/100], Batch [237/300],  Cross Loss: 0.4175\n",
            "237/300, Train_loss: 0.4175 Train_dice: 0.4175\n",
            "Epoch [4/100], Batch [238/300],  Cross Loss: 0.4301\n",
            "238/300, Train_loss: 0.4301 Train_dice: 0.4301\n",
            "Epoch [4/100], Batch [239/300],  Cross Loss: 0.3746\n",
            "239/300, Train_loss: 0.3746 Train_dice: 0.3746\n",
            "Epoch [4/100], Batch [240/300],  Cross Loss: 0.3997\n",
            "240/300, Train_loss: 0.3997 Train_dice: 0.3997\n",
            "Epoch [4/100], Batch [241/300],  Cross Loss: 0.3776\n",
            "241/300, Train_loss: 0.3776 Train_dice: 0.3776\n",
            "Epoch [4/100], Batch [242/300],  Cross Loss: 0.2734\n",
            "242/300, Train_loss: 0.2734 Train_dice: 0.2734\n",
            "Epoch [4/100], Batch [243/300],  Cross Loss: 0.3953\n",
            "243/300, Train_loss: 0.3953 Train_dice: 0.3953\n",
            "Epoch [4/100], Batch [244/300],  Cross Loss: 0.3297\n",
            "244/300, Train_loss: 0.3297 Train_dice: 0.3297\n",
            "Epoch [4/100], Batch [245/300],  Cross Loss: 0.4026\n",
            "245/300, Train_loss: 0.4026 Train_dice: 0.4026\n",
            "Epoch [4/100], Batch [246/300],  Cross Loss: 0.3981\n",
            "246/300, Train_loss: 0.3981 Train_dice: 0.3981\n",
            "Epoch [4/100], Batch [247/300],  Cross Loss: 0.3139\n",
            "247/300, Train_loss: 0.3139 Train_dice: 0.3139\n",
            "Epoch [4/100], Batch [248/300],  Cross Loss: 0.3491\n",
            "248/300, Train_loss: 0.3491 Train_dice: 0.3491\n",
            "Epoch [4/100], Batch [249/300],  Cross Loss: 0.2720\n",
            "249/300, Train_loss: 0.2720 Train_dice: 0.2720\n",
            "Epoch [4/100], Batch [250/300],  Cross Loss: 0.3471\n",
            "250/300, Train_loss: 0.3471 Train_dice: 0.3471\n",
            "Epoch [4/100], Batch [251/300],  Cross Loss: 0.4104\n",
            "251/300, Train_loss: 0.4104 Train_dice: 0.4104\n",
            "Epoch [4/100], Batch [252/300],  Cross Loss: 0.4506\n",
            "252/300, Train_loss: 0.4506 Train_dice: 0.4506\n",
            "Epoch [4/100], Batch [253/300],  Cross Loss: 0.3888\n",
            "253/300, Train_loss: 0.3888 Train_dice: 0.3888\n",
            "Epoch [4/100], Batch [254/300],  Cross Loss: 0.3924\n",
            "254/300, Train_loss: 0.3924 Train_dice: 0.3924\n",
            "Epoch [4/100], Batch [255/300],  Cross Loss: 0.3696\n",
            "255/300, Train_loss: 0.3696 Train_dice: 0.3696\n",
            "Epoch [4/100], Batch [256/300],  Cross Loss: 0.3914\n",
            "256/300, Train_loss: 0.3914 Train_dice: 0.3914\n",
            "Epoch [4/100], Batch [257/300],  Cross Loss: 0.4968\n",
            "257/300, Train_loss: 0.4968 Train_dice: 0.4968\n",
            "Epoch [4/100], Batch [258/300],  Cross Loss: 0.3123\n",
            "258/300, Train_loss: 0.3123 Train_dice: 0.3123\n",
            "Epoch [4/100], Batch [259/300],  Cross Loss: 0.4652\n",
            "259/300, Train_loss: 0.4652 Train_dice: 0.4652\n",
            "Epoch [4/100], Batch [260/300],  Cross Loss: 0.4151\n",
            "260/300, Train_loss: 0.4151 Train_dice: 0.4151\n",
            "Epoch [4/100], Batch [261/300],  Cross Loss: 0.4035\n",
            "261/300, Train_loss: 0.4035 Train_dice: 0.4035\n",
            "Epoch [4/100], Batch [262/300],  Cross Loss: 0.4007\n",
            "262/300, Train_loss: 0.4007 Train_dice: 0.4007\n",
            "Epoch [4/100], Batch [263/300],  Cross Loss: 0.2854\n",
            "263/300, Train_loss: 0.2854 Train_dice: 0.2854\n",
            "Epoch [4/100], Batch [264/300],  Cross Loss: 0.3693\n",
            "264/300, Train_loss: 0.3693 Train_dice: 0.3693\n",
            "Epoch [4/100], Batch [265/300],  Cross Loss: 0.4847\n",
            "265/300, Train_loss: 0.4847 Train_dice: 0.4847\n",
            "Epoch [4/100], Batch [266/300],  Cross Loss: 0.3548\n",
            "266/300, Train_loss: 0.3548 Train_dice: 0.3548\n",
            "Epoch [4/100], Batch [267/300],  Cross Loss: 0.4236\n",
            "267/300, Train_loss: 0.4236 Train_dice: 0.4236\n",
            "Epoch [4/100], Batch [268/300],  Cross Loss: 0.4946\n",
            "268/300, Train_loss: 0.4946 Train_dice: 0.4946\n",
            "Epoch [4/100], Batch [269/300],  Cross Loss: 0.2661\n",
            "269/300, Train_loss: 0.2661 Train_dice: 0.2661\n",
            "Epoch [4/100], Batch [270/300],  Cross Loss: 0.4897\n",
            "270/300, Train_loss: 0.4897 Train_dice: 0.4897\n",
            "Epoch [4/100], Batch [271/300],  Cross Loss: 0.3722\n",
            "271/300, Train_loss: 0.3722 Train_dice: 0.3722\n",
            "Epoch [4/100], Batch [272/300],  Cross Loss: 0.5099\n",
            "272/300, Train_loss: 0.5099 Train_dice: 0.5099\n",
            "Epoch [4/100], Batch [273/300],  Cross Loss: 0.4982\n",
            "273/300, Train_loss: 0.4982 Train_dice: 0.4982\n",
            "Epoch [4/100], Batch [274/300],  Cross Loss: 0.3801\n",
            "274/300, Train_loss: 0.3801 Train_dice: 0.3801\n",
            "Epoch [4/100], Batch [275/300],  Cross Loss: 0.4378\n",
            "275/300, Train_loss: 0.4378 Train_dice: 0.4378\n",
            "Epoch [4/100], Batch [276/300],  Cross Loss: 0.4553\n",
            "276/300, Train_loss: 0.4553 Train_dice: 0.4553\n",
            "Epoch [4/100], Batch [277/300],  Cross Loss: 0.4457\n",
            "277/300, Train_loss: 0.4457 Train_dice: 0.4457\n",
            "Epoch [4/100], Batch [278/300],  Cross Loss: 0.4948\n",
            "278/300, Train_loss: 0.4948 Train_dice: 0.4948\n",
            "Epoch [4/100], Batch [279/300],  Cross Loss: 0.4104\n",
            "279/300, Train_loss: 0.4104 Train_dice: 0.4104\n",
            "Epoch [4/100], Batch [280/300],  Cross Loss: 0.4130\n",
            "280/300, Train_loss: 0.4130 Train_dice: 0.4130\n",
            "Epoch [4/100], Batch [281/300],  Cross Loss: 0.3628\n",
            "281/300, Train_loss: 0.3628 Train_dice: 0.3628\n",
            "Epoch [4/100], Batch [282/300],  Cross Loss: 0.3665\n",
            "282/300, Train_loss: 0.3665 Train_dice: 0.3665\n",
            "Epoch [4/100], Batch [283/300],  Cross Loss: 0.3377\n",
            "283/300, Train_loss: 0.3377 Train_dice: 0.3377\n",
            "Epoch [4/100], Batch [284/300],  Cross Loss: 0.4401\n",
            "284/300, Train_loss: 0.4401 Train_dice: 0.4401\n",
            "Epoch [4/100], Batch [285/300],  Cross Loss: 0.4020\n",
            "285/300, Train_loss: 0.4020 Train_dice: 0.4020\n",
            "Epoch [4/100], Batch [286/300],  Cross Loss: 0.3700\n",
            "286/300, Train_loss: 0.3700 Train_dice: 0.3700\n",
            "Epoch [4/100], Batch [287/300],  Cross Loss: 0.4498\n",
            "287/300, Train_loss: 0.4498 Train_dice: 0.4498\n",
            "Epoch [4/100], Batch [288/300],  Cross Loss: 0.3876\n",
            "288/300, Train_loss: 0.3876 Train_dice: 0.3876\n",
            "Epoch [4/100], Batch [289/300],  Cross Loss: 0.3260\n",
            "289/300, Train_loss: 0.3260 Train_dice: 0.3260\n",
            "Epoch [4/100], Batch [290/300],  Cross Loss: 0.4142\n",
            "290/300, Train_loss: 0.4142 Train_dice: 0.4142\n",
            "Epoch [4/100], Batch [291/300],  Cross Loss: 0.3883\n",
            "291/300, Train_loss: 0.3883 Train_dice: 0.3883\n",
            "Epoch [4/100], Batch [292/300],  Cross Loss: 0.3781\n",
            "292/300, Train_loss: 0.3781 Train_dice: 0.3781\n",
            "Epoch [4/100], Batch [293/300],  Cross Loss: 0.3969\n",
            "293/300, Train_loss: 0.3969 Train_dice: 0.3969\n",
            "Epoch [4/100], Batch [294/300],  Cross Loss: 0.4015\n",
            "294/300, Train_loss: 0.4015 Train_dice: 0.4015\n",
            "Epoch [4/100], Batch [295/300],  Cross Loss: 0.4936\n",
            "295/300, Train_loss: 0.4936 Train_dice: 0.4936\n",
            "Epoch [4/100], Batch [296/300],  Cross Loss: 0.4185\n",
            "296/300, Train_loss: 0.4185 Train_dice: 0.4185\n",
            "Epoch [4/100], Batch [297/300],  Cross Loss: 0.5336\n",
            "297/300, Train_loss: 0.5336 Train_dice: 0.5336\n",
            "Epoch [4/100], Batch [298/300],  Cross Loss: 0.3804\n",
            "298/300, Train_loss: 0.3804 Train_dice: 0.3804\n",
            "Epoch [4/100], Batch [299/300],  Cross Loss: 0.5086\n",
            "299/300, Train_loss: 0.5086 Train_dice: 0.5086\n",
            "Epoch [4/100], Batch [300/300],  Cross Loss: 0.2742\n",
            "300/300, Train_loss: 0.2742 Train_dice: 0.2742\n",
            "--------------------\n",
            "Epoch_loss: 0.3947\n",
            "Epoch_metric: tensor(0.3947, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "----------\n",
            "epoch 5/100\n",
            "Epoch [5/100], Batch [1/300],  Cross Loss: 0.2058\n",
            "1/300, Train_loss: 0.2058 Train_dice: 0.2058\n",
            "Epoch [5/100], Batch [2/300],  Cross Loss: 0.3911\n",
            "2/300, Train_loss: 0.3911 Train_dice: 0.3911\n",
            "Epoch [5/100], Batch [3/300],  Cross Loss: 0.2474\n",
            "3/300, Train_loss: 0.2474 Train_dice: 0.2474\n",
            "Epoch [5/100], Batch [4/300],  Cross Loss: 0.5298\n",
            "4/300, Train_loss: 0.5298 Train_dice: 0.5298\n",
            "Epoch [5/100], Batch [5/300],  Cross Loss: 0.4086\n",
            "5/300, Train_loss: 0.4086 Train_dice: 0.4086\n",
            "Epoch [5/100], Batch [6/300],  Cross Loss: 0.3852\n",
            "6/300, Train_loss: 0.3852 Train_dice: 0.3852\n",
            "Epoch [5/100], Batch [7/300],  Cross Loss: 0.3480\n",
            "7/300, Train_loss: 0.3480 Train_dice: 0.3480\n",
            "Epoch [5/100], Batch [8/300],  Cross Loss: 0.2304\n",
            "8/300, Train_loss: 0.2304 Train_dice: 0.2304\n",
            "Epoch [5/100], Batch [9/300],  Cross Loss: 0.4031\n",
            "9/300, Train_loss: 0.4031 Train_dice: 0.4031\n",
            "Epoch [5/100], Batch [10/300],  Cross Loss: 0.4008\n",
            "10/300, Train_loss: 0.4008 Train_dice: 0.4008\n",
            "Epoch [5/100], Batch [11/300],  Cross Loss: 0.4577\n",
            "11/300, Train_loss: 0.4577 Train_dice: 0.4577\n",
            "Epoch [5/100], Batch [12/300],  Cross Loss: 0.3224\n",
            "12/300, Train_loss: 0.3224 Train_dice: 0.3224\n",
            "Epoch [5/100], Batch [13/300],  Cross Loss: 0.3484\n",
            "13/300, Train_loss: 0.3484 Train_dice: 0.3484\n",
            "Epoch [5/100], Batch [14/300],  Cross Loss: 0.4402\n",
            "14/300, Train_loss: 0.4402 Train_dice: 0.4402\n",
            "Epoch [5/100], Batch [15/300],  Cross Loss: 0.3707\n",
            "15/300, Train_loss: 0.3707 Train_dice: 0.3707\n",
            "Epoch [5/100], Batch [16/300],  Cross Loss: 0.3439\n",
            "16/300, Train_loss: 0.3439 Train_dice: 0.3439\n",
            "Epoch [5/100], Batch [17/300],  Cross Loss: 0.5017\n",
            "17/300, Train_loss: 0.5017 Train_dice: 0.5017\n",
            "Epoch [5/100], Batch [18/300],  Cross Loss: 0.3342\n",
            "18/300, Train_loss: 0.3342 Train_dice: 0.3342\n",
            "Epoch [5/100], Batch [19/300],  Cross Loss: 0.3311\n",
            "19/300, Train_loss: 0.3311 Train_dice: 0.3311\n",
            "Epoch [5/100], Batch [20/300],  Cross Loss: 0.3628\n",
            "20/300, Train_loss: 0.3628 Train_dice: 0.3628\n",
            "Epoch [5/100], Batch [21/300],  Cross Loss: 0.2541\n",
            "21/300, Train_loss: 0.2541 Train_dice: 0.2541\n",
            "Epoch [5/100], Batch [22/300],  Cross Loss: 0.4040\n",
            "22/300, Train_loss: 0.4040 Train_dice: 0.4040\n",
            "Epoch [5/100], Batch [23/300],  Cross Loss: 0.4713\n",
            "23/300, Train_loss: 0.4713 Train_dice: 0.4713\n",
            "Epoch [5/100], Batch [24/300],  Cross Loss: 0.4424\n",
            "24/300, Train_loss: 0.4424 Train_dice: 0.4424\n",
            "Epoch [5/100], Batch [25/300],  Cross Loss: 0.3626\n",
            "25/300, Train_loss: 0.3626 Train_dice: 0.3626\n",
            "Epoch [5/100], Batch [26/300],  Cross Loss: 0.3912\n",
            "26/300, Train_loss: 0.3912 Train_dice: 0.3912\n",
            "Epoch [5/100], Batch [27/300],  Cross Loss: 0.4373\n",
            "27/300, Train_loss: 0.4373 Train_dice: 0.4373\n",
            "Epoch [5/100], Batch [28/300],  Cross Loss: 0.5017\n",
            "28/300, Train_loss: 0.5017 Train_dice: 0.5017\n",
            "Epoch [5/100], Batch [29/300],  Cross Loss: 0.4416\n",
            "29/300, Train_loss: 0.4416 Train_dice: 0.4416\n",
            "Epoch [5/100], Batch [30/300],  Cross Loss: 0.3480\n",
            "30/300, Train_loss: 0.3480 Train_dice: 0.3480\n",
            "Epoch [5/100], Batch [31/300],  Cross Loss: 0.3580\n",
            "31/300, Train_loss: 0.3580 Train_dice: 0.3580\n",
            "Epoch [5/100], Batch [32/300],  Cross Loss: 0.3884\n",
            "32/300, Train_loss: 0.3884 Train_dice: 0.3884\n",
            "Epoch [5/100], Batch [33/300],  Cross Loss: 0.3452\n",
            "33/300, Train_loss: 0.3452 Train_dice: 0.3452\n",
            "Epoch [5/100], Batch [34/300],  Cross Loss: 0.4253\n",
            "34/300, Train_loss: 0.4253 Train_dice: 0.4253\n",
            "Epoch [5/100], Batch [35/300],  Cross Loss: 0.4148\n",
            "35/300, Train_loss: 0.4148 Train_dice: 0.4148\n",
            "Epoch [5/100], Batch [36/300],  Cross Loss: 0.3617\n",
            "36/300, Train_loss: 0.3617 Train_dice: 0.3617\n",
            "Epoch [5/100], Batch [37/300],  Cross Loss: 0.4506\n",
            "37/300, Train_loss: 0.4506 Train_dice: 0.4506\n",
            "Epoch [5/100], Batch [38/300],  Cross Loss: 0.3419\n",
            "38/300, Train_loss: 0.3419 Train_dice: 0.3419\n",
            "Epoch [5/100], Batch [39/300],  Cross Loss: 0.4894\n",
            "39/300, Train_loss: 0.4894 Train_dice: 0.4894\n",
            "Epoch [5/100], Batch [40/300],  Cross Loss: 0.3597\n",
            "40/300, Train_loss: 0.3597 Train_dice: 0.3597\n",
            "Epoch [5/100], Batch [41/300],  Cross Loss: 0.4369\n",
            "41/300, Train_loss: 0.4369 Train_dice: 0.4369\n",
            "Epoch [5/100], Batch [42/300],  Cross Loss: 0.3601\n",
            "42/300, Train_loss: 0.3601 Train_dice: 0.3601\n",
            "Epoch [5/100], Batch [43/300],  Cross Loss: 0.5011\n",
            "43/300, Train_loss: 0.5011 Train_dice: 0.5011\n",
            "Epoch [5/100], Batch [44/300],  Cross Loss: 0.3400\n",
            "44/300, Train_loss: 0.3400 Train_dice: 0.3400\n",
            "Epoch [5/100], Batch [45/300],  Cross Loss: 0.4374\n",
            "45/300, Train_loss: 0.4374 Train_dice: 0.4374\n",
            "Epoch [5/100], Batch [46/300],  Cross Loss: 0.4192\n",
            "46/300, Train_loss: 0.4192 Train_dice: 0.4192\n",
            "Epoch [5/100], Batch [47/300],  Cross Loss: 0.3293\n",
            "47/300, Train_loss: 0.3293 Train_dice: 0.3293\n",
            "Epoch [5/100], Batch [48/300],  Cross Loss: 0.4184\n",
            "48/300, Train_loss: 0.4184 Train_dice: 0.4184\n",
            "Epoch [5/100], Batch [49/300],  Cross Loss: 0.4355\n",
            "49/300, Train_loss: 0.4355 Train_dice: 0.4355\n",
            "Epoch [5/100], Batch [50/300],  Cross Loss: 0.2930\n",
            "50/300, Train_loss: 0.2930 Train_dice: 0.2930\n",
            "Epoch [5/100], Batch [51/300],  Cross Loss: 0.5071\n",
            "51/300, Train_loss: 0.5071 Train_dice: 0.5071\n",
            "Epoch [5/100], Batch [52/300],  Cross Loss: 0.3545\n",
            "52/300, Train_loss: 0.3545 Train_dice: 0.3545\n",
            "Epoch [5/100], Batch [53/300],  Cross Loss: 0.3534\n",
            "53/300, Train_loss: 0.3534 Train_dice: 0.3534\n",
            "Epoch [5/100], Batch [54/300],  Cross Loss: 0.5062\n",
            "54/300, Train_loss: 0.5062 Train_dice: 0.5062\n",
            "Epoch [5/100], Batch [55/300],  Cross Loss: 0.4367\n",
            "55/300, Train_loss: 0.4367 Train_dice: 0.4367\n",
            "Epoch [5/100], Batch [56/300],  Cross Loss: 0.4065\n",
            "56/300, Train_loss: 0.4065 Train_dice: 0.4065\n",
            "Epoch [5/100], Batch [57/300],  Cross Loss: 0.3504\n",
            "57/300, Train_loss: 0.3504 Train_dice: 0.3504\n",
            "Epoch [5/100], Batch [58/300],  Cross Loss: 0.2976\n",
            "58/300, Train_loss: 0.2976 Train_dice: 0.2976\n",
            "Epoch [5/100], Batch [59/300],  Cross Loss: 0.3567\n",
            "59/300, Train_loss: 0.3567 Train_dice: 0.3567\n",
            "Epoch [5/100], Batch [60/300],  Cross Loss: 0.5037\n",
            "60/300, Train_loss: 0.5037 Train_dice: 0.5037\n",
            "Epoch [5/100], Batch [61/300],  Cross Loss: 0.3977\n",
            "61/300, Train_loss: 0.3977 Train_dice: 0.3977\n",
            "Epoch [5/100], Batch [62/300],  Cross Loss: 0.4291\n",
            "62/300, Train_loss: 0.4291 Train_dice: 0.4291\n",
            "Epoch [5/100], Batch [63/300],  Cross Loss: 0.4391\n",
            "63/300, Train_loss: 0.4391 Train_dice: 0.4391\n",
            "Epoch [5/100], Batch [64/300],  Cross Loss: 0.3198\n",
            "64/300, Train_loss: 0.3198 Train_dice: 0.3198\n",
            "Epoch [5/100], Batch [65/300],  Cross Loss: 0.2390\n",
            "65/300, Train_loss: 0.2390 Train_dice: 0.2390\n",
            "Epoch [5/100], Batch [66/300],  Cross Loss: 0.3053\n",
            "66/300, Train_loss: 0.3053 Train_dice: 0.3053\n",
            "Epoch [5/100], Batch [67/300],  Cross Loss: 0.3056\n",
            "67/300, Train_loss: 0.3056 Train_dice: 0.3056\n",
            "Epoch [5/100], Batch [68/300],  Cross Loss: 0.4348\n",
            "68/300, Train_loss: 0.4348 Train_dice: 0.4348\n",
            "Epoch [5/100], Batch [69/300],  Cross Loss: 0.4340\n",
            "69/300, Train_loss: 0.4340 Train_dice: 0.4340\n",
            "Epoch [5/100], Batch [70/300],  Cross Loss: 0.3064\n",
            "70/300, Train_loss: 0.3064 Train_dice: 0.3064\n",
            "Epoch [5/100], Batch [71/300],  Cross Loss: 0.4427\n",
            "71/300, Train_loss: 0.4427 Train_dice: 0.4427\n",
            "Epoch [5/100], Batch [72/300],  Cross Loss: 0.3273\n",
            "72/300, Train_loss: 0.3273 Train_dice: 0.3273\n",
            "Epoch [5/100], Batch [73/300],  Cross Loss: 0.4319\n",
            "73/300, Train_loss: 0.4319 Train_dice: 0.4319\n",
            "Epoch [5/100], Batch [74/300],  Cross Loss: 0.2948\n",
            "74/300, Train_loss: 0.2948 Train_dice: 0.2948\n",
            "Epoch [5/100], Batch [75/300],  Cross Loss: 0.5148\n",
            "75/300, Train_loss: 0.5148 Train_dice: 0.5148\n",
            "Epoch [5/100], Batch [76/300],  Cross Loss: 0.4093\n",
            "76/300, Train_loss: 0.4093 Train_dice: 0.4093\n",
            "Epoch [5/100], Batch [77/300],  Cross Loss: 0.4142\n",
            "77/300, Train_loss: 0.4142 Train_dice: 0.4142\n",
            "Epoch [5/100], Batch [78/300],  Cross Loss: 0.3160\n",
            "78/300, Train_loss: 0.3160 Train_dice: 0.3160\n",
            "Epoch [5/100], Batch [79/300],  Cross Loss: 0.3842\n",
            "79/300, Train_loss: 0.3842 Train_dice: 0.3842\n",
            "Epoch [5/100], Batch [80/300],  Cross Loss: 0.3410\n",
            "80/300, Train_loss: 0.3410 Train_dice: 0.3410\n",
            "Epoch [5/100], Batch [81/300],  Cross Loss: 0.4665\n",
            "81/300, Train_loss: 0.4665 Train_dice: 0.4665\n",
            "Epoch [5/100], Batch [82/300],  Cross Loss: 0.4449\n",
            "82/300, Train_loss: 0.4449 Train_dice: 0.4449\n",
            "Epoch [5/100], Batch [83/300],  Cross Loss: 0.5092\n",
            "83/300, Train_loss: 0.5092 Train_dice: 0.5092\n",
            "Epoch [5/100], Batch [84/300],  Cross Loss: 0.4248\n",
            "84/300, Train_loss: 0.4248 Train_dice: 0.4248\n",
            "Epoch [5/100], Batch [85/300],  Cross Loss: 0.3145\n",
            "85/300, Train_loss: 0.3145 Train_dice: 0.3145\n",
            "Epoch [5/100], Batch [86/300],  Cross Loss: 0.3008\n",
            "86/300, Train_loss: 0.3008 Train_dice: 0.3008\n",
            "Epoch [5/100], Batch [87/300],  Cross Loss: 0.4755\n",
            "87/300, Train_loss: 0.4755 Train_dice: 0.4755\n",
            "Epoch [5/100], Batch [88/300],  Cross Loss: 0.4298\n",
            "88/300, Train_loss: 0.4298 Train_dice: 0.4298\n",
            "Epoch [5/100], Batch [89/300],  Cross Loss: 0.4390\n",
            "89/300, Train_loss: 0.4390 Train_dice: 0.4390\n",
            "Epoch [5/100], Batch [90/300],  Cross Loss: 0.4667\n",
            "90/300, Train_loss: 0.4667 Train_dice: 0.4667\n",
            "Epoch [5/100], Batch [91/300],  Cross Loss: 0.5058\n",
            "91/300, Train_loss: 0.5058 Train_dice: 0.5058\n",
            "Epoch [5/100], Batch [92/300],  Cross Loss: 0.4142\n",
            "92/300, Train_loss: 0.4142 Train_dice: 0.4142\n",
            "Epoch [5/100], Batch [93/300],  Cross Loss: 0.4069\n",
            "93/300, Train_loss: 0.4069 Train_dice: 0.4069\n",
            "Epoch [5/100], Batch [94/300],  Cross Loss: 0.2584\n",
            "94/300, Train_loss: 0.2584 Train_dice: 0.2584\n",
            "Epoch [5/100], Batch [95/300],  Cross Loss: 0.2238\n",
            "95/300, Train_loss: 0.2238 Train_dice: 0.2238\n",
            "Epoch [5/100], Batch [96/300],  Cross Loss: 0.4376\n",
            "96/300, Train_loss: 0.4376 Train_dice: 0.4376\n",
            "Epoch [5/100], Batch [97/300],  Cross Loss: 0.2747\n",
            "97/300, Train_loss: 0.2747 Train_dice: 0.2747\n",
            "Epoch [5/100], Batch [98/300],  Cross Loss: 0.4233\n",
            "98/300, Train_loss: 0.4233 Train_dice: 0.4233\n",
            "Epoch [5/100], Batch [99/300],  Cross Loss: 0.4123\n",
            "99/300, Train_loss: 0.4123 Train_dice: 0.4123\n",
            "Epoch [5/100], Batch [100/300],  Cross Loss: 0.3298\n",
            "100/300, Train_loss: 0.3298 Train_dice: 0.3298\n",
            "Epoch [5/100], Batch [101/300],  Cross Loss: 0.3071\n",
            "101/300, Train_loss: 0.3071 Train_dice: 0.3071\n",
            "Epoch [5/100], Batch [102/300],  Cross Loss: 0.4054\n",
            "102/300, Train_loss: 0.4054 Train_dice: 0.4054\n",
            "Epoch [5/100], Batch [103/300],  Cross Loss: 0.4321\n",
            "103/300, Train_loss: 0.4321 Train_dice: 0.4321\n",
            "Epoch [5/100], Batch [104/300],  Cross Loss: 0.4664\n",
            "104/300, Train_loss: 0.4664 Train_dice: 0.4664\n",
            "Epoch [5/100], Batch [105/300],  Cross Loss: 0.3362\n",
            "105/300, Train_loss: 0.3362 Train_dice: 0.3362\n",
            "Epoch [5/100], Batch [106/300],  Cross Loss: 0.4045\n",
            "106/300, Train_loss: 0.4045 Train_dice: 0.4045\n",
            "Epoch [5/100], Batch [107/300],  Cross Loss: 0.4890\n",
            "107/300, Train_loss: 0.4890 Train_dice: 0.4890\n",
            "Epoch [5/100], Batch [108/300],  Cross Loss: 0.4054\n",
            "108/300, Train_loss: 0.4054 Train_dice: 0.4054\n",
            "Epoch [5/100], Batch [109/300],  Cross Loss: 0.4248\n",
            "109/300, Train_loss: 0.4248 Train_dice: 0.4248\n",
            "Epoch [5/100], Batch [110/300],  Cross Loss: 0.3273\n",
            "110/300, Train_loss: 0.3273 Train_dice: 0.3273\n",
            "Epoch [5/100], Batch [111/300],  Cross Loss: 0.4394\n",
            "111/300, Train_loss: 0.4394 Train_dice: 0.4394\n",
            "Epoch [5/100], Batch [112/300],  Cross Loss: 0.3147\n",
            "112/300, Train_loss: 0.3147 Train_dice: 0.3147\n",
            "Epoch [5/100], Batch [113/300],  Cross Loss: 0.4043\n",
            "113/300, Train_loss: 0.4043 Train_dice: 0.4043\n",
            "Epoch [5/100], Batch [114/300],  Cross Loss: 0.4833\n",
            "114/300, Train_loss: 0.4833 Train_dice: 0.4833\n",
            "Epoch [5/100], Batch [115/300],  Cross Loss: 0.3115\n",
            "115/300, Train_loss: 0.3115 Train_dice: 0.3115\n",
            "Epoch [5/100], Batch [116/300],  Cross Loss: 0.3341\n",
            "116/300, Train_loss: 0.3341 Train_dice: 0.3341\n",
            "Epoch [5/100], Batch [117/300],  Cross Loss: 0.3958\n",
            "117/300, Train_loss: 0.3958 Train_dice: 0.3958\n",
            "Epoch [5/100], Batch [118/300],  Cross Loss: 0.4728\n",
            "118/300, Train_loss: 0.4728 Train_dice: 0.4728\n",
            "Epoch [5/100], Batch [119/300],  Cross Loss: 0.3939\n",
            "119/300, Train_loss: 0.3939 Train_dice: 0.3939\n",
            "Epoch [5/100], Batch [120/300],  Cross Loss: 0.4518\n",
            "120/300, Train_loss: 0.4518 Train_dice: 0.4518\n",
            "Epoch [5/100], Batch [121/300],  Cross Loss: 0.4194\n",
            "121/300, Train_loss: 0.4194 Train_dice: 0.4194\n",
            "Epoch [5/100], Batch [122/300],  Cross Loss: 0.4377\n",
            "122/300, Train_loss: 0.4377 Train_dice: 0.4377\n",
            "Epoch [5/100], Batch [123/300],  Cross Loss: 0.4767\n",
            "123/300, Train_loss: 0.4767 Train_dice: 0.4767\n",
            "Epoch [5/100], Batch [124/300],  Cross Loss: 0.4561\n",
            "124/300, Train_loss: 0.4561 Train_dice: 0.4561\n",
            "Epoch [5/100], Batch [125/300],  Cross Loss: 0.4796\n",
            "125/300, Train_loss: 0.4796 Train_dice: 0.4796\n",
            "Epoch [5/100], Batch [126/300],  Cross Loss: 0.4803\n",
            "126/300, Train_loss: 0.4803 Train_dice: 0.4803\n",
            "Epoch [5/100], Batch [127/300],  Cross Loss: 0.4396\n",
            "127/300, Train_loss: 0.4396 Train_dice: 0.4396\n",
            "Epoch [5/100], Batch [128/300],  Cross Loss: 0.4043\n",
            "128/300, Train_loss: 0.4043 Train_dice: 0.4043\n",
            "Epoch [5/100], Batch [129/300],  Cross Loss: 0.4306\n",
            "129/300, Train_loss: 0.4306 Train_dice: 0.4306\n",
            "Epoch [5/100], Batch [130/300],  Cross Loss: 0.4190\n",
            "130/300, Train_loss: 0.4190 Train_dice: 0.4190\n",
            "Epoch [5/100], Batch [131/300],  Cross Loss: 0.3228\n",
            "131/300, Train_loss: 0.3228 Train_dice: 0.3228\n",
            "Epoch [5/100], Batch [132/300],  Cross Loss: 0.3994\n",
            "132/300, Train_loss: 0.3994 Train_dice: 0.3994\n",
            "Epoch [5/100], Batch [133/300],  Cross Loss: 0.3821\n",
            "133/300, Train_loss: 0.3821 Train_dice: 0.3821\n",
            "Epoch [5/100], Batch [134/300],  Cross Loss: 0.3962\n",
            "134/300, Train_loss: 0.3962 Train_dice: 0.3962\n",
            "Epoch [5/100], Batch [135/300],  Cross Loss: 0.4172\n",
            "135/300, Train_loss: 0.4172 Train_dice: 0.4172\n",
            "Epoch [5/100], Batch [136/300],  Cross Loss: 0.4769\n",
            "136/300, Train_loss: 0.4769 Train_dice: 0.4769\n",
            "Epoch [5/100], Batch [137/300],  Cross Loss: 0.3880\n",
            "137/300, Train_loss: 0.3880 Train_dice: 0.3880\n",
            "Epoch [5/100], Batch [138/300],  Cross Loss: 0.3911\n",
            "138/300, Train_loss: 0.3911 Train_dice: 0.3911\n",
            "Epoch [5/100], Batch [139/300],  Cross Loss: 0.3158\n",
            "139/300, Train_loss: 0.3158 Train_dice: 0.3158\n",
            "Epoch [5/100], Batch [140/300],  Cross Loss: 0.2530\n",
            "140/300, Train_loss: 0.2530 Train_dice: 0.2530\n",
            "Epoch [5/100], Batch [141/300],  Cross Loss: 0.3201\n",
            "141/300, Train_loss: 0.3201 Train_dice: 0.3201\n",
            "Epoch [5/100], Batch [142/300],  Cross Loss: 0.2590\n",
            "142/300, Train_loss: 0.2590 Train_dice: 0.2590\n",
            "Epoch [5/100], Batch [143/300],  Cross Loss: 0.4512\n",
            "143/300, Train_loss: 0.4512 Train_dice: 0.4512\n",
            "Epoch [5/100], Batch [144/300],  Cross Loss: 0.4526\n",
            "144/300, Train_loss: 0.4526 Train_dice: 0.4526\n",
            "Epoch [5/100], Batch [145/300],  Cross Loss: 0.4125\n",
            "145/300, Train_loss: 0.4125 Train_dice: 0.4125\n",
            "Epoch [5/100], Batch [146/300],  Cross Loss: 0.3558\n",
            "146/300, Train_loss: 0.3558 Train_dice: 0.3558\n",
            "Epoch [5/100], Batch [147/300],  Cross Loss: 0.4239\n",
            "147/300, Train_loss: 0.4239 Train_dice: 0.4239\n",
            "Epoch [5/100], Batch [148/300],  Cross Loss: 0.2624\n",
            "148/300, Train_loss: 0.2624 Train_dice: 0.2624\n",
            "Epoch [5/100], Batch [149/300],  Cross Loss: 0.4381\n",
            "149/300, Train_loss: 0.4381 Train_dice: 0.4381\n",
            "Epoch [5/100], Batch [150/300],  Cross Loss: 0.3120\n",
            "150/300, Train_loss: 0.3120 Train_dice: 0.3120\n",
            "Epoch [5/100], Batch [151/300],  Cross Loss: 0.4485\n",
            "151/300, Train_loss: 0.4485 Train_dice: 0.4485\n",
            "Epoch [5/100], Batch [152/300],  Cross Loss: 0.5075\n",
            "152/300, Train_loss: 0.5075 Train_dice: 0.5075\n",
            "Epoch [5/100], Batch [153/300],  Cross Loss: 0.4461\n",
            "153/300, Train_loss: 0.4461 Train_dice: 0.4461\n",
            "Epoch [5/100], Batch [154/300],  Cross Loss: 0.4014\n",
            "154/300, Train_loss: 0.4014 Train_dice: 0.4014\n",
            "Epoch [5/100], Batch [155/300],  Cross Loss: 0.4776\n",
            "155/300, Train_loss: 0.4776 Train_dice: 0.4776\n",
            "Epoch [5/100], Batch [156/300],  Cross Loss: 0.3876\n",
            "156/300, Train_loss: 0.3876 Train_dice: 0.3876\n",
            "Epoch [5/100], Batch [157/300],  Cross Loss: 0.4877\n",
            "157/300, Train_loss: 0.4877 Train_dice: 0.4877\n",
            "Epoch [5/100], Batch [158/300],  Cross Loss: 0.5216\n",
            "158/300, Train_loss: 0.5216 Train_dice: 0.5216\n",
            "Epoch [5/100], Batch [159/300],  Cross Loss: 0.3475\n",
            "159/300, Train_loss: 0.3475 Train_dice: 0.3475\n",
            "Epoch [5/100], Batch [160/300],  Cross Loss: 0.3782\n",
            "160/300, Train_loss: 0.3782 Train_dice: 0.3782\n",
            "Epoch [5/100], Batch [161/300],  Cross Loss: 0.3055\n",
            "161/300, Train_loss: 0.3055 Train_dice: 0.3055\n",
            "Epoch [5/100], Batch [162/300],  Cross Loss: 0.3044\n",
            "162/300, Train_loss: 0.3044 Train_dice: 0.3044\n",
            "Epoch [5/100], Batch [163/300],  Cross Loss: 0.3776\n",
            "163/300, Train_loss: 0.3776 Train_dice: 0.3776\n",
            "Epoch [5/100], Batch [164/300],  Cross Loss: 0.2694\n",
            "164/300, Train_loss: 0.2694 Train_dice: 0.2694\n",
            "Epoch [5/100], Batch [165/300],  Cross Loss: 0.3516\n",
            "165/300, Train_loss: 0.3516 Train_dice: 0.3516\n",
            "Epoch [5/100], Batch [166/300],  Cross Loss: 0.2874\n",
            "166/300, Train_loss: 0.2874 Train_dice: 0.2874\n",
            "Epoch [5/100], Batch [167/300],  Cross Loss: 0.4397\n",
            "167/300, Train_loss: 0.4397 Train_dice: 0.4397\n",
            "Epoch [5/100], Batch [168/300],  Cross Loss: 0.4461\n",
            "168/300, Train_loss: 0.4461 Train_dice: 0.4461\n",
            "Epoch [5/100], Batch [169/300],  Cross Loss: 0.3792\n",
            "169/300, Train_loss: 0.3792 Train_dice: 0.3792\n",
            "Epoch [5/100], Batch [170/300],  Cross Loss: 0.3012\n",
            "170/300, Train_loss: 0.3012 Train_dice: 0.3012\n",
            "Epoch [5/100], Batch [171/300],  Cross Loss: 0.4414\n",
            "171/300, Train_loss: 0.4414 Train_dice: 0.4414\n",
            "Epoch [5/100], Batch [172/300],  Cross Loss: 0.4024\n",
            "172/300, Train_loss: 0.4024 Train_dice: 0.4024\n",
            "Epoch [5/100], Batch [173/300],  Cross Loss: 0.2650\n",
            "173/300, Train_loss: 0.2650 Train_dice: 0.2650\n",
            "Epoch [5/100], Batch [174/300],  Cross Loss: 0.4197\n",
            "174/300, Train_loss: 0.4197 Train_dice: 0.4197\n",
            "Epoch [5/100], Batch [175/300],  Cross Loss: 0.4134\n",
            "175/300, Train_loss: 0.4134 Train_dice: 0.4134\n",
            "Epoch [5/100], Batch [176/300],  Cross Loss: 0.4824\n",
            "176/300, Train_loss: 0.4824 Train_dice: 0.4824\n",
            "Epoch [5/100], Batch [177/300],  Cross Loss: 0.4009\n",
            "177/300, Train_loss: 0.4009 Train_dice: 0.4009\n",
            "Epoch [5/100], Batch [178/300],  Cross Loss: 0.4242\n",
            "178/300, Train_loss: 0.4242 Train_dice: 0.4242\n",
            "Epoch [5/100], Batch [179/300],  Cross Loss: 0.4554\n",
            "179/300, Train_loss: 0.4554 Train_dice: 0.4554\n",
            "Epoch [5/100], Batch [180/300],  Cross Loss: 0.3662\n",
            "180/300, Train_loss: 0.3662 Train_dice: 0.3662\n",
            "Epoch [5/100], Batch [181/300],  Cross Loss: 0.3586\n",
            "181/300, Train_loss: 0.3586 Train_dice: 0.3586\n",
            "Epoch [5/100], Batch [182/300],  Cross Loss: 0.3515\n",
            "182/300, Train_loss: 0.3515 Train_dice: 0.3515\n",
            "Epoch [5/100], Batch [183/300],  Cross Loss: 0.4144\n",
            "183/300, Train_loss: 0.4144 Train_dice: 0.4144\n",
            "Epoch [5/100], Batch [184/300],  Cross Loss: 0.4272\n",
            "184/300, Train_loss: 0.4272 Train_dice: 0.4272\n",
            "Epoch [5/100], Batch [185/300],  Cross Loss: 0.3301\n",
            "185/300, Train_loss: 0.3301 Train_dice: 0.3301\n",
            "Epoch [5/100], Batch [186/300],  Cross Loss: 0.2847\n",
            "186/300, Train_loss: 0.2847 Train_dice: 0.2847\n",
            "Epoch [5/100], Batch [187/300],  Cross Loss: 0.5092\n",
            "187/300, Train_loss: 0.5092 Train_dice: 0.5092\n",
            "Epoch [5/100], Batch [188/300],  Cross Loss: 0.4896\n",
            "188/300, Train_loss: 0.4896 Train_dice: 0.4896\n",
            "Epoch [5/100], Batch [189/300],  Cross Loss: 0.3939\n",
            "189/300, Train_loss: 0.3939 Train_dice: 0.3939\n",
            "Epoch [5/100], Batch [190/300],  Cross Loss: 0.3609\n",
            "190/300, Train_loss: 0.3609 Train_dice: 0.3609\n",
            "Epoch [5/100], Batch [191/300],  Cross Loss: 0.3866\n",
            "191/300, Train_loss: 0.3866 Train_dice: 0.3866\n",
            "Epoch [5/100], Batch [192/300],  Cross Loss: 0.3833\n",
            "192/300, Train_loss: 0.3833 Train_dice: 0.3833\n",
            "Epoch [5/100], Batch [193/300],  Cross Loss: 0.4086\n",
            "193/300, Train_loss: 0.4086 Train_dice: 0.4086\n",
            "Epoch [5/100], Batch [194/300],  Cross Loss: 0.3920\n",
            "194/300, Train_loss: 0.3920 Train_dice: 0.3920\n",
            "Epoch [5/100], Batch [195/300],  Cross Loss: 0.4803\n",
            "195/300, Train_loss: 0.4803 Train_dice: 0.4803\n",
            "Epoch [5/100], Batch [196/300],  Cross Loss: 0.3629\n",
            "196/300, Train_loss: 0.3629 Train_dice: 0.3629\n",
            "Epoch [5/100], Batch [197/300],  Cross Loss: 0.4916\n",
            "197/300, Train_loss: 0.4916 Train_dice: 0.4916\n",
            "Epoch [5/100], Batch [198/300],  Cross Loss: 0.3563\n",
            "198/300, Train_loss: 0.3563 Train_dice: 0.3563\n",
            "Epoch [5/100], Batch [199/300],  Cross Loss: 0.1773\n",
            "199/300, Train_loss: 0.1773 Train_dice: 0.1773\n",
            "Epoch [5/100], Batch [200/300],  Cross Loss: 0.3205\n",
            "200/300, Train_loss: 0.3205 Train_dice: 0.3205\n",
            "Epoch [5/100], Batch [201/300],  Cross Loss: 0.4192\n",
            "201/300, Train_loss: 0.4192 Train_dice: 0.4192\n",
            "Epoch [5/100], Batch [202/300],  Cross Loss: 0.3495\n",
            "202/300, Train_loss: 0.3495 Train_dice: 0.3495\n",
            "Epoch [5/100], Batch [203/300],  Cross Loss: 0.3711\n",
            "203/300, Train_loss: 0.3711 Train_dice: 0.3711\n",
            "Epoch [5/100], Batch [204/300],  Cross Loss: 0.3775\n",
            "204/300, Train_loss: 0.3775 Train_dice: 0.3775\n",
            "Epoch [5/100], Batch [205/300],  Cross Loss: 0.3729\n",
            "205/300, Train_loss: 0.3729 Train_dice: 0.3729\n",
            "Epoch [5/100], Batch [206/300],  Cross Loss: 0.3070\n",
            "206/300, Train_loss: 0.3070 Train_dice: 0.3070\n",
            "Epoch [5/100], Batch [207/300],  Cross Loss: 0.4446\n",
            "207/300, Train_loss: 0.4446 Train_dice: 0.4446\n",
            "Epoch [5/100], Batch [208/300],  Cross Loss: 0.4235\n",
            "208/300, Train_loss: 0.4235 Train_dice: 0.4235\n",
            "Epoch [5/100], Batch [209/300],  Cross Loss: 0.3532\n",
            "209/300, Train_loss: 0.3532 Train_dice: 0.3532\n",
            "Epoch [5/100], Batch [210/300],  Cross Loss: 0.4364\n",
            "210/300, Train_loss: 0.4364 Train_dice: 0.4364\n",
            "Epoch [5/100], Batch [211/300],  Cross Loss: 0.3910\n",
            "211/300, Train_loss: 0.3910 Train_dice: 0.3910\n",
            "Epoch [5/100], Batch [212/300],  Cross Loss: 0.3714\n",
            "212/300, Train_loss: 0.3714 Train_dice: 0.3714\n",
            "Epoch [5/100], Batch [213/300],  Cross Loss: 0.3810\n",
            "213/300, Train_loss: 0.3810 Train_dice: 0.3810\n",
            "Epoch [5/100], Batch [214/300],  Cross Loss: 0.3502\n",
            "214/300, Train_loss: 0.3502 Train_dice: 0.3502\n",
            "Epoch [5/100], Batch [215/300],  Cross Loss: 0.4389\n",
            "215/300, Train_loss: 0.4389 Train_dice: 0.4389\n",
            "Epoch [5/100], Batch [216/300],  Cross Loss: 0.4468\n",
            "216/300, Train_loss: 0.4468 Train_dice: 0.4468\n",
            "Epoch [5/100], Batch [217/300],  Cross Loss: 0.2959\n",
            "217/300, Train_loss: 0.2959 Train_dice: 0.2959\n",
            "Epoch [5/100], Batch [218/300],  Cross Loss: 0.3455\n",
            "218/300, Train_loss: 0.3455 Train_dice: 0.3455\n",
            "Epoch [5/100], Batch [219/300],  Cross Loss: 0.4239\n",
            "219/300, Train_loss: 0.4239 Train_dice: 0.4239\n",
            "Epoch [5/100], Batch [220/300],  Cross Loss: 0.4452\n",
            "220/300, Train_loss: 0.4452 Train_dice: 0.4452\n",
            "Epoch [5/100], Batch [221/300],  Cross Loss: 0.3594\n",
            "221/300, Train_loss: 0.3594 Train_dice: 0.3594\n",
            "Epoch [5/100], Batch [222/300],  Cross Loss: 0.4558\n",
            "222/300, Train_loss: 0.4558 Train_dice: 0.4558\n",
            "Epoch [5/100], Batch [223/300],  Cross Loss: 0.4476\n",
            "223/300, Train_loss: 0.4476 Train_dice: 0.4476\n",
            "Epoch [5/100], Batch [224/300],  Cross Loss: 0.4209\n",
            "224/300, Train_loss: 0.4209 Train_dice: 0.4209\n",
            "Epoch [5/100], Batch [225/300],  Cross Loss: 0.3393\n",
            "225/300, Train_loss: 0.3393 Train_dice: 0.3393\n",
            "Epoch [5/100], Batch [226/300],  Cross Loss: 0.4375\n",
            "226/300, Train_loss: 0.4375 Train_dice: 0.4375\n",
            "Epoch [5/100], Batch [227/300],  Cross Loss: 0.4398\n",
            "227/300, Train_loss: 0.4398 Train_dice: 0.4398\n",
            "Epoch [5/100], Batch [228/300],  Cross Loss: 0.4503\n",
            "228/300, Train_loss: 0.4503 Train_dice: 0.4503\n",
            "Epoch [5/100], Batch [229/300],  Cross Loss: 0.3378\n",
            "229/300, Train_loss: 0.3378 Train_dice: 0.3378\n",
            "Epoch [5/100], Batch [230/300],  Cross Loss: 0.4638\n",
            "230/300, Train_loss: 0.4638 Train_dice: 0.4638\n",
            "Epoch [5/100], Batch [231/300],  Cross Loss: 0.3688\n",
            "231/300, Train_loss: 0.3688 Train_dice: 0.3688\n",
            "Epoch [5/100], Batch [232/300],  Cross Loss: 0.3513\n",
            "232/300, Train_loss: 0.3513 Train_dice: 0.3513\n",
            "Epoch [5/100], Batch [233/300],  Cross Loss: 0.3326\n",
            "233/300, Train_loss: 0.3326 Train_dice: 0.3326\n",
            "Epoch [5/100], Batch [234/300],  Cross Loss: 0.4158\n",
            "234/300, Train_loss: 0.4158 Train_dice: 0.4158\n",
            "Epoch [5/100], Batch [235/300],  Cross Loss: 0.4727\n",
            "235/300, Train_loss: 0.4727 Train_dice: 0.4727\n",
            "Epoch [5/100], Batch [236/300],  Cross Loss: 0.4076\n",
            "236/300, Train_loss: 0.4076 Train_dice: 0.4076\n",
            "Epoch [5/100], Batch [237/300],  Cross Loss: 0.2991\n",
            "237/300, Train_loss: 0.2991 Train_dice: 0.2991\n",
            "Epoch [5/100], Batch [238/300],  Cross Loss: 0.5576\n",
            "238/300, Train_loss: 0.5576 Train_dice: 0.5576\n",
            "Epoch [5/100], Batch [239/300],  Cross Loss: 0.4061\n",
            "239/300, Train_loss: 0.4061 Train_dice: 0.4061\n",
            "Epoch [5/100], Batch [240/300],  Cross Loss: 0.4114\n",
            "240/300, Train_loss: 0.4114 Train_dice: 0.4114\n",
            "Epoch [5/100], Batch [241/300],  Cross Loss: 0.3894\n",
            "241/300, Train_loss: 0.3894 Train_dice: 0.3894\n",
            "Epoch [5/100], Batch [242/300],  Cross Loss: 0.4378\n",
            "242/300, Train_loss: 0.4378 Train_dice: 0.4378\n",
            "Epoch [5/100], Batch [243/300],  Cross Loss: 0.3385\n",
            "243/300, Train_loss: 0.3385 Train_dice: 0.3385\n",
            "Epoch [5/100], Batch [244/300],  Cross Loss: 0.3975\n",
            "244/300, Train_loss: 0.3975 Train_dice: 0.3975\n",
            "Epoch [5/100], Batch [245/300],  Cross Loss: 0.2730\n",
            "245/300, Train_loss: 0.2730 Train_dice: 0.2730\n",
            "Epoch [5/100], Batch [246/300],  Cross Loss: 0.3725\n",
            "246/300, Train_loss: 0.3725 Train_dice: 0.3725\n",
            "Epoch [5/100], Batch [247/300],  Cross Loss: 0.4157\n",
            "247/300, Train_loss: 0.4157 Train_dice: 0.4157\n",
            "Epoch [5/100], Batch [248/300],  Cross Loss: 0.3445\n",
            "248/300, Train_loss: 0.3445 Train_dice: 0.3445\n",
            "Epoch [5/100], Batch [249/300],  Cross Loss: 0.3094\n",
            "249/300, Train_loss: 0.3094 Train_dice: 0.3094\n",
            "Epoch [5/100], Batch [250/300],  Cross Loss: 0.2954\n",
            "250/300, Train_loss: 0.2954 Train_dice: 0.2954\n",
            "Epoch [5/100], Batch [251/300],  Cross Loss: 0.3812\n",
            "251/300, Train_loss: 0.3812 Train_dice: 0.3812\n",
            "Epoch [5/100], Batch [252/300],  Cross Loss: 0.2838\n",
            "252/300, Train_loss: 0.2838 Train_dice: 0.2838\n",
            "Epoch [5/100], Batch [253/300],  Cross Loss: 0.3638\n",
            "253/300, Train_loss: 0.3638 Train_dice: 0.3638\n",
            "Epoch [5/100], Batch [254/300],  Cross Loss: 0.4189\n",
            "254/300, Train_loss: 0.4189 Train_dice: 0.4189\n",
            "Epoch [5/100], Batch [255/300],  Cross Loss: 0.3445\n",
            "255/300, Train_loss: 0.3445 Train_dice: 0.3445\n",
            "Epoch [5/100], Batch [256/300],  Cross Loss: 0.3469\n",
            "256/300, Train_loss: 0.3469 Train_dice: 0.3469\n",
            "Epoch [5/100], Batch [257/300],  Cross Loss: 0.4598\n",
            "257/300, Train_loss: 0.4598 Train_dice: 0.4598\n",
            "Epoch [5/100], Batch [258/300],  Cross Loss: 0.4163\n",
            "258/300, Train_loss: 0.4163 Train_dice: 0.4163\n",
            "Epoch [5/100], Batch [259/300],  Cross Loss: 0.2696\n",
            "259/300, Train_loss: 0.2696 Train_dice: 0.2696\n",
            "Epoch [5/100], Batch [260/300],  Cross Loss: 0.5200\n",
            "260/300, Train_loss: 0.5200 Train_dice: 0.5200\n",
            "Epoch [5/100], Batch [261/300],  Cross Loss: 0.3695\n",
            "261/300, Train_loss: 0.3695 Train_dice: 0.3695\n",
            "Epoch [5/100], Batch [262/300],  Cross Loss: 0.2612\n",
            "262/300, Train_loss: 0.2612 Train_dice: 0.2612\n",
            "Epoch [5/100], Batch [263/300],  Cross Loss: 0.4196\n",
            "263/300, Train_loss: 0.4196 Train_dice: 0.4196\n",
            "Epoch [5/100], Batch [264/300],  Cross Loss: 0.2886\n",
            "264/300, Train_loss: 0.2886 Train_dice: 0.2886\n",
            "Epoch [5/100], Batch [265/300],  Cross Loss: 0.4529\n",
            "265/300, Train_loss: 0.4529 Train_dice: 0.4529\n",
            "Epoch [5/100], Batch [266/300],  Cross Loss: 0.4149\n",
            "266/300, Train_loss: 0.4149 Train_dice: 0.4149\n",
            "Epoch [5/100], Batch [267/300],  Cross Loss: 0.4227\n",
            "267/300, Train_loss: 0.4227 Train_dice: 0.4227\n",
            "Epoch [5/100], Batch [268/300],  Cross Loss: 0.2383\n",
            "268/300, Train_loss: 0.2383 Train_dice: 0.2383\n",
            "Epoch [5/100], Batch [269/300],  Cross Loss: 0.3941\n",
            "269/300, Train_loss: 0.3941 Train_dice: 0.3941\n",
            "Epoch [5/100], Batch [270/300],  Cross Loss: 0.3217\n",
            "270/300, Train_loss: 0.3217 Train_dice: 0.3217\n",
            "Epoch [5/100], Batch [271/300],  Cross Loss: 0.4282\n",
            "271/300, Train_loss: 0.4282 Train_dice: 0.4282\n",
            "Epoch [5/100], Batch [272/300],  Cross Loss: 0.3585\n",
            "272/300, Train_loss: 0.3585 Train_dice: 0.3585\n",
            "Epoch [5/100], Batch [273/300],  Cross Loss: 0.3025\n",
            "273/300, Train_loss: 0.3025 Train_dice: 0.3025\n",
            "Epoch [5/100], Batch [274/300],  Cross Loss: 0.4104\n",
            "274/300, Train_loss: 0.4104 Train_dice: 0.4104\n",
            "Epoch [5/100], Batch [275/300],  Cross Loss: 0.3104\n",
            "275/300, Train_loss: 0.3104 Train_dice: 0.3104\n",
            "Epoch [5/100], Batch [276/300],  Cross Loss: 0.3720\n",
            "276/300, Train_loss: 0.3720 Train_dice: 0.3720\n",
            "Epoch [5/100], Batch [277/300],  Cross Loss: 0.4058\n",
            "277/300, Train_loss: 0.4058 Train_dice: 0.4058\n",
            "Epoch [5/100], Batch [278/300],  Cross Loss: 0.4546\n",
            "278/300, Train_loss: 0.4546 Train_dice: 0.4546\n",
            "Epoch [5/100], Batch [279/300],  Cross Loss: 0.3285\n",
            "279/300, Train_loss: 0.3285 Train_dice: 0.3285\n",
            "Epoch [5/100], Batch [280/300],  Cross Loss: 0.3520\n",
            "280/300, Train_loss: 0.3520 Train_dice: 0.3520\n",
            "Epoch [5/100], Batch [281/300],  Cross Loss: 0.4973\n",
            "281/300, Train_loss: 0.4973 Train_dice: 0.4973\n",
            "Epoch [5/100], Batch [282/300],  Cross Loss: 0.3907\n",
            "282/300, Train_loss: 0.3907 Train_dice: 0.3907\n",
            "Epoch [5/100], Batch [283/300],  Cross Loss: 0.2353\n",
            "283/300, Train_loss: 0.2353 Train_dice: 0.2353\n",
            "Epoch [5/100], Batch [284/300],  Cross Loss: 0.4177\n",
            "284/300, Train_loss: 0.4177 Train_dice: 0.4177\n",
            "Epoch [5/100], Batch [285/300],  Cross Loss: 0.4134\n",
            "285/300, Train_loss: 0.4134 Train_dice: 0.4134\n",
            "Epoch [5/100], Batch [286/300],  Cross Loss: 0.1832\n",
            "286/300, Train_loss: 0.1832 Train_dice: 0.1832\n",
            "Epoch [5/100], Batch [287/300],  Cross Loss: 0.4065\n",
            "287/300, Train_loss: 0.4065 Train_dice: 0.4065\n",
            "Epoch [5/100], Batch [288/300],  Cross Loss: 0.4996\n",
            "288/300, Train_loss: 0.4996 Train_dice: 0.4996\n",
            "Epoch [5/100], Batch [289/300],  Cross Loss: 0.3975\n",
            "289/300, Train_loss: 0.3975 Train_dice: 0.3975\n",
            "Epoch [5/100], Batch [290/300],  Cross Loss: 0.4086\n",
            "290/300, Train_loss: 0.4086 Train_dice: 0.4086\n",
            "Epoch [5/100], Batch [291/300],  Cross Loss: 0.5050\n",
            "291/300, Train_loss: 0.5050 Train_dice: 0.5050\n",
            "Epoch [5/100], Batch [292/300],  Cross Loss: 0.3511\n",
            "292/300, Train_loss: 0.3511 Train_dice: 0.3511\n",
            "Epoch [5/100], Batch [293/300],  Cross Loss: 0.4048\n",
            "293/300, Train_loss: 0.4048 Train_dice: 0.4048\n",
            "Epoch [5/100], Batch [294/300],  Cross Loss: 0.4074\n",
            "294/300, Train_loss: 0.4074 Train_dice: 0.4074\n",
            "Epoch [5/100], Batch [295/300],  Cross Loss: 0.3758\n",
            "295/300, Train_loss: 0.3758 Train_dice: 0.3758\n",
            "Epoch [5/100], Batch [296/300],  Cross Loss: 0.4937\n",
            "296/300, Train_loss: 0.4937 Train_dice: 0.4937\n",
            "Epoch [5/100], Batch [297/300],  Cross Loss: 0.2804\n",
            "297/300, Train_loss: 0.2804 Train_dice: 0.2804\n",
            "Epoch [5/100], Batch [298/300],  Cross Loss: 0.4450\n",
            "298/300, Train_loss: 0.4450 Train_dice: 0.4450\n",
            "Epoch [5/100], Batch [299/300],  Cross Loss: 0.3580\n",
            "299/300, Train_loss: 0.3580 Train_dice: 0.3580\n",
            "Epoch [5/100], Batch [300/300],  Cross Loss: 0.2127\n",
            "300/300, Train_loss: 0.2127 Train_dice: 0.2127\n",
            "--------------------\n",
            "Epoch_loss: 0.3887\n",
            "Epoch_metric: tensor(0.3887, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.4307\n",
            "test_dice_epoch: tensor(0.4307, device='cuda:0')\n",
            "current epoch: 5 current mean dice: tensor(0.4307, device='cuda:0')\n",
            "best mean dice: tensor(0.4307, device='cuda:0') at epoch: 5\n",
            "----------\n",
            "epoch 6/100\n",
            "Epoch [6/100], Batch [1/300],  Cross Loss: 0.2885\n",
            "1/300, Train_loss: 0.2885 Train_dice: 0.2885\n",
            "Epoch [6/100], Batch [2/300],  Cross Loss: 0.4734\n",
            "2/300, Train_loss: 0.4734 Train_dice: 0.4734\n",
            "Epoch [6/100], Batch [3/300],  Cross Loss: 0.4794\n",
            "3/300, Train_loss: 0.4794 Train_dice: 0.4794\n",
            "Epoch [6/100], Batch [4/300],  Cross Loss: 0.4479\n",
            "4/300, Train_loss: 0.4479 Train_dice: 0.4479\n",
            "Epoch [6/100], Batch [5/300],  Cross Loss: 0.3554\n",
            "5/300, Train_loss: 0.3554 Train_dice: 0.3554\n",
            "Epoch [6/100], Batch [6/300],  Cross Loss: 0.3189\n",
            "6/300, Train_loss: 0.3189 Train_dice: 0.3189\n",
            "Epoch [6/100], Batch [7/300],  Cross Loss: 0.4009\n",
            "7/300, Train_loss: 0.4009 Train_dice: 0.4009\n",
            "Epoch [6/100], Batch [8/300],  Cross Loss: 0.4993\n",
            "8/300, Train_loss: 0.4993 Train_dice: 0.4993\n",
            "Epoch [6/100], Batch [9/300],  Cross Loss: 0.4025\n",
            "9/300, Train_loss: 0.4025 Train_dice: 0.4025\n",
            "Epoch [6/100], Batch [10/300],  Cross Loss: 0.4535\n",
            "10/300, Train_loss: 0.4535 Train_dice: 0.4535\n",
            "Epoch [6/100], Batch [11/300],  Cross Loss: 0.3292\n",
            "11/300, Train_loss: 0.3292 Train_dice: 0.3292\n",
            "Epoch [6/100], Batch [12/300],  Cross Loss: 0.3154\n",
            "12/300, Train_loss: 0.3154 Train_dice: 0.3154\n",
            "Epoch [6/100], Batch [13/300],  Cross Loss: 0.5077\n",
            "13/300, Train_loss: 0.5077 Train_dice: 0.5077\n",
            "Epoch [6/100], Batch [14/300],  Cross Loss: 0.2909\n",
            "14/300, Train_loss: 0.2909 Train_dice: 0.2909\n",
            "Epoch [6/100], Batch [15/300],  Cross Loss: 0.3965\n",
            "15/300, Train_loss: 0.3965 Train_dice: 0.3965\n",
            "Epoch [6/100], Batch [16/300],  Cross Loss: 0.3968\n",
            "16/300, Train_loss: 0.3968 Train_dice: 0.3968\n",
            "Epoch [6/100], Batch [17/300],  Cross Loss: 0.3033\n",
            "17/300, Train_loss: 0.3033 Train_dice: 0.3033\n",
            "Epoch [6/100], Batch [18/300],  Cross Loss: 0.3952\n",
            "18/300, Train_loss: 0.3952 Train_dice: 0.3952\n",
            "Epoch [6/100], Batch [19/300],  Cross Loss: 0.3974\n",
            "19/300, Train_loss: 0.3974 Train_dice: 0.3974\n",
            "Epoch [6/100], Batch [20/300],  Cross Loss: 0.3563\n",
            "20/300, Train_loss: 0.3563 Train_dice: 0.3563\n",
            "Epoch [6/100], Batch [21/300],  Cross Loss: 0.4472\n",
            "21/300, Train_loss: 0.4472 Train_dice: 0.4472\n",
            "Epoch [6/100], Batch [22/300],  Cross Loss: 0.4902\n",
            "22/300, Train_loss: 0.4902 Train_dice: 0.4902\n",
            "Epoch [6/100], Batch [23/300],  Cross Loss: 0.4366\n",
            "23/300, Train_loss: 0.4366 Train_dice: 0.4366\n",
            "Epoch [6/100], Batch [24/300],  Cross Loss: 0.3898\n",
            "24/300, Train_loss: 0.3898 Train_dice: 0.3898\n",
            "Epoch [6/100], Batch [25/300],  Cross Loss: 0.2831\n",
            "25/300, Train_loss: 0.2831 Train_dice: 0.2831\n",
            "Epoch [6/100], Batch [26/300],  Cross Loss: 0.2034\n",
            "26/300, Train_loss: 0.2034 Train_dice: 0.2034\n",
            "Epoch [6/100], Batch [27/300],  Cross Loss: 0.4182\n",
            "27/300, Train_loss: 0.4182 Train_dice: 0.4182\n",
            "Epoch [6/100], Batch [28/300],  Cross Loss: 0.4025\n",
            "28/300, Train_loss: 0.4025 Train_dice: 0.4025\n",
            "Epoch [6/100], Batch [29/300],  Cross Loss: 0.3812\n",
            "29/300, Train_loss: 0.3812 Train_dice: 0.3812\n",
            "Epoch [6/100], Batch [30/300],  Cross Loss: 0.4706\n",
            "30/300, Train_loss: 0.4706 Train_dice: 0.4706\n",
            "Epoch [6/100], Batch [31/300],  Cross Loss: 0.4116\n",
            "31/300, Train_loss: 0.4116 Train_dice: 0.4116\n",
            "Epoch [6/100], Batch [32/300],  Cross Loss: 0.2494\n",
            "32/300, Train_loss: 0.2494 Train_dice: 0.2494\n",
            "Epoch [6/100], Batch [33/300],  Cross Loss: 0.3546\n",
            "33/300, Train_loss: 0.3546 Train_dice: 0.3546\n",
            "Epoch [6/100], Batch [34/300],  Cross Loss: 0.3635\n",
            "34/300, Train_loss: 0.3635 Train_dice: 0.3635\n",
            "Epoch [6/100], Batch [35/300],  Cross Loss: 0.2969\n",
            "35/300, Train_loss: 0.2969 Train_dice: 0.2969\n",
            "Epoch [6/100], Batch [36/300],  Cross Loss: 0.1392\n",
            "36/300, Train_loss: 0.1392 Train_dice: 0.1392\n",
            "Epoch [6/100], Batch [37/300],  Cross Loss: 0.4775\n",
            "37/300, Train_loss: 0.4775 Train_dice: 0.4775\n",
            "Epoch [6/100], Batch [38/300],  Cross Loss: 0.3822\n",
            "38/300, Train_loss: 0.3822 Train_dice: 0.3822\n",
            "Epoch [6/100], Batch [39/300],  Cross Loss: 0.3964\n",
            "39/300, Train_loss: 0.3964 Train_dice: 0.3964\n",
            "Epoch [6/100], Batch [40/300],  Cross Loss: 0.3673\n",
            "40/300, Train_loss: 0.3673 Train_dice: 0.3673\n",
            "Epoch [6/100], Batch [41/300],  Cross Loss: 0.2567\n",
            "41/300, Train_loss: 0.2567 Train_dice: 0.2567\n",
            "Epoch [6/100], Batch [42/300],  Cross Loss: 0.2865\n",
            "42/300, Train_loss: 0.2865 Train_dice: 0.2865\n",
            "Epoch [6/100], Batch [43/300],  Cross Loss: 0.3681\n",
            "43/300, Train_loss: 0.3681 Train_dice: 0.3681\n",
            "Epoch [6/100], Batch [44/300],  Cross Loss: 0.2828\n",
            "44/300, Train_loss: 0.2828 Train_dice: 0.2828\n",
            "Epoch [6/100], Batch [45/300],  Cross Loss: 0.4616\n",
            "45/300, Train_loss: 0.4616 Train_dice: 0.4616\n",
            "Epoch [6/100], Batch [46/300],  Cross Loss: 0.4344\n",
            "46/300, Train_loss: 0.4344 Train_dice: 0.4344\n",
            "Epoch [6/100], Batch [47/300],  Cross Loss: 0.4080\n",
            "47/300, Train_loss: 0.4080 Train_dice: 0.4080\n",
            "Epoch [6/100], Batch [48/300],  Cross Loss: 0.4921\n",
            "48/300, Train_loss: 0.4921 Train_dice: 0.4921\n",
            "Epoch [6/100], Batch [49/300],  Cross Loss: 0.2453\n",
            "49/300, Train_loss: 0.2453 Train_dice: 0.2453\n",
            "Epoch [6/100], Batch [50/300],  Cross Loss: 0.4370\n",
            "50/300, Train_loss: 0.4370 Train_dice: 0.4370\n",
            "Epoch [6/100], Batch [51/300],  Cross Loss: 0.4941\n",
            "51/300, Train_loss: 0.4941 Train_dice: 0.4941\n",
            "Epoch [6/100], Batch [52/300],  Cross Loss: 0.4080\n",
            "52/300, Train_loss: 0.4080 Train_dice: 0.4080\n",
            "Epoch [6/100], Batch [53/300],  Cross Loss: 0.3987\n",
            "53/300, Train_loss: 0.3987 Train_dice: 0.3987\n",
            "Epoch [6/100], Batch [54/300],  Cross Loss: 0.4048\n",
            "54/300, Train_loss: 0.4048 Train_dice: 0.4048\n",
            "Epoch [6/100], Batch [55/300],  Cross Loss: 0.4131\n",
            "55/300, Train_loss: 0.4131 Train_dice: 0.4131\n",
            "Epoch [6/100], Batch [56/300],  Cross Loss: 0.1956\n",
            "56/300, Train_loss: 0.1956 Train_dice: 0.1956\n",
            "Epoch [6/100], Batch [57/300],  Cross Loss: 0.3418\n",
            "57/300, Train_loss: 0.3418 Train_dice: 0.3418\n",
            "Epoch [6/100], Batch [58/300],  Cross Loss: 0.3472\n",
            "58/300, Train_loss: 0.3472 Train_dice: 0.3472\n",
            "Epoch [6/100], Batch [59/300],  Cross Loss: 0.4349\n",
            "59/300, Train_loss: 0.4349 Train_dice: 0.4349\n",
            "Epoch [6/100], Batch [60/300],  Cross Loss: 0.3561\n",
            "60/300, Train_loss: 0.3561 Train_dice: 0.3561\n",
            "Epoch [6/100], Batch [61/300],  Cross Loss: 0.3342\n",
            "61/300, Train_loss: 0.3342 Train_dice: 0.3342\n",
            "Epoch [6/100], Batch [62/300],  Cross Loss: 0.4142\n",
            "62/300, Train_loss: 0.4142 Train_dice: 0.4142\n",
            "Epoch [6/100], Batch [63/300],  Cross Loss: 0.3849\n",
            "63/300, Train_loss: 0.3849 Train_dice: 0.3849\n",
            "Epoch [6/100], Batch [64/300],  Cross Loss: 0.4385\n",
            "64/300, Train_loss: 0.4385 Train_dice: 0.4385\n",
            "Epoch [6/100], Batch [65/300],  Cross Loss: 0.3890\n",
            "65/300, Train_loss: 0.3890 Train_dice: 0.3890\n",
            "Epoch [6/100], Batch [66/300],  Cross Loss: 0.2895\n",
            "66/300, Train_loss: 0.2895 Train_dice: 0.2895\n",
            "Epoch [6/100], Batch [67/300],  Cross Loss: 0.4870\n",
            "67/300, Train_loss: 0.4870 Train_dice: 0.4870\n",
            "Epoch [6/100], Batch [68/300],  Cross Loss: 0.3457\n",
            "68/300, Train_loss: 0.3457 Train_dice: 0.3457\n",
            "Epoch [6/100], Batch [69/300],  Cross Loss: 0.4568\n",
            "69/300, Train_loss: 0.4568 Train_dice: 0.4568\n",
            "Epoch [6/100], Batch [70/300],  Cross Loss: 0.4796\n",
            "70/300, Train_loss: 0.4796 Train_dice: 0.4796\n",
            "Epoch [6/100], Batch [71/300],  Cross Loss: 0.3957\n",
            "71/300, Train_loss: 0.3957 Train_dice: 0.3957\n",
            "Epoch [6/100], Batch [72/300],  Cross Loss: 0.3805\n",
            "72/300, Train_loss: 0.3805 Train_dice: 0.3805\n",
            "Epoch [6/100], Batch [73/300],  Cross Loss: 0.4900\n",
            "73/300, Train_loss: 0.4900 Train_dice: 0.4900\n",
            "Epoch [6/100], Batch [74/300],  Cross Loss: 0.1810\n",
            "74/300, Train_loss: 0.1810 Train_dice: 0.1810\n",
            "Epoch [6/100], Batch [75/300],  Cross Loss: 0.3955\n",
            "75/300, Train_loss: 0.3955 Train_dice: 0.3955\n",
            "Epoch [6/100], Batch [76/300],  Cross Loss: 0.3196\n",
            "76/300, Train_loss: 0.3196 Train_dice: 0.3196\n",
            "Epoch [6/100], Batch [77/300],  Cross Loss: 0.2398\n",
            "77/300, Train_loss: 0.2398 Train_dice: 0.2398\n",
            "Epoch [6/100], Batch [78/300],  Cross Loss: 0.4223\n",
            "78/300, Train_loss: 0.4223 Train_dice: 0.4223\n",
            "Epoch [6/100], Batch [79/300],  Cross Loss: 0.4360\n",
            "79/300, Train_loss: 0.4360 Train_dice: 0.4360\n",
            "Epoch [6/100], Batch [80/300],  Cross Loss: 0.3315\n",
            "80/300, Train_loss: 0.3315 Train_dice: 0.3315\n",
            "Epoch [6/100], Batch [81/300],  Cross Loss: 0.3937\n",
            "81/300, Train_loss: 0.3937 Train_dice: 0.3937\n",
            "Epoch [6/100], Batch [82/300],  Cross Loss: 0.3433\n",
            "82/300, Train_loss: 0.3433 Train_dice: 0.3433\n",
            "Epoch [6/100], Batch [83/300],  Cross Loss: 0.4334\n",
            "83/300, Train_loss: 0.4334 Train_dice: 0.4334\n",
            "Epoch [6/100], Batch [84/300],  Cross Loss: 0.4553\n",
            "84/300, Train_loss: 0.4553 Train_dice: 0.4553\n",
            "Epoch [6/100], Batch [85/300],  Cross Loss: 0.4347\n",
            "85/300, Train_loss: 0.4347 Train_dice: 0.4347\n",
            "Epoch [6/100], Batch [86/300],  Cross Loss: 0.3006\n",
            "86/300, Train_loss: 0.3006 Train_dice: 0.3006\n",
            "Epoch [6/100], Batch [87/300],  Cross Loss: 0.2436\n",
            "87/300, Train_loss: 0.2436 Train_dice: 0.2436\n",
            "Epoch [6/100], Batch [88/300],  Cross Loss: 0.1975\n",
            "88/300, Train_loss: 0.1975 Train_dice: 0.1975\n",
            "Epoch [6/100], Batch [89/300],  Cross Loss: 0.4301\n",
            "89/300, Train_loss: 0.4301 Train_dice: 0.4301\n",
            "Epoch [6/100], Batch [90/300],  Cross Loss: 0.4221\n",
            "90/300, Train_loss: 0.4221 Train_dice: 0.4221\n",
            "Epoch [6/100], Batch [91/300],  Cross Loss: 0.3273\n",
            "91/300, Train_loss: 0.3273 Train_dice: 0.3273\n",
            "Epoch [6/100], Batch [92/300],  Cross Loss: 0.3251\n",
            "92/300, Train_loss: 0.3251 Train_dice: 0.3251\n",
            "Epoch [6/100], Batch [93/300],  Cross Loss: 0.3955\n",
            "93/300, Train_loss: 0.3955 Train_dice: 0.3955\n",
            "Epoch [6/100], Batch [94/300],  Cross Loss: 0.2613\n",
            "94/300, Train_loss: 0.2613 Train_dice: 0.2613\n",
            "Epoch [6/100], Batch [95/300],  Cross Loss: 0.3872\n",
            "95/300, Train_loss: 0.3872 Train_dice: 0.3872\n",
            "Epoch [6/100], Batch [96/300],  Cross Loss: 0.2630\n",
            "96/300, Train_loss: 0.2630 Train_dice: 0.2630\n",
            "Epoch [6/100], Batch [97/300],  Cross Loss: 0.4086\n",
            "97/300, Train_loss: 0.4086 Train_dice: 0.4086\n",
            "Epoch [6/100], Batch [98/300],  Cross Loss: 0.4244\n",
            "98/300, Train_loss: 0.4244 Train_dice: 0.4244\n",
            "Epoch [6/100], Batch [99/300],  Cross Loss: 0.3357\n",
            "99/300, Train_loss: 0.3357 Train_dice: 0.3357\n",
            "Epoch [6/100], Batch [100/300],  Cross Loss: 0.4594\n",
            "100/300, Train_loss: 0.4594 Train_dice: 0.4594\n",
            "Epoch [6/100], Batch [101/300],  Cross Loss: 0.3821\n",
            "101/300, Train_loss: 0.3821 Train_dice: 0.3821\n",
            "Epoch [6/100], Batch [102/300],  Cross Loss: 0.3255\n",
            "102/300, Train_loss: 0.3255 Train_dice: 0.3255\n",
            "Epoch [6/100], Batch [103/300],  Cross Loss: 0.4004\n",
            "103/300, Train_loss: 0.4004 Train_dice: 0.4004\n",
            "Epoch [6/100], Batch [104/300],  Cross Loss: 0.3557\n",
            "104/300, Train_loss: 0.3557 Train_dice: 0.3557\n",
            "Epoch [6/100], Batch [105/300],  Cross Loss: 0.4035\n",
            "105/300, Train_loss: 0.4035 Train_dice: 0.4035\n",
            "Epoch [6/100], Batch [106/300],  Cross Loss: 0.3783\n",
            "106/300, Train_loss: 0.3783 Train_dice: 0.3783\n",
            "Epoch [6/100], Batch [107/300],  Cross Loss: 0.4949\n",
            "107/300, Train_loss: 0.4949 Train_dice: 0.4949\n",
            "Epoch [6/100], Batch [108/300],  Cross Loss: 0.3740\n",
            "108/300, Train_loss: 0.3740 Train_dice: 0.3740\n",
            "Epoch [6/100], Batch [109/300],  Cross Loss: 0.4157\n",
            "109/300, Train_loss: 0.4157 Train_dice: 0.4157\n",
            "Epoch [6/100], Batch [110/300],  Cross Loss: 0.1939\n",
            "110/300, Train_loss: 0.1939 Train_dice: 0.1939\n",
            "Epoch [6/100], Batch [111/300],  Cross Loss: 0.4234\n",
            "111/300, Train_loss: 0.4234 Train_dice: 0.4234\n",
            "Epoch [6/100], Batch [112/300],  Cross Loss: 0.3903\n",
            "112/300, Train_loss: 0.3903 Train_dice: 0.3903\n",
            "Epoch [6/100], Batch [113/300],  Cross Loss: 0.3846\n",
            "113/300, Train_loss: 0.3846 Train_dice: 0.3846\n",
            "Epoch [6/100], Batch [114/300],  Cross Loss: 0.3188\n",
            "114/300, Train_loss: 0.3188 Train_dice: 0.3188\n",
            "Epoch [6/100], Batch [115/300],  Cross Loss: 0.3539\n",
            "115/300, Train_loss: 0.3539 Train_dice: 0.3539\n",
            "Epoch [6/100], Batch [116/300],  Cross Loss: 0.3778\n",
            "116/300, Train_loss: 0.3778 Train_dice: 0.3778\n",
            "Epoch [6/100], Batch [117/300],  Cross Loss: 0.4048\n",
            "117/300, Train_loss: 0.4048 Train_dice: 0.4048\n",
            "Epoch [6/100], Batch [118/300],  Cross Loss: 0.3702\n",
            "118/300, Train_loss: 0.3702 Train_dice: 0.3702\n",
            "Epoch [6/100], Batch [119/300],  Cross Loss: 0.4031\n",
            "119/300, Train_loss: 0.4031 Train_dice: 0.4031\n",
            "Epoch [6/100], Batch [120/300],  Cross Loss: 0.4593\n",
            "120/300, Train_loss: 0.4593 Train_dice: 0.4593\n",
            "Epoch [6/100], Batch [121/300],  Cross Loss: 0.1340\n",
            "121/300, Train_loss: 0.1340 Train_dice: 0.1340\n",
            "Epoch [6/100], Batch [122/300],  Cross Loss: 0.4608\n",
            "122/300, Train_loss: 0.4608 Train_dice: 0.4608\n",
            "Epoch [6/100], Batch [123/300],  Cross Loss: 0.4024\n",
            "123/300, Train_loss: 0.4024 Train_dice: 0.4024\n",
            "Epoch [6/100], Batch [124/300],  Cross Loss: 0.4117\n",
            "124/300, Train_loss: 0.4117 Train_dice: 0.4117\n",
            "Epoch [6/100], Batch [125/300],  Cross Loss: 0.3571\n",
            "125/300, Train_loss: 0.3571 Train_dice: 0.3571\n",
            "Epoch [6/100], Batch [126/300],  Cross Loss: 0.3777\n",
            "126/300, Train_loss: 0.3777 Train_dice: 0.3777\n",
            "Epoch [6/100], Batch [127/300],  Cross Loss: 0.4230\n",
            "127/300, Train_loss: 0.4230 Train_dice: 0.4230\n",
            "Epoch [6/100], Batch [128/300],  Cross Loss: 0.2229\n",
            "128/300, Train_loss: 0.2229 Train_dice: 0.2229\n",
            "Epoch [6/100], Batch [129/300],  Cross Loss: 0.4085\n",
            "129/300, Train_loss: 0.4085 Train_dice: 0.4085\n",
            "Epoch [6/100], Batch [130/300],  Cross Loss: 0.2428\n",
            "130/300, Train_loss: 0.2428 Train_dice: 0.2428\n",
            "Epoch [6/100], Batch [131/300],  Cross Loss: 0.4270\n",
            "131/300, Train_loss: 0.4270 Train_dice: 0.4270\n",
            "Epoch [6/100], Batch [132/300],  Cross Loss: 0.3234\n",
            "132/300, Train_loss: 0.3234 Train_dice: 0.3234\n",
            "Epoch [6/100], Batch [133/300],  Cross Loss: 0.4295\n",
            "133/300, Train_loss: 0.4295 Train_dice: 0.4295\n",
            "Epoch [6/100], Batch [134/300],  Cross Loss: 0.3295\n",
            "134/300, Train_loss: 0.3295 Train_dice: 0.3295\n",
            "Epoch [6/100], Batch [135/300],  Cross Loss: 0.3628\n",
            "135/300, Train_loss: 0.3628 Train_dice: 0.3628\n",
            "Epoch [6/100], Batch [136/300],  Cross Loss: 0.4963\n",
            "136/300, Train_loss: 0.4963 Train_dice: 0.4963\n",
            "Epoch [6/100], Batch [137/300],  Cross Loss: 0.4481\n",
            "137/300, Train_loss: 0.4481 Train_dice: 0.4481\n",
            "Epoch [6/100], Batch [138/300],  Cross Loss: 0.4785\n",
            "138/300, Train_loss: 0.4785 Train_dice: 0.4785\n",
            "Epoch [6/100], Batch [139/300],  Cross Loss: 0.1807\n",
            "139/300, Train_loss: 0.1807 Train_dice: 0.1807\n",
            "Epoch [6/100], Batch [140/300],  Cross Loss: 0.3433\n",
            "140/300, Train_loss: 0.3433 Train_dice: 0.3433\n",
            "Epoch [6/100], Batch [141/300],  Cross Loss: 0.4228\n",
            "141/300, Train_loss: 0.4228 Train_dice: 0.4228\n",
            "Epoch [6/100], Batch [142/300],  Cross Loss: 0.4169\n",
            "142/300, Train_loss: 0.4169 Train_dice: 0.4169\n",
            "Epoch [6/100], Batch [143/300],  Cross Loss: 0.2971\n",
            "143/300, Train_loss: 0.2971 Train_dice: 0.2971\n",
            "Epoch [6/100], Batch [144/300],  Cross Loss: 0.3301\n",
            "144/300, Train_loss: 0.3301 Train_dice: 0.3301\n",
            "Epoch [6/100], Batch [145/300],  Cross Loss: 0.2059\n",
            "145/300, Train_loss: 0.2059 Train_dice: 0.2059\n",
            "Epoch [6/100], Batch [146/300],  Cross Loss: 0.4086\n",
            "146/300, Train_loss: 0.4086 Train_dice: 0.4086\n",
            "Epoch [6/100], Batch [147/300],  Cross Loss: 0.4196\n",
            "147/300, Train_loss: 0.4196 Train_dice: 0.4196\n",
            "Epoch [6/100], Batch [148/300],  Cross Loss: 0.3043\n",
            "148/300, Train_loss: 0.3043 Train_dice: 0.3043\n",
            "Epoch [6/100], Batch [149/300],  Cross Loss: 0.4078\n",
            "149/300, Train_loss: 0.4078 Train_dice: 0.4078\n",
            "Epoch [6/100], Batch [150/300],  Cross Loss: 0.3044\n",
            "150/300, Train_loss: 0.3044 Train_dice: 0.3044\n",
            "Epoch [6/100], Batch [151/300],  Cross Loss: 0.3286\n",
            "151/300, Train_loss: 0.3286 Train_dice: 0.3286\n",
            "Epoch [6/100], Batch [152/300],  Cross Loss: 0.3778\n",
            "152/300, Train_loss: 0.3778 Train_dice: 0.3778\n",
            "Epoch [6/100], Batch [153/300],  Cross Loss: 0.5092\n",
            "153/300, Train_loss: 0.5092 Train_dice: 0.5092\n",
            "Epoch [6/100], Batch [154/300],  Cross Loss: 0.3648\n",
            "154/300, Train_loss: 0.3648 Train_dice: 0.3648\n",
            "Epoch [6/100], Batch [155/300],  Cross Loss: 0.2791\n",
            "155/300, Train_loss: 0.2791 Train_dice: 0.2791\n",
            "Epoch [6/100], Batch [156/300],  Cross Loss: 0.4589\n",
            "156/300, Train_loss: 0.4589 Train_dice: 0.4589\n",
            "Epoch [6/100], Batch [157/300],  Cross Loss: 0.4969\n",
            "157/300, Train_loss: 0.4969 Train_dice: 0.4969\n",
            "Epoch [6/100], Batch [158/300],  Cross Loss: 0.4743\n",
            "158/300, Train_loss: 0.4743 Train_dice: 0.4743\n",
            "Epoch [6/100], Batch [159/300],  Cross Loss: 0.3373\n",
            "159/300, Train_loss: 0.3373 Train_dice: 0.3373\n",
            "Epoch [6/100], Batch [160/300],  Cross Loss: 0.4520\n",
            "160/300, Train_loss: 0.4520 Train_dice: 0.4520\n",
            "Epoch [6/100], Batch [161/300],  Cross Loss: 0.3318\n",
            "161/300, Train_loss: 0.3318 Train_dice: 0.3318\n",
            "Epoch [6/100], Batch [162/300],  Cross Loss: 0.4000\n",
            "162/300, Train_loss: 0.4000 Train_dice: 0.4000\n",
            "Epoch [6/100], Batch [163/300],  Cross Loss: 0.3068\n",
            "163/300, Train_loss: 0.3068 Train_dice: 0.3068\n",
            "Epoch [6/100], Batch [164/300],  Cross Loss: 0.3999\n",
            "164/300, Train_loss: 0.3999 Train_dice: 0.3999\n",
            "Epoch [6/100], Batch [165/300],  Cross Loss: 0.4072\n",
            "165/300, Train_loss: 0.4072 Train_dice: 0.4072\n",
            "Epoch [6/100], Batch [166/300],  Cross Loss: 0.4260\n",
            "166/300, Train_loss: 0.4260 Train_dice: 0.4260\n",
            "Epoch [6/100], Batch [167/300],  Cross Loss: 0.4366\n",
            "167/300, Train_loss: 0.4366 Train_dice: 0.4366\n",
            "Epoch [6/100], Batch [168/300],  Cross Loss: 0.4956\n",
            "168/300, Train_loss: 0.4956 Train_dice: 0.4956\n",
            "Epoch [6/100], Batch [169/300],  Cross Loss: 0.3529\n",
            "169/300, Train_loss: 0.3529 Train_dice: 0.3529\n",
            "Epoch [6/100], Batch [170/300],  Cross Loss: 0.5251\n",
            "170/300, Train_loss: 0.5251 Train_dice: 0.5251\n",
            "Epoch [6/100], Batch [171/300],  Cross Loss: 0.4352\n",
            "171/300, Train_loss: 0.4352 Train_dice: 0.4352\n",
            "Epoch [6/100], Batch [172/300],  Cross Loss: 0.5040\n",
            "172/300, Train_loss: 0.5040 Train_dice: 0.5040\n",
            "Epoch [6/100], Batch [173/300],  Cross Loss: 0.3746\n",
            "173/300, Train_loss: 0.3746 Train_dice: 0.3746\n",
            "Epoch [6/100], Batch [174/300],  Cross Loss: 0.3274\n",
            "174/300, Train_loss: 0.3274 Train_dice: 0.3274\n",
            "Epoch [6/100], Batch [175/300],  Cross Loss: 0.4917\n",
            "175/300, Train_loss: 0.4917 Train_dice: 0.4917\n",
            "Epoch [6/100], Batch [176/300],  Cross Loss: 0.1192\n",
            "176/300, Train_loss: 0.1192 Train_dice: 0.1192\n",
            "Epoch [6/100], Batch [177/300],  Cross Loss: 0.4699\n",
            "177/300, Train_loss: 0.4699 Train_dice: 0.4699\n",
            "Epoch [6/100], Batch [178/300],  Cross Loss: 0.3869\n",
            "178/300, Train_loss: 0.3869 Train_dice: 0.3869\n",
            "Epoch [6/100], Batch [179/300],  Cross Loss: 0.1654\n",
            "179/300, Train_loss: 0.1654 Train_dice: 0.1654\n",
            "Epoch [6/100], Batch [180/300],  Cross Loss: 0.2968\n",
            "180/300, Train_loss: 0.2968 Train_dice: 0.2968\n"
          ]
        }
      ],
      "source": [
        "train(model, loss, optimizer,  model_dir, num_epochs = 100, start_from=1, test_interval = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOE9Lac4k6cy"
      },
      "source": [
        "# ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zcL1O0rZbm5"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1), bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=(3, 3, 3), stride=1, padding=(1, 1, 1), bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut.add_module('shortcut_conv', nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False))\n",
        "            self.shortcut.add_module('shortcut_bn', nn.BatchNorm3d(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = self.shortcut(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x += residual\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4muGWiZXnbn",
        "outputId": "3b8d62c2-7b2e-4e47-c825-4d6810d5838e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14645894\n",
            "residual\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "residual\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "residual\n",
            "torch.Size([1, 64, 32, 32, 4])\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "residual\n",
            "torch.Size([1, 128, 32, 32, 4])\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "residual\n",
            "torch.Size([1, 256, 32, 32, 4])\n",
            "torch.Size([1, 512, 32, 32, 4])\n",
            "torch.Size([1, 512, 32, 32, 4])\n",
            "torch.Size([1, 512, 32, 32, 4])\n",
            "torch.Size([1, 512, 32, 32, 4])\n",
            "torch.Size([1, 6])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 1)#3\n",
        "        self.layer2 = self._make_layer(64, 128, 1)# 4\n",
        "        self.layer3 = self._make_layer(128, 256,1)# 6\n",
        "        self.layer4 = self._make_layer(256, 512,1)# 3\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(512, 6)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks):\n",
        "        layers = []\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(ResidualBlock(in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "   \n",
        "        x = self.layer4(x)\n",
        "\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "x =ResNet()\n",
        "print(sum(p.numel() for p in x.parameters()))\n",
        "print(x(torch.randn(1,4,128,128,16)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfIiqA-9ThwQ"
      },
      "outputs": [],
      "source": [
        "model_dir = OUT_DIR + \"resnet/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkmHI2IeTggl"
      },
      "outputs": [],
      "source": [
        "model =  MNetWithBBClassifier().to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Update with the appropriate learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8-LP3vGTsbv"
      },
      "outputs": [],
      "source": [
        "from monai.losses import FocalLoss\n",
        "\n",
        "# Define the Focal Loss function\n",
        "#loss = FocalLoss()\n",
        "#loss = nn.BCELoss()\n",
        "loss = nn.SmoothL1Loss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiujDrLSTt2b"
      },
      "outputs": [],
      "source": [
        "train(model, loss, optimizer,  model_dir, num_epochs = 100, start_from=1, test_interval = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txse4NidVs-S"
      },
      "source": [
        "# Trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1K40oUSk-9Y"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define a basic convolutional block\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.stride = stride\n",
        "\n",
        "        # If the input and output dimensions are different, use a 1x1 convolutional layer\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet architecture\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=6):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv3d(4, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(num_blocks - 1):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the ResNet architecture\n",
        "# Instantiate the ResNet architecture\n",
        "def ResNet18(num_classes=6):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "\n",
        "  import torch\n",
        "\n",
        "  x =ResNet18(num_classes=6)\n",
        "  print(sum(p.numel() for p in x.parameters()))\n",
        "  print(x(torch.randn(1,4,128,128,16)).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NygYfhuEVriD"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# # Define U-Net model architecture with bounding box classifier\n",
        "# class UNetWithBBClassifier(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, num_classes):\n",
        "#         super(UNetWithBBClassifier, self).__init__()\n",
        "#         self.encoder = nn.Sequential(\n",
        "#             nn.Conv3d(in_channels, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "#         self.decoder = nn.Sequential(\n",
        "#             nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv3d(64, 1, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             #nn.ConvTranspose3d(64, out_channels, kernel_size=2, stride=2)\n",
        "#         )\n",
        "#         # Calculate the input size for the bounding box classifier\n",
        "#         encoder_output_size = 64 * 64 * 64 * 8 #64 * (input_height // (2 ** num_downsamples)) * (input_width // (2 ** num_downsamples)) * (input_depth // (2 ** num_downsamples))\n",
        "#         self.bb_classifier = nn.Sequential(\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(encoder_output_size, 256),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Linear(256, 128),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Linear(128, num_classes),\n",
        "#             nn.Softmax(dim=1)\n",
        "#         )\n",
        "#         self.segmentation_mask = nn.Conv3d(out_channels, num_classes, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x1 = self.encoder(x)\n",
        "#         print(x1.shape)\n",
        "#         x2 = self.decoder(x1)\n",
        "#         bb_output = self.bb_classifier(x1.view(x1.size(0), -1))\n",
        "#         mask_output = self.segmentation_mask(x2)\n",
        "#         return mask_output, bb_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZZBjRPcp1Au"
      },
      "outputs": [],
      "source": [
        "# Define Gleason score prediction loss function\n",
        "class GleasonScoreLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GleasonScoreLoss, self).__init__()\n",
        "        self.criterion = nn.CrossEntropyLoss()  # You can choose an appropriate loss function for Gleason score prediction\n",
        "\n",
        "    def forward(self, predicted_scores, target_scores):\n",
        "        loss = self.criterion(predicted_scores, target_scores)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ9Scqg4Yb9K"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class UNetWithBBClassifierLoss(nn.Module):\n",
        "#     def __init__(self, weight_seg=1.0, weight_bb=1.0):\n",
        "#         super(UNetWithBBClassifierLoss, self).__init__()\n",
        "#         self.weight_seg = weight_seg\n",
        "#         self.weight_bb = weight_bb\n",
        "\n",
        "#     def forward(self, output, target_seg, target_bb):\n",
        "#         # Unpack the output tuple\n",
        "#         seg_output, bb_output = output\n",
        "\n",
        "#         # Compute the segmentation loss using cross-entropy\n",
        "#         # seg_criterion = nn.CrossEntropyLoss()\n",
        "#         # seg_loss = seg_criterion(seg_output, target_seg)\n",
        "#         seg_criterion = nn.CrossEntropyLoss()\n",
        "#         seg_loss = seg_criterion(seg_output, target_seg)\n",
        "\n",
        "#         # Compute the bounding box classification loss using cross-entropy\n",
        "#         bb_criterion = nn.CrossEntropyLoss()\n",
        "#         bb_loss = bb_criterion(bb_output, target_bb)\n",
        "\n",
        "#         # Combine the segmentation and bounding box classification losses\n",
        "#         total_loss = self.weight_seg * seg_loss + self.weight_bb * bb_loss\n",
        "\n",
        "#         return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKX-N7w2Q_MO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNetWithBBClassifierLoss(nn.Module):\n",
        "    def __init__(self, weight_seg=0, weight_bb=1.0):\n",
        "        super(UNetWithBBClassifierLoss, self).__init__()\n",
        "        self.weight_seg = weight_seg\n",
        "        self.weight_bb = weight_bb\n",
        "\n",
        "    def forward(self, seg_output, target_seg, bb_output,  target_bb):\n",
        "\n",
        "        # Compute the segmentation loss using cross-entropy\n",
        "        seg_criterion =  DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
        "        seg_loss = seg_criterion(seg_output, target_seg)\n",
        "        print(\"seg_loss :\", seg_loss)\n",
        "        # Compute the bounding box classification loss using cross-entropy\n",
        "        bb_criterion = nn.LogLoss() #CrossEntropyLoss()\n",
        "        bb_loss = bb_criterion(bb_output, target_bb)\n",
        "        print(\"bb_loss :\",bb_loss)\n",
        "        #bb_loss = F.smooth_l1_loss(bb_output,  target_bb, reduction='mean', beta=1.0)\n",
        "        # Combine the segmentation and bounding box classification losses\n",
        "        total_loss = self.weight_seg * seg_loss + self.weight_bb * bb_loss\n",
        "\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4RUsYAEJPuE",
        "outputId": "cfa6a3b5-3cd1-413d-a0d5-b5e7105dd185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Perform one-hot encoding on the string column\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "one_hot = encoder.fit_transform(df[['Gleason Grade Group']])  # Extract the column as a DataFrame and perform one-hot encoding\n",
        "one_hot_tensor = torch.tensor(one_hot) \n",
        "\n",
        "# Print the resulting tensor\n",
        "print(len(one_hot_tensor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bujWet3gtfNb"
      },
      "outputs": [],
      "source": [
        "#loss_function = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XW7BLhTUXFB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajWTB28Ou26A"
      },
      "outputs": [],
      "source": [
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation during testing\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):  # Update with your test data and data loader\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        mask_output, bb_output = model(data)\n",
        "        \n",
        "        # Compute loss for segmentation mask (optional)\n",
        "        # mask_loss = loss_function(mask_output, target)\n",
        "        \n",
        "        # Compute loss for bounding box classifier\n",
        "        bb_target = target.view(target.size(0), -1)\n",
        "        bb_loss = criterion(bb_output, bb_target)\n",
        "        \n",
        "        # Update test loss\n",
        "        test_loss += bb_loss.item()  # Add the batch loss to the total test loss\n",
        "        \n",
        "        # Perform any other evaluation or metrics calculation here\n",
        "        \n",
        "# Average test loss over all batches\n",
        "test_loss /= len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKuIT2WdHomI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class YourModel(nn.Module):\n",
        "    def __init__(self, n_input_channels=256, n_features=64, n_output_channels=6, anchor_stride=2, dim=3):\n",
        "        super(YourModel, self).__init__()\n",
        "        self.n_classes =6\n",
        "        self.dim = dim\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv3d(n_input_channels, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_features, kernel_size=3, stride=anchor_stride, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(n_features, n_output_channels, kernel_size=3, stride=anchor_stride, padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        \n",
        "        # Rearrange dimensions based on self.dim\n",
        "        if self.dim == 2:\n",
        "            x = x.permute(0, 2, 3, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "        else:\n",
        "            x = x.permute(0, 2, 3, 4, 1).contiguous()\n",
        "            x = x.view(x.size()[0], -1, self.n_classes)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHxUviG0F9WH"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  x = YourModel()\n",
        "  print(sum(p.numel() for p in x.parameters()))\n",
        "  print(x(torch.randn(1,256,32,32,16)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQSPYGh7oGdX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from monai.losses import DiceLoss, DiceCELoss\n",
        "\n",
        "# Define the two loss functions\n",
        "dsc_loss = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
        "cross_entropy_loss = FocalLoss() #torch.nn.BCELoss()\n",
        "\n",
        "# Define the two optimizers\n",
        "optimizer_out = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer_bb = torch.optim.Adam(model.bb_classifier.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgFSNpTEaTcd"
      },
      "outputs": [],
      "source": [
        "from monai.losses import FocalLoss\n",
        "\n",
        "# Define the Focal Loss function\n",
        "focal_loss = FocalLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iNHCrT7pFB7",
        "outputId": "02c608e3-dbd0-4cc8-99dc-5ce4cc855f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/monai/losses/dice.py:144: UserWarning: single channel prediction, `to_onehot_y=True` ignored.\n",
            "  warnings.warn(\"single channel prediction, `to_onehot_y=True` ignored.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Batch [1/239], Dice Loss: 0.9993, Cross Loss: 0.2378\n",
            "1/239, Train_loss: 0.23780295252799988 Train_dice: tensor(0.2378, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [2/239], Dice Loss: 0.9980, Cross Loss: 0.2239\n",
            "2/239, Train_loss: 0.22385910153388977 Train_dice: tensor(0.2239, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [3/239], Dice Loss: 0.9981, Cross Loss: 0.2227\n",
            "3/239, Train_loss: 0.22269460558891296 Train_dice: tensor(0.2227, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [4/239], Dice Loss: 0.9974, Cross Loss: 0.2138\n",
            "4/239, Train_loss: 0.21382629871368408 Train_dice: tensor(0.2138, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [5/239], Dice Loss: 0.9907, Cross Loss: 0.1866\n",
            "5/239, Train_loss: 0.18659573793411255 Train_dice: tensor(0.1866, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [6/239], Dice Loss: 0.9920, Cross Loss: 0.1877\n",
            "6/239, Train_loss: 0.18771815299987793 Train_dice: tensor(0.1877, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [7/239], Dice Loss: 0.9670, Cross Loss: 0.2073\n",
            "7/239, Train_loss: 0.20729057490825653 Train_dice: tensor(0.2073, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [8/239], Dice Loss: 0.9984, Cross Loss: 0.2164\n",
            "8/239, Train_loss: 0.2164442390203476 Train_dice: tensor(0.2164, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [9/239], Dice Loss: 0.9992, Cross Loss: 0.2258\n",
            "9/239, Train_loss: 0.2258002609014511 Train_dice: tensor(0.2258, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [10/239], Dice Loss: 0.9872, Cross Loss: 0.1959\n",
            "10/239, Train_loss: 0.19589529931545258 Train_dice: tensor(0.1959, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [11/239], Dice Loss: 0.9878, Cross Loss: 0.1965\n",
            "11/239, Train_loss: 0.19649827480316162 Train_dice: tensor(0.1965, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [12/239], Dice Loss: 0.9938, Cross Loss: 0.1931\n",
            "12/239, Train_loss: 0.19314977526664734 Train_dice: tensor(0.1931, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [13/239], Dice Loss: 0.9953, Cross Loss: 0.1907\n",
            "13/239, Train_loss: 0.1906648725271225 Train_dice: tensor(0.1907, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [14/239], Dice Loss: 0.9871, Cross Loss: 0.1913\n",
            "14/239, Train_loss: 0.1912812888622284 Train_dice: tensor(0.1913, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [15/239], Dice Loss: 0.9891, Cross Loss: 0.1885\n",
            "15/239, Train_loss: 0.1885383576154709 Train_dice: tensor(0.1885, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [16/239], Dice Loss: 0.9941, Cross Loss: 0.1841\n",
            "16/239, Train_loss: 0.18408799171447754 Train_dice: tensor(0.1841, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [17/239], Dice Loss: 0.9845, Cross Loss: 0.1796\n",
            "17/239, Train_loss: 0.17961280047893524 Train_dice: tensor(0.1796, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [18/239], Dice Loss: 0.9976, Cross Loss: 0.1759\n",
            "18/239, Train_loss: 0.17589136958122253 Train_dice: tensor(0.1759, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [19/239], Dice Loss: 0.9857, Cross Loss: 0.2276\n",
            "19/239, Train_loss: 0.227583646774292 Train_dice: tensor(0.2276, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [20/239], Dice Loss: 0.9372, Cross Loss: 0.2351\n",
            "20/239, Train_loss: 0.23507222533226013 Train_dice: tensor(0.2351, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [21/239], Dice Loss: 0.9965, Cross Loss: 0.2269\n",
            "21/239, Train_loss: 0.22694794833660126 Train_dice: tensor(0.2269, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [22/239], Dice Loss: 0.9955, Cross Loss: 0.1689\n",
            "22/239, Train_loss: 0.16890716552734375 Train_dice: tensor(0.1689, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [23/239], Dice Loss: 0.9733, Cross Loss: 0.2311\n",
            "23/239, Train_loss: 0.2311311960220337 Train_dice: tensor(0.2311, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [24/239], Dice Loss: 0.9502, Cross Loss: 0.2322\n",
            "24/239, Train_loss: 0.23215100169181824 Train_dice: tensor(0.2322, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [25/239], Dice Loss: 0.9958, Cross Loss: 0.1728\n",
            "25/239, Train_loss: 0.17275582253932953 Train_dice: tensor(0.1728, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [26/239], Dice Loss: 0.9941, Cross Loss: 0.1695\n",
            "26/239, Train_loss: 0.16951638460159302 Train_dice: tensor(0.1695, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [27/239], Dice Loss: 0.9971, Cross Loss: 0.1655\n",
            "27/239, Train_loss: 0.16552649438381195 Train_dice: tensor(0.1655, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [28/239], Dice Loss: 0.9675, Cross Loss: 0.1624\n",
            "28/239, Train_loss: 0.16244889795780182 Train_dice: tensor(0.1624, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [29/239], Dice Loss: 0.9921, Cross Loss: 0.1641\n",
            "29/239, Train_loss: 0.164062961935997 Train_dice: tensor(0.1641, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [30/239], Dice Loss: 0.9988, Cross Loss: 0.1596\n",
            "30/239, Train_loss: 0.15962176024913788 Train_dice: tensor(0.1596, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [31/239], Dice Loss: 0.9856, Cross Loss: 0.1589\n",
            "31/239, Train_loss: 0.1588946282863617 Train_dice: tensor(0.1589, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [32/239], Dice Loss: 0.9941, Cross Loss: 0.1556\n",
            "32/239, Train_loss: 0.15564881265163422 Train_dice: tensor(0.1556, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [33/239], Dice Loss: 0.9351, Cross Loss: 0.2537\n",
            "33/239, Train_loss: 0.25373375415802 Train_dice: tensor(0.2537, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [34/239], Dice Loss: 0.9966, Cross Loss: 0.1530\n",
            "34/239, Train_loss: 0.15298426151275635 Train_dice: tensor(0.1530, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [35/239], Dice Loss: 0.9897, Cross Loss: 0.2531\n",
            "35/239, Train_loss: 0.25306278467178345 Train_dice: tensor(0.2531, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [36/239], Dice Loss: 0.9930, Cross Loss: 0.1530\n",
            "36/239, Train_loss: 0.1530241221189499 Train_dice: tensor(0.1530, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [37/239], Dice Loss: 0.9927, Cross Loss: 0.1518\n",
            "37/239, Train_loss: 0.15179190039634705 Train_dice: tensor(0.1518, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [38/239], Dice Loss: 0.9861, Cross Loss: 0.1519\n",
            "38/239, Train_loss: 0.151941180229187 Train_dice: tensor(0.1519, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [39/239], Dice Loss: 0.9650, Cross Loss: 0.2565\n",
            "39/239, Train_loss: 0.25651150941848755 Train_dice: tensor(0.2565, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [40/239], Dice Loss: 0.9665, Cross Loss: 0.2561\n",
            "40/239, Train_loss: 0.256112277507782 Train_dice: tensor(0.2561, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [41/239], Dice Loss: 0.9911, Cross Loss: 0.1506\n",
            "41/239, Train_loss: 0.1506267786026001 Train_dice: tensor(0.1506, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [42/239], Dice Loss: 0.9354, Cross Loss: 0.1507\n",
            "42/239, Train_loss: 0.15074631571769714 Train_dice: tensor(0.1507, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [43/239], Dice Loss: 0.9927, Cross Loss: 0.1505\n",
            "43/239, Train_loss: 0.15054172277450562 Train_dice: tensor(0.1505, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [44/239], Dice Loss: 0.9532, Cross Loss: 0.1500\n",
            "44/239, Train_loss: 0.1499655842781067 Train_dice: tensor(0.1500, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [45/239], Dice Loss: 0.9626, Cross Loss: 0.2578\n",
            "45/239, Train_loss: 0.2577595114707947 Train_dice: tensor(0.2578, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [46/239], Dice Loss: 0.9972, Cross Loss: 0.1500\n",
            "46/239, Train_loss: 0.15001636743545532 Train_dice: tensor(0.1500, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [47/239], Dice Loss: 0.9834, Cross Loss: 0.1496\n",
            "47/239, Train_loss: 0.14961740374565125 Train_dice: tensor(0.1496, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [48/239], Dice Loss: 0.9771, Cross Loss: 0.2584\n",
            "48/239, Train_loss: 0.2584323287010193 Train_dice: tensor(0.2584, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [49/239], Dice Loss: 0.9906, Cross Loss: 0.1497\n",
            "49/239, Train_loss: 0.1496673822402954 Train_dice: tensor(0.1497, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [50/239], Dice Loss: 0.9647, Cross Loss: 0.1494\n",
            "50/239, Train_loss: 0.1494373381137848 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [51/239], Dice Loss: 0.9495, Cross Loss: 0.1493\n",
            "51/239, Train_loss: 0.14934825897216797 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [52/239], Dice Loss: 0.9806, Cross Loss: 0.1493\n",
            "52/239, Train_loss: 0.14925456047058105 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [53/239], Dice Loss: 0.9880, Cross Loss: 0.1493\n",
            "53/239, Train_loss: 0.14927318692207336 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [54/239], Dice Loss: 0.9846, Cross Loss: 0.2593\n",
            "54/239, Train_loss: 0.2593235373497009 Train_dice: tensor(0.2593, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [55/239], Dice Loss: 0.9634, Cross Loss: 0.1492\n",
            "55/239, Train_loss: 0.1492227017879486 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [56/239], Dice Loss: 0.9971, Cross Loss: 0.1492\n",
            "56/239, Train_loss: 0.1491832733154297 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [57/239], Dice Loss: 0.9819, Cross Loss: 0.2596\n",
            "57/239, Train_loss: 0.2596209645271301 Train_dice: tensor(0.2596, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [58/239], Dice Loss: 0.9873, Cross Loss: 0.1489\n",
            "58/239, Train_loss: 0.1489114910364151 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [59/239], Dice Loss: 0.9760, Cross Loss: 0.1490\n",
            "59/239, Train_loss: 0.14902310073375702 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [60/239], Dice Loss: 0.9458, Cross Loss: 0.1488\n",
            "60/239, Train_loss: 0.14884643256664276 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [61/239], Dice Loss: 0.9879, Cross Loss: 0.1488\n",
            "61/239, Train_loss: 0.1488458216190338 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [62/239], Dice Loss: 0.9728, Cross Loss: 0.2601\n",
            "62/239, Train_loss: 0.2600753903388977 Train_dice: tensor(0.2601, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [63/239], Dice Loss: 0.9857, Cross Loss: 0.1490\n",
            "63/239, Train_loss: 0.14901551604270935 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [64/239], Dice Loss: 0.9933, Cross Loss: 0.1488\n",
            "64/239, Train_loss: 0.14882925152778625 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [65/239], Dice Loss: 0.9630, Cross Loss: 0.1487\n",
            "65/239, Train_loss: 0.14874543249607086 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [66/239], Dice Loss: 0.9660, Cross Loss: 0.1488\n",
            "66/239, Train_loss: 0.14882437884807587 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [67/239], Dice Loss: 0.8947, Cross Loss: 0.1487\n",
            "67/239, Train_loss: 0.14873595535755157 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [68/239], Dice Loss: 0.9797, Cross Loss: 0.1487\n",
            "68/239, Train_loss: 0.14872437715530396 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [69/239], Dice Loss: 0.9969, Cross Loss: 0.2605\n",
            "69/239, Train_loss: 0.26046064496040344 Train_dice: tensor(0.2605, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [70/239], Dice Loss: 0.9494, Cross Loss: 0.2602\n",
            "70/239, Train_loss: 0.26024115085601807 Train_dice: tensor(0.2602, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [71/239], Dice Loss: 0.9351, Cross Loss: 0.1487\n",
            "71/239, Train_loss: 0.14873486757278442 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [72/239], Dice Loss: 0.8151, Cross Loss: 0.2605\n",
            "72/239, Train_loss: 0.2605196237564087 Train_dice: tensor(0.2605, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [73/239], Dice Loss: 0.9657, Cross Loss: 0.1487\n",
            "73/239, Train_loss: 0.14869728684425354 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [74/239], Dice Loss: 0.9798, Cross Loss: 0.1487\n",
            "74/239, Train_loss: 0.14869865775108337 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [75/239], Dice Loss: 0.9916, Cross Loss: 0.1488\n",
            "75/239, Train_loss: 0.14875178039073944 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [76/239], Dice Loss: 0.9912, Cross Loss: 0.1487\n",
            "76/239, Train_loss: 0.14868682622909546 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [77/239], Dice Loss: 0.9920, Cross Loss: 0.1487\n",
            "77/239, Train_loss: 0.14868782460689545 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [78/239], Dice Loss: 0.9602, Cross Loss: 0.1487\n",
            "78/239, Train_loss: 0.14868679642677307 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [79/239], Dice Loss: 0.9889, Cross Loss: 0.1487\n",
            "79/239, Train_loss: 0.14865827560424805 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [80/239], Dice Loss: 0.9857, Cross Loss: 0.1486\n",
            "80/239, Train_loss: 0.14855453372001648 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [81/239], Dice Loss: 0.9941, Cross Loss: 0.1486\n",
            "81/239, Train_loss: 0.14857077598571777 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [82/239], Dice Loss: 0.9481, Cross Loss: 0.1485\n",
            "82/239, Train_loss: 0.14852765202522278 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [83/239], Dice Loss: 0.9869, Cross Loss: 0.1485\n",
            "83/239, Train_loss: 0.14850929379463196 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [84/239], Dice Loss: 0.9801, Cross Loss: 0.1485\n",
            "84/239, Train_loss: 0.14852464199066162 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [85/239], Dice Loss: 0.9651, Cross Loss: 0.2607\n",
            "85/239, Train_loss: 0.26071643829345703 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [86/239], Dice Loss: 0.9685, Cross Loss: 0.1485\n",
            "86/239, Train_loss: 0.14851421117782593 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [87/239], Dice Loss: 0.9926, Cross Loss: 0.1485\n",
            "87/239, Train_loss: 0.14852474629878998 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [88/239], Dice Loss: 0.9976, Cross Loss: 0.1485\n",
            "88/239, Train_loss: 0.14846499264240265 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [89/239], Dice Loss: 0.9982, Cross Loss: 0.1485\n",
            "89/239, Train_loss: 0.14845576882362366 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [90/239], Dice Loss: 0.9704, Cross Loss: 0.1484\n",
            "90/239, Train_loss: 0.14844508469104767 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [91/239], Dice Loss: 0.9648, Cross Loss: 0.2606\n",
            "91/239, Train_loss: 0.2605997323989868 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [92/239], Dice Loss: 0.9762, Cross Loss: 0.1484\n",
            "92/239, Train_loss: 0.14843982458114624 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [93/239], Dice Loss: 0.9464, Cross Loss: 0.1484\n",
            "93/239, Train_loss: 0.14843595027923584 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [94/239], Dice Loss: 0.9671, Cross Loss: 0.2609\n",
            "94/239, Train_loss: 0.26088947057724 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [95/239], Dice Loss: 0.9677, Cross Loss: 0.1486\n",
            "95/239, Train_loss: 0.1486291140317917 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [96/239], Dice Loss: 0.9997, Cross Loss: 0.1486\n",
            "96/239, Train_loss: 0.14857451617717743 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [97/239], Dice Loss: 0.9998, Cross Loss: 0.1486\n",
            "97/239, Train_loss: 0.14863526821136475 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [98/239], Dice Loss: 0.9969, Cross Loss: 0.1484\n",
            "98/239, Train_loss: 0.14844590425491333 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [99/239], Dice Loss: 0.9999, Cross Loss: 0.2606\n",
            "99/239, Train_loss: 0.26058077812194824 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [100/239], Dice Loss: 0.9993, Cross Loss: 0.2610\n",
            "100/239, Train_loss: 0.2610166072845459 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [101/239], Dice Loss: 0.9950, Cross Loss: 0.2602\n",
            "101/239, Train_loss: 0.26024866104125977 Train_dice: tensor(0.2602, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [102/239], Dice Loss: 0.9546, Cross Loss: 0.1485\n",
            "102/239, Train_loss: 0.14848831295967102 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [103/239], Dice Loss: 0.9993, Cross Loss: 0.1494\n",
            "103/239, Train_loss: 0.14939603209495544 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [104/239], Dice Loss: 0.9121, Cross Loss: 0.2592\n",
            "104/239, Train_loss: 0.2591772675514221 Train_dice: tensor(0.2592, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [105/239], Dice Loss: 0.9737, Cross Loss: 0.1502\n",
            "105/239, Train_loss: 0.15021055936813354 Train_dice: tensor(0.1502, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [106/239], Dice Loss: 0.9819, Cross Loss: 0.1501\n",
            "106/239, Train_loss: 0.15007156133651733 Train_dice: tensor(0.1501, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [107/239], Dice Loss: 0.9454, Cross Loss: 0.1493\n",
            "107/239, Train_loss: 0.14927195012569427 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [108/239], Dice Loss: 0.9735, Cross Loss: 0.2603\n",
            "108/239, Train_loss: 0.2602556347846985 Train_dice: tensor(0.2603, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [109/239], Dice Loss: 0.9898, Cross Loss: 0.1491\n",
            "109/239, Train_loss: 0.1491299271583557 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [110/239], Dice Loss: 0.9748, Cross Loss: 0.1488\n",
            "110/239, Train_loss: 0.14882037043571472 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [111/239], Dice Loss: 0.9836, Cross Loss: 0.1486\n",
            "111/239, Train_loss: 0.1485910266637802 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [112/239], Dice Loss: 0.9526, Cross Loss: 0.1485\n",
            "112/239, Train_loss: 0.1484798789024353 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [113/239], Dice Loss: 0.9657, Cross Loss: 0.1484\n",
            "113/239, Train_loss: 0.14844417572021484 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [114/239], Dice Loss: 0.9451, Cross Loss: 0.1484\n",
            "114/239, Train_loss: 0.14843936264514923 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [115/239], Dice Loss: 0.9904, Cross Loss: 0.1484\n",
            "115/239, Train_loss: 0.14842236042022705 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [116/239], Dice Loss: 0.9848, Cross Loss: 0.1484\n",
            "116/239, Train_loss: 0.14841559529304504 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [117/239], Dice Loss: 0.9815, Cross Loss: 0.1484\n",
            "117/239, Train_loss: 0.14840279519557953 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [118/239], Dice Loss: 0.9910, Cross Loss: 0.1484\n",
            "118/239, Train_loss: 0.14839577674865723 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [119/239], Dice Loss: 0.9856, Cross Loss: 0.1484\n",
            "119/239, Train_loss: 0.1483839601278305 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [120/239], Dice Loss: 0.9824, Cross Loss: 0.1484\n",
            "120/239, Train_loss: 0.1483738273382187 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [121/239], Dice Loss: 0.9565, Cross Loss: 0.1484\n",
            "121/239, Train_loss: 0.148362934589386 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [122/239], Dice Loss: 0.9051, Cross Loss: 0.2611\n",
            "122/239, Train_loss: 0.2611023187637329 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [123/239], Dice Loss: 0.9938, Cross Loss: 0.1484\n",
            "123/239, Train_loss: 0.14835336804389954 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [124/239], Dice Loss: 0.8210, Cross Loss: 0.2611\n",
            "124/239, Train_loss: 0.2610979974269867 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [125/239], Dice Loss: 0.9943, Cross Loss: 0.2611\n",
            "125/239, Train_loss: 0.26113080978393555 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [126/239], Dice Loss: 0.9807, Cross Loss: 0.1483\n",
            "126/239, Train_loss: 0.14833995699882507 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [127/239], Dice Loss: 0.9843, Cross Loss: 0.1483\n",
            "127/239, Train_loss: 0.1483326405286789 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [128/239], Dice Loss: 0.9499, Cross Loss: 0.2611\n",
            "128/239, Train_loss: 0.2610543668270111 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [129/239], Dice Loss: 0.9800, Cross Loss: 0.1483\n",
            "129/239, Train_loss: 0.1483387053012848 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [130/239], Dice Loss: 0.9698, Cross Loss: 0.2610\n",
            "130/239, Train_loss: 0.2610486149787903 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [131/239], Dice Loss: 0.9825, Cross Loss: 0.1483\n",
            "131/239, Train_loss: 0.14834824204444885 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [132/239], Dice Loss: 0.9984, Cross Loss: 0.1483\n",
            "132/239, Train_loss: 0.14834102988243103 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [133/239], Dice Loss: 0.9678, Cross Loss: 0.1483\n",
            "133/239, Train_loss: 0.14833858609199524 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [134/239], Dice Loss: 0.9281, Cross Loss: 0.2611\n",
            "134/239, Train_loss: 0.2610931694507599 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [135/239], Dice Loss: 0.9425, Cross Loss: 0.1483\n",
            "135/239, Train_loss: 0.1483244150876999 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [136/239], Dice Loss: 0.9970, Cross Loss: 0.2611\n",
            "136/239, Train_loss: 0.2610793113708496 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [137/239], Dice Loss: 0.8154, Cross Loss: 0.2612\n",
            "137/239, Train_loss: 0.26117387413978577 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [138/239], Dice Loss: 0.9659, Cross Loss: 0.1483\n",
            "138/239, Train_loss: 0.1483454555273056 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [139/239], Dice Loss: 0.9975, Cross Loss: 0.2610\n",
            "139/239, Train_loss: 0.26103633642196655 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [140/239], Dice Loss: 0.9700, Cross Loss: 0.1484\n",
            "140/239, Train_loss: 0.1483641117811203 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [141/239], Dice Loss: 0.9992, Cross Loss: 0.2610\n",
            "141/239, Train_loss: 0.2610013484954834 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [142/239], Dice Loss: 0.9427, Cross Loss: 0.1484\n",
            "142/239, Train_loss: 0.14835266768932343 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [143/239], Dice Loss: 0.9999, Cross Loss: 0.2611\n",
            "143/239, Train_loss: 0.2611064016819 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [144/239], Dice Loss: 0.9204, Cross Loss: 0.1484\n",
            "144/239, Train_loss: 0.1484033763408661 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [145/239], Dice Loss: 0.9047, Cross Loss: 0.1484\n",
            "145/239, Train_loss: 0.14835774898529053 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [146/239], Dice Loss: 0.9431, Cross Loss: 0.1484\n",
            "146/239, Train_loss: 0.14835867285728455 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [147/239], Dice Loss: 0.9570, Cross Loss: 0.1484\n",
            "147/239, Train_loss: 0.14841139316558838 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [148/239], Dice Loss: 0.9844, Cross Loss: 0.1484\n",
            "148/239, Train_loss: 0.148399218916893 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [149/239], Dice Loss: 0.7631, Cross Loss: 0.2610\n",
            "149/239, Train_loss: 0.2609867453575134 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [150/239], Dice Loss: 0.9972, Cross Loss: 0.1484\n",
            "150/239, Train_loss: 0.1483820676803589 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [151/239], Dice Loss: 0.8638, Cross Loss: 0.2610\n",
            "151/239, Train_loss: 0.260962575674057 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [152/239], Dice Loss: 0.9686, Cross Loss: 0.1484\n",
            "152/239, Train_loss: 0.14837782084941864 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [153/239], Dice Loss: 0.9933, Cross Loss: 0.1484\n",
            "153/239, Train_loss: 0.1483958214521408 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [154/239], Dice Loss: 0.8334, Cross Loss: 0.2609\n",
            "154/239, Train_loss: 0.26094383001327515 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [155/239], Dice Loss: 0.9241, Cross Loss: 0.1485\n",
            "155/239, Train_loss: 0.14848965406417847 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [156/239], Dice Loss: 0.8719, Cross Loss: 0.2610\n",
            "156/239, Train_loss: 0.2609604001045227 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [157/239], Dice Loss: 0.8919, Cross Loss: 0.2611\n",
            "157/239, Train_loss: 0.26105067133903503 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [158/239], Dice Loss: 0.9839, Cross Loss: 0.2610\n",
            "158/239, Train_loss: 0.26096493005752563 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [159/239], Dice Loss: 0.9911, Cross Loss: 0.1484\n",
            "159/239, Train_loss: 0.14839942753314972 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [160/239], Dice Loss: 0.8897, Cross Loss: 0.2609\n",
            "160/239, Train_loss: 0.2609081268310547 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [161/239], Dice Loss: 0.9610, Cross Loss: 0.1484\n",
            "161/239, Train_loss: 0.1484348177909851 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [162/239], Dice Loss: 0.9886, Cross Loss: 0.2608\n",
            "162/239, Train_loss: 0.2608255445957184 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [163/239], Dice Loss: 0.9948, Cross Loss: 0.1485\n",
            "163/239, Train_loss: 0.14853249490261078 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [164/239], Dice Loss: 0.9795, Cross Loss: 0.1485\n",
            "164/239, Train_loss: 0.1485276222229004 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [165/239], Dice Loss: 0.8681, Cross Loss: 0.2609\n",
            "165/239, Train_loss: 0.2609443962574005 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [166/239], Dice Loss: 0.9984, Cross Loss: 0.2609\n",
            "166/239, Train_loss: 0.26088958978652954 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [167/239], Dice Loss: 0.9445, Cross Loss: 0.2607\n",
            "167/239, Train_loss: 0.2607390284538269 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [168/239], Dice Loss: 0.8530, Cross Loss: 0.2607\n",
            "168/239, Train_loss: 0.2606709897518158 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [169/239], Dice Loss: 0.9812, Cross Loss: 0.2609\n",
            "169/239, Train_loss: 0.26087686419487 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [170/239], Dice Loss: 0.9960, Cross Loss: 0.2607\n",
            "170/239, Train_loss: 0.260739803314209 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [171/239], Dice Loss: 0.9343, Cross Loss: 0.2605\n",
            "171/239, Train_loss: 0.2604721188545227 Train_dice: tensor(0.2605, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [172/239], Dice Loss: 0.9440, Cross Loss: 0.1487\n",
            "172/239, Train_loss: 0.14865195751190186 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [173/239], Dice Loss: 0.8492, Cross Loss: 0.2607\n",
            "173/239, Train_loss: 0.26070913672447205 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [174/239], Dice Loss: 0.8874, Cross Loss: 0.2602\n",
            "174/239, Train_loss: 0.2601791024208069 Train_dice: tensor(0.2602, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [175/239], Dice Loss: 0.9607, Cross Loss: 0.2599\n",
            "175/239, Train_loss: 0.25994598865509033 Train_dice: tensor(0.2599, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [176/239], Dice Loss: 0.9535, Cross Loss: 0.1491\n",
            "176/239, Train_loss: 0.1490616500377655 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [177/239], Dice Loss: 0.8745, Cross Loss: 0.1492\n",
            "177/239, Train_loss: 0.14915934205055237 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [178/239], Dice Loss: 0.9243, Cross Loss: 0.2594\n",
            "178/239, Train_loss: 0.25943854451179504 Train_dice: tensor(0.2594, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [179/239], Dice Loss: 0.9803, Cross Loss: 0.1494\n",
            "179/239, Train_loss: 0.14938871562480927 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [180/239], Dice Loss: 0.8993, Cross Loss: 0.1494\n",
            "180/239, Train_loss: 0.14944937825202942 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [181/239], Dice Loss: 0.8566, Cross Loss: 0.2598\n",
            "181/239, Train_loss: 0.25979775190353394 Train_dice: tensor(0.2598, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [182/239], Dice Loss: 0.8617, Cross Loss: 0.1491\n",
            "182/239, Train_loss: 0.14914122223854065 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [183/239], Dice Loss: 0.9317, Cross Loss: 0.2594\n",
            "183/239, Train_loss: 0.25936996936798096 Train_dice: tensor(0.2594, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [184/239], Dice Loss: 0.9768, Cross Loss: 0.1495\n",
            "184/239, Train_loss: 0.14946232736110687 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [185/239], Dice Loss: 0.7906, Cross Loss: 0.1495\n",
            "185/239, Train_loss: 0.14949189126491547 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [186/239], Dice Loss: 0.8402, Cross Loss: 0.1496\n",
            "186/239, Train_loss: 0.14964431524276733 Train_dice: tensor(0.1496, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [187/239], Dice Loss: 0.9810, Cross Loss: 0.1495\n",
            "187/239, Train_loss: 0.14951631426811218 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [188/239], Dice Loss: 0.6701, Cross Loss: 0.2590\n",
            "188/239, Train_loss: 0.25902241468429565 Train_dice: tensor(0.2590, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [189/239], Dice Loss: 0.8819, Cross Loss: 0.1494\n",
            "189/239, Train_loss: 0.14938504993915558 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [190/239], Dice Loss: 0.9840, Cross Loss: 0.1492\n",
            "190/239, Train_loss: 0.14924991130828857 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [191/239], Dice Loss: 0.8706, Cross Loss: 0.2589\n",
            "191/239, Train_loss: 0.25894999504089355 Train_dice: tensor(0.2589, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [192/239], Dice Loss: 0.9452, Cross Loss: 0.1495\n",
            "192/239, Train_loss: 0.1494508683681488 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [193/239], Dice Loss: 0.9860, Cross Loss: 0.2589\n",
            "193/239, Train_loss: 0.25889700651168823 Train_dice: tensor(0.2589, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [194/239], Dice Loss: 0.9956, Cross Loss: 0.1491\n",
            "194/239, Train_loss: 0.14914876222610474 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [195/239], Dice Loss: 0.9869, Cross Loss: 0.1492\n",
            "195/239, Train_loss: 0.14916075766086578 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [196/239], Dice Loss: 0.9873, Cross Loss: 0.2591\n",
            "196/239, Train_loss: 0.25914931297302246 Train_dice: tensor(0.2591, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [197/239], Dice Loss: 0.9894, Cross Loss: 0.1494\n",
            "197/239, Train_loss: 0.14944833517074585 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [198/239], Dice Loss: 0.9879, Cross Loss: 0.1495\n",
            "198/239, Train_loss: 0.1494957059621811 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [199/239], Dice Loss: 0.7767, Cross Loss: 0.1491\n",
            "199/239, Train_loss: 0.14910036325454712 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [200/239], Dice Loss: 0.8390, Cross Loss: 0.1490\n",
            "200/239, Train_loss: 0.14904922246932983 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [201/239], Dice Loss: 0.9195, Cross Loss: 0.2598\n",
            "201/239, Train_loss: 0.25975221395492554 Train_dice: tensor(0.2598, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [202/239], Dice Loss: 0.8218, Cross Loss: 0.2593\n",
            "202/239, Train_loss: 0.2593034505844116 Train_dice: tensor(0.2593, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [203/239], Dice Loss: 0.9419, Cross Loss: 0.1494\n",
            "203/239, Train_loss: 0.14938372373580933 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [204/239], Dice Loss: 0.9411, Cross Loss: 0.1493\n",
            "204/239, Train_loss: 0.1493057906627655 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [205/239], Dice Loss: 0.8871, Cross Loss: 0.2591\n",
            "205/239, Train_loss: 0.25906965136528015 Train_dice: tensor(0.2591, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [206/239], Dice Loss: 0.9755, Cross Loss: 0.1491\n",
            "206/239, Train_loss: 0.1490916758775711 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [207/239], Dice Loss: 0.9081, Cross Loss: 0.1495\n",
            "207/239, Train_loss: 0.14947864413261414 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [208/239], Dice Loss: 0.8194, Cross Loss: 0.1494\n",
            "208/239, Train_loss: 0.14940857887268066 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [209/239], Dice Loss: 0.9093, Cross Loss: 0.2592\n",
            "209/239, Train_loss: 0.2592366635799408 Train_dice: tensor(0.2592, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [210/239], Dice Loss: 0.9962, Cross Loss: 0.1494\n",
            "210/239, Train_loss: 0.14937305450439453 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [211/239], Dice Loss: 0.9326, Cross Loss: 0.1494\n",
            "211/239, Train_loss: 0.14943420886993408 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [212/239], Dice Loss: 0.9233, Cross Loss: 0.1493\n",
            "212/239, Train_loss: 0.14932921528816223 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [213/239], Dice Loss: 0.6970, Cross Loss: 0.2593\n",
            "213/239, Train_loss: 0.2592821717262268 Train_dice: tensor(0.2593, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [214/239], Dice Loss: 0.8173, Cross Loss: 0.2593\n",
            "214/239, Train_loss: 0.25928911566734314 Train_dice: tensor(0.2593, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [215/239], Dice Loss: 0.8549, Cross Loss: 0.1491\n",
            "215/239, Train_loss: 0.14912360906600952 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [216/239], Dice Loss: 0.9094, Cross Loss: 0.1491\n",
            "216/239, Train_loss: 0.14913266897201538 Train_dice: tensor(0.1491, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [217/239], Dice Loss: 0.7313, Cross Loss: 0.2591\n",
            "217/239, Train_loss: 0.25906193256378174 Train_dice: tensor(0.2591, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [218/239], Dice Loss: 0.7351, Cross Loss: 0.2592\n",
            "218/239, Train_loss: 0.2592485845088959 Train_dice: tensor(0.2592, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [219/239], Dice Loss: 0.8683, Cross Loss: 0.1493\n",
            "219/239, Train_loss: 0.14932647347450256 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [220/239], Dice Loss: 0.9907, Cross Loss: 0.1494\n",
            "220/239, Train_loss: 0.14937622845172882 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [221/239], Dice Loss: 0.9774, Cross Loss: 0.1494\n",
            "221/239, Train_loss: 0.14939714968204498 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [222/239], Dice Loss: 0.9979, Cross Loss: 0.1493\n",
            "222/239, Train_loss: 0.14931292831897736 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [223/239], Dice Loss: 0.9999, Cross Loss: 0.1497\n",
            "223/239, Train_loss: 0.14968442916870117 Train_dice: tensor(0.1497, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [224/239], Dice Loss: 0.6465, Cross Loss: 0.1495\n",
            "224/239, Train_loss: 0.1495310664176941 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [225/239], Dice Loss: 0.9917, Cross Loss: 0.1492\n",
            "225/239, Train_loss: 0.14918845891952515 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [226/239], Dice Loss: 0.9992, Cross Loss: 0.1490\n",
            "226/239, Train_loss: 0.1490018367767334 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [227/239], Dice Loss: 0.9987, Cross Loss: 0.1489\n",
            "227/239, Train_loss: 0.1488681137561798 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [228/239], Dice Loss: 0.9990, Cross Loss: 0.1488\n",
            "228/239, Train_loss: 0.14882045984268188 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [229/239], Dice Loss: 0.9986, Cross Loss: 0.1488\n",
            "229/239, Train_loss: 0.1487557739019394 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [230/239], Dice Loss: 0.8444, Cross Loss: 0.1487\n",
            "230/239, Train_loss: 0.14869946241378784 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [231/239], Dice Loss: 0.9607, Cross Loss: 0.1486\n",
            "231/239, Train_loss: 0.14864560961723328 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [232/239], Dice Loss: 0.9699, Cross Loss: 0.2604\n",
            "232/239, Train_loss: 0.26039180159568787 Train_dice: tensor(0.2604, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [233/239], Dice Loss: 0.8215, Cross Loss: 0.1485\n",
            "233/239, Train_loss: 0.14852012693881989 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [234/239], Dice Loss: 0.9999, Cross Loss: 0.1485\n",
            "234/239, Train_loss: 0.14851124584674835 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [235/239], Dice Loss: 0.6791, Cross Loss: 0.2608\n",
            "235/239, Train_loss: 0.260750949382782 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [236/239], Dice Loss: 0.9899, Cross Loss: 0.2606\n",
            "236/239, Train_loss: 0.26063165068626404 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [237/239], Dice Loss: 0.9999, Cross Loss: 0.1485\n",
            "237/239, Train_loss: 0.14847171306610107 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [238/239], Dice Loss: 0.9780, Cross Loss: 0.1485\n",
            "238/239, Train_loss: 0.14847467839717865 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [1/100], Batch [239/239], Dice Loss: 0.9945, Cross Loss: 0.2600\n",
            "239/239, Train_loss: 0.26000264286994934 Train_dice: tensor(0.2600, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1868\n",
            "Epoch_metric: tensor(0.1868, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.1865\n",
            "test_dice_epoch: tensor(0.1865, device='cuda:0')\n",
            "current epoch: 1 current mean dice: tensor(0.1865, device='cuda:0')\n",
            "best mean dice: tensor(0.1865, device='cuda:0') at epoch: 1\n",
            "----------\n",
            "epoch 2/100\n",
            "Epoch [2/100], Batch [1/239], Dice Loss: 0.9466, Cross Loss: 0.2596\n",
            "1/239, Train_loss: 0.2596440613269806 Train_dice: tensor(0.2596, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [2/239], Dice Loss: 0.8051, Cross Loss: 0.2600\n",
            "2/239, Train_loss: 0.2600366175174713 Train_dice: tensor(0.2600, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [3/239], Dice Loss: 0.9942, Cross Loss: 0.2579\n",
            "3/239, Train_loss: 0.25785666704177856 Train_dice: tensor(0.2579, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [4/239], Dice Loss: 0.9437, Cross Loss: 0.2570\n",
            "4/239, Train_loss: 0.257007360458374 Train_dice: tensor(0.2570, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [5/239], Dice Loss: 0.9283, Cross Loss: 0.1504\n",
            "5/239, Train_loss: 0.1504037082195282 Train_dice: tensor(0.1504, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [6/239], Dice Loss: 0.9087, Cross Loss: 0.1502\n",
            "6/239, Train_loss: 0.15024901926517487 Train_dice: tensor(0.1502, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [7/239], Dice Loss: 0.7763, Cross Loss: 0.2575\n",
            "7/239, Train_loss: 0.2574708163738251 Train_dice: tensor(0.2575, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [8/239], Dice Loss: 0.9528, Cross Loss: 0.2589\n",
            "8/239, Train_loss: 0.25890395045280457 Train_dice: tensor(0.2589, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [9/239], Dice Loss: 0.9833, Cross Loss: 0.2599\n",
            "9/239, Train_loss: 0.2599044442176819 Train_dice: tensor(0.2599, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [10/239], Dice Loss: 0.8958, Cross Loss: 0.1488\n",
            "10/239, Train_loss: 0.1487710177898407 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [11/239], Dice Loss: 0.8235, Cross Loss: 0.1490\n",
            "11/239, Train_loss: 0.149031400680542 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [12/239], Dice Loss: 0.9251, Cross Loss: 0.1487\n",
            "12/239, Train_loss: 0.14871147274971008 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [13/239], Dice Loss: 0.9591, Cross Loss: 0.1487\n",
            "13/239, Train_loss: 0.14872246980667114 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [14/239], Dice Loss: 0.7791, Cross Loss: 0.1490\n",
            "14/239, Train_loss: 0.14896848797798157 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [15/239], Dice Loss: 0.7974, Cross Loss: 0.1488\n",
            "15/239, Train_loss: 0.14880309998989105 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [16/239], Dice Loss: 0.9287, Cross Loss: 0.1486\n",
            "16/239, Train_loss: 0.14863204956054688 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [17/239], Dice Loss: 0.7858, Cross Loss: 0.1486\n",
            "17/239, Train_loss: 0.1486019343137741 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [18/239], Dice Loss: 0.9715, Cross Loss: 0.1486\n",
            "18/239, Train_loss: 0.1485500931739807 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [19/239], Dice Loss: 0.7967, Cross Loss: 0.2607\n",
            "19/239, Train_loss: 0.2606847286224365 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [20/239], Dice Loss: 0.6524, Cross Loss: 0.2609\n",
            "20/239, Train_loss: 0.2609030604362488 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [21/239], Dice Loss: 0.9662, Cross Loss: 0.2607\n",
            "21/239, Train_loss: 0.2607044577598572 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [22/239], Dice Loss: 0.9471, Cross Loss: 0.1485\n",
            "22/239, Train_loss: 0.14850476384162903 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [23/239], Dice Loss: 0.7334, Cross Loss: 0.2607\n",
            "23/239, Train_loss: 0.2606644332408905 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [24/239], Dice Loss: 0.8330, Cross Loss: 0.2607\n",
            "24/239, Train_loss: 0.2606702446937561 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [25/239], Dice Loss: 0.9395, Cross Loss: 0.1486\n",
            "25/239, Train_loss: 0.1485743373632431 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [26/239], Dice Loss: 0.9401, Cross Loss: 0.1486\n",
            "26/239, Train_loss: 0.14864115417003632 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [27/239], Dice Loss: 0.9651, Cross Loss: 0.1487\n",
            "27/239, Train_loss: 0.14865708351135254 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [28/239], Dice Loss: 0.7693, Cross Loss: 0.1487\n",
            "28/239, Train_loss: 0.14866217970848083 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [29/239], Dice Loss: 0.8906, Cross Loss: 0.1486\n",
            "29/239, Train_loss: 0.14860711991786957 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [30/239], Dice Loss: 0.9866, Cross Loss: 0.1486\n",
            "30/239, Train_loss: 0.14857925474643707 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [31/239], Dice Loss: 0.8554, Cross Loss: 0.1486\n",
            "31/239, Train_loss: 0.148557648062706 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [32/239], Dice Loss: 0.9467, Cross Loss: 0.1485\n",
            "32/239, Train_loss: 0.1485031396150589 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [33/239], Dice Loss: 0.6654, Cross Loss: 0.2609\n",
            "33/239, Train_loss: 0.2608948349952698 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [34/239], Dice Loss: 0.9841, Cross Loss: 0.1485\n",
            "34/239, Train_loss: 0.14848092198371887 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [35/239], Dice Loss: 0.8973, Cross Loss: 0.2608\n",
            "35/239, Train_loss: 0.2607859671115875 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [36/239], Dice Loss: 0.9110, Cross Loss: 0.1485\n",
            "36/239, Train_loss: 0.14846734702587128 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [37/239], Dice Loss: 0.9572, Cross Loss: 0.1485\n",
            "37/239, Train_loss: 0.1484609693288803 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [38/239], Dice Loss: 0.8546, Cross Loss: 0.1485\n",
            "38/239, Train_loss: 0.14845022559165955 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [39/239], Dice Loss: 0.8410, Cross Loss: 0.2608\n",
            "39/239, Train_loss: 0.2608107328414917 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [40/239], Dice Loss: 0.8181, Cross Loss: 0.2608\n",
            "40/239, Train_loss: 0.2608221769332886 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [41/239], Dice Loss: 0.9449, Cross Loss: 0.1484\n",
            "41/239, Train_loss: 0.14843393862247467 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [42/239], Dice Loss: 0.6903, Cross Loss: 0.1484\n",
            "42/239, Train_loss: 0.14842894673347473 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [43/239], Dice Loss: 0.9310, Cross Loss: 0.1484\n",
            "43/239, Train_loss: 0.1484287977218628 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [44/239], Dice Loss: 0.7866, Cross Loss: 0.1484\n",
            "44/239, Train_loss: 0.14842122793197632 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [45/239], Dice Loss: 0.7856, Cross Loss: 0.2609\n",
            "45/239, Train_loss: 0.2609122693538666 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [46/239], Dice Loss: 0.9647, Cross Loss: 0.1484\n",
            "46/239, Train_loss: 0.14840996265411377 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [47/239], Dice Loss: 0.8995, Cross Loss: 0.1484\n",
            "47/239, Train_loss: 0.14841386675834656 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [48/239], Dice Loss: 0.7963, Cross Loss: 0.2609\n",
            "48/239, Train_loss: 0.2608807682991028 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [49/239], Dice Loss: 0.8838, Cross Loss: 0.1484\n",
            "49/239, Train_loss: 0.14844438433647156 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [50/239], Dice Loss: 0.7971, Cross Loss: 0.1484\n",
            "50/239, Train_loss: 0.1484294980764389 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [51/239], Dice Loss: 0.7496, Cross Loss: 0.1484\n",
            "51/239, Train_loss: 0.14843831956386566 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [52/239], Dice Loss: 0.8327, Cross Loss: 0.1484\n",
            "52/239, Train_loss: 0.14843352138996124 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [53/239], Dice Loss: 0.8958, Cross Loss: 0.1484\n",
            "53/239, Train_loss: 0.148440420627594 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [54/239], Dice Loss: 0.8506, Cross Loss: 0.2609\n",
            "54/239, Train_loss: 0.2609347701072693 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [55/239], Dice Loss: 0.7425, Cross Loss: 0.1484\n",
            "55/239, Train_loss: 0.14840516448020935 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [56/239], Dice Loss: 0.9309, Cross Loss: 0.1484\n",
            "56/239, Train_loss: 0.14840060472488403 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [57/239], Dice Loss: 0.8288, Cross Loss: 0.2610\n",
            "57/239, Train_loss: 0.2609540820121765 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [58/239], Dice Loss: 0.8846, Cross Loss: 0.1484\n",
            "58/239, Train_loss: 0.14837254583835602 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [59/239], Dice Loss: 0.8355, Cross Loss: 0.1484\n",
            "59/239, Train_loss: 0.14837253093719482 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [60/239], Dice Loss: 0.8278, Cross Loss: 0.1484\n",
            "60/239, Train_loss: 0.1483592987060547 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [61/239], Dice Loss: 0.9609, Cross Loss: 0.1484\n",
            "61/239, Train_loss: 0.1483539193868637 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [62/239], Dice Loss: 0.8050, Cross Loss: 0.2611\n",
            "62/239, Train_loss: 0.26109588146209717 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [63/239], Dice Loss: 0.8754, Cross Loss: 0.1484\n",
            "63/239, Train_loss: 0.14835557341575623 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [64/239], Dice Loss: 0.9240, Cross Loss: 0.1483\n",
            "64/239, Train_loss: 0.14834675192832947 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [65/239], Dice Loss: 0.7355, Cross Loss: 0.1483\n",
            "65/239, Train_loss: 0.14834460616111755 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [66/239], Dice Loss: 0.7359, Cross Loss: 0.1483\n",
            "66/239, Train_loss: 0.14833402633666992 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [67/239], Dice Loss: 0.7284, Cross Loss: 0.1483\n",
            "67/239, Train_loss: 0.14832937717437744 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [68/239], Dice Loss: 0.8237, Cross Loss: 0.1483\n",
            "68/239, Train_loss: 0.1483292579650879 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [69/239], Dice Loss: 0.9714, Cross Loss: 0.2611\n",
            "69/239, Train_loss: 0.2610880136489868 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [70/239], Dice Loss: 0.8405, Cross Loss: 0.2611\n",
            "70/239, Train_loss: 0.2611008584499359 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [71/239], Dice Loss: 0.7152, Cross Loss: 0.1483\n",
            "71/239, Train_loss: 0.14831486344337463 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [72/239], Dice Loss: 0.8371, Cross Loss: 0.2612\n",
            "72/239, Train_loss: 0.26118865609169006 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [73/239], Dice Loss: 0.8821, Cross Loss: 0.1483\n",
            "73/239, Train_loss: 0.148313969373703 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [74/239], Dice Loss: 0.8874, Cross Loss: 0.1483\n",
            "74/239, Train_loss: 0.14831215143203735 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [75/239], Dice Loss: 0.9636, Cross Loss: 0.1483\n",
            "75/239, Train_loss: 0.14831143617630005 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [76/239], Dice Loss: 0.9351, Cross Loss: 0.1483\n",
            "76/239, Train_loss: 0.14830917119979858 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [77/239], Dice Loss: 0.9581, Cross Loss: 0.1483\n",
            "77/239, Train_loss: 0.14831791818141937 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [78/239], Dice Loss: 0.8583, Cross Loss: 0.1483\n",
            "78/239, Train_loss: 0.14830665290355682 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [79/239], Dice Loss: 0.9432, Cross Loss: 0.1483\n",
            "79/239, Train_loss: 0.14830781519412994 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [80/239], Dice Loss: 0.9006, Cross Loss: 0.1483\n",
            "80/239, Train_loss: 0.14833365380764008 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [81/239], Dice Loss: 0.9683, Cross Loss: 0.1483\n",
            "81/239, Train_loss: 0.1482921540737152 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [82/239], Dice Loss: 0.6921, Cross Loss: 0.1483\n",
            "82/239, Train_loss: 0.14831441640853882 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [83/239], Dice Loss: 0.9333, Cross Loss: 0.1483\n",
            "83/239, Train_loss: 0.14830827713012695 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [84/239], Dice Loss: 0.9531, Cross Loss: 0.1483\n",
            "84/239, Train_loss: 0.14828042685985565 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [85/239], Dice Loss: 0.8372, Cross Loss: 0.2611\n",
            "85/239, Train_loss: 0.26113590598106384 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [86/239], Dice Loss: 0.8187, Cross Loss: 0.1483\n",
            "86/239, Train_loss: 0.14829683303833008 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [87/239], Dice Loss: 0.9449, Cross Loss: 0.1483\n",
            "87/239, Train_loss: 0.1482928991317749 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [88/239], Dice Loss: 0.9829, Cross Loss: 0.1483\n",
            "88/239, Train_loss: 0.14827629923820496 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [89/239], Dice Loss: 0.9877, Cross Loss: 0.1483\n",
            "89/239, Train_loss: 0.14827287197113037 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [90/239], Dice Loss: 0.9510, Cross Loss: 0.1483\n",
            "90/239, Train_loss: 0.14826947450637817 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [91/239], Dice Loss: 0.7914, Cross Loss: 0.2612\n",
            "91/239, Train_loss: 0.2612118422985077 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [92/239], Dice Loss: 0.8693, Cross Loss: 0.1483\n",
            "92/239, Train_loss: 0.14826220273971558 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [93/239], Dice Loss: 0.7752, Cross Loss: 0.1483\n",
            "93/239, Train_loss: 0.1482602059841156 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [94/239], Dice Loss: 0.9323, Cross Loss: 0.2613\n",
            "94/239, Train_loss: 0.2612610459327698 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [95/239], Dice Loss: 0.8509, Cross Loss: 0.1483\n",
            "95/239, Train_loss: 0.14826032519340515 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [96/239], Dice Loss: 0.9794, Cross Loss: 0.1483\n",
            "96/239, Train_loss: 0.14825689792633057 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [97/239], Dice Loss: 0.8581, Cross Loss: 0.1483\n",
            "97/239, Train_loss: 0.14825446903705597 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [98/239], Dice Loss: 0.8473, Cross Loss: 0.1483\n",
            "98/239, Train_loss: 0.1482534557580948 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [99/239], Dice Loss: 0.9669, Cross Loss: 0.2612\n",
            "99/239, Train_loss: 0.26123350858688354 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [100/239], Dice Loss: 0.9870, Cross Loss: 0.2613\n",
            "100/239, Train_loss: 0.261285662651062 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [101/239], Dice Loss: 0.7571, Cross Loss: 0.2613\n",
            "101/239, Train_loss: 0.26127123832702637 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [102/239], Dice Loss: 0.8290, Cross Loss: 0.1483\n",
            "102/239, Train_loss: 0.1482531726360321 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [103/239], Dice Loss: 0.9794, Cross Loss: 0.1482\n",
            "103/239, Train_loss: 0.14824876189231873 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [104/239], Dice Loss: 0.8510, Cross Loss: 0.2612\n",
            "104/239, Train_loss: 0.26123544573783875 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [105/239], Dice Loss: 0.8516, Cross Loss: 0.1483\n",
            "105/239, Train_loss: 0.14825445413589478 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [106/239], Dice Loss: 0.9658, Cross Loss: 0.1483\n",
            "106/239, Train_loss: 0.14825478196144104 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [107/239], Dice Loss: 0.7765, Cross Loss: 0.1483\n",
            "107/239, Train_loss: 0.1482512503862381 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [108/239], Dice Loss: 0.8438, Cross Loss: 0.2612\n",
            "108/239, Train_loss: 0.26124095916748047 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [109/239], Dice Loss: 0.9302, Cross Loss: 0.1482\n",
            "109/239, Train_loss: 0.14824944734573364 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [110/239], Dice Loss: 0.8862, Cross Loss: 0.1482\n",
            "110/239, Train_loss: 0.14824926853179932 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [111/239], Dice Loss: 0.9312, Cross Loss: 0.1482\n",
            "111/239, Train_loss: 0.14824891090393066 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [112/239], Dice Loss: 0.8012, Cross Loss: 0.1482\n",
            "112/239, Train_loss: 0.14824837446212769 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [113/239], Dice Loss: 0.8528, Cross Loss: 0.1483\n",
            "113/239, Train_loss: 0.14827340841293335 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [114/239], Dice Loss: 0.7775, Cross Loss: 0.1483\n",
            "114/239, Train_loss: 0.14827242493629456 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [115/239], Dice Loss: 0.9548, Cross Loss: 0.1482\n",
            "115/239, Train_loss: 0.1482471525669098 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [116/239], Dice Loss: 0.9170, Cross Loss: 0.1482\n",
            "116/239, Train_loss: 0.14824554324150085 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [117/239], Dice Loss: 0.9245, Cross Loss: 0.1482\n",
            "117/239, Train_loss: 0.1482420563697815 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [118/239], Dice Loss: 0.9251, Cross Loss: 0.1482\n",
            "118/239, Train_loss: 0.14824044704437256 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [119/239], Dice Loss: 0.9171, Cross Loss: 0.1482\n",
            "119/239, Train_loss: 0.14824795722961426 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [120/239], Dice Loss: 0.9134, Cross Loss: 0.1482\n",
            "120/239, Train_loss: 0.14823868870735168 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [121/239], Dice Loss: 0.7912, Cross Loss: 0.1482\n",
            "121/239, Train_loss: 0.1482377052307129 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [122/239], Dice Loss: 0.7370, Cross Loss: 0.2613\n",
            "122/239, Train_loss: 0.26130178570747375 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [123/239], Dice Loss: 0.9308, Cross Loss: 0.1482\n",
            "123/239, Train_loss: 0.14823374152183533 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [124/239], Dice Loss: 0.7031, Cross Loss: 0.2613\n",
            "124/239, Train_loss: 0.2612992227077484 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [125/239], Dice Loss: 0.9197, Cross Loss: 0.2613\n",
            "125/239, Train_loss: 0.2613067626953125 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [126/239], Dice Loss: 0.8924, Cross Loss: 0.1482\n",
            "126/239, Train_loss: 0.14823246002197266 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [127/239], Dice Loss: 0.9153, Cross Loss: 0.1482\n",
            "127/239, Train_loss: 0.14823320508003235 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [128/239], Dice Loss: 0.8913, Cross Loss: 0.2613\n",
            "128/239, Train_loss: 0.2612815797328949 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [129/239], Dice Loss: 0.8625, Cross Loss: 0.1482\n",
            "129/239, Train_loss: 0.14822928607463837 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [130/239], Dice Loss: 0.7455, Cross Loss: 0.2613\n",
            "130/239, Train_loss: 0.26128193736076355 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [131/239], Dice Loss: 0.8892, Cross Loss: 0.1482\n",
            "131/239, Train_loss: 0.14823108911514282 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [132/239], Dice Loss: 0.9974, Cross Loss: 0.1482\n",
            "132/239, Train_loss: 0.14822962880134583 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [133/239], Dice Loss: 0.8112, Cross Loss: 0.1482\n",
            "133/239, Train_loss: 0.14822912216186523 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [134/239], Dice Loss: 0.8273, Cross Loss: 0.2613\n",
            "134/239, Train_loss: 0.26128554344177246 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [135/239], Dice Loss: 0.7883, Cross Loss: 0.1482\n",
            "135/239, Train_loss: 0.14822939038276672 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [136/239], Dice Loss: 0.9985, Cross Loss: 0.2613\n",
            "136/239, Train_loss: 0.26128488779067993 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [137/239], Dice Loss: 0.6925, Cross Loss: 0.2613\n",
            "137/239, Train_loss: 0.26131734251976013 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [138/239], Dice Loss: 0.9151, Cross Loss: 0.1482\n",
            "138/239, Train_loss: 0.14823175966739655 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [139/239], Dice Loss: 0.8378, Cross Loss: 0.2613\n",
            "139/239, Train_loss: 0.2612835764884949 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [140/239], Dice Loss: 0.9160, Cross Loss: 0.1482\n",
            "140/239, Train_loss: 0.14823046326637268 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [141/239], Dice Loss: 0.6910, Cross Loss: 0.2613\n",
            "141/239, Train_loss: 0.2612748444080353 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [142/239], Dice Loss: 0.8930, Cross Loss: 0.1482\n",
            "142/239, Train_loss: 0.1482294797897339 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [143/239], Dice Loss: 0.9349, Cross Loss: 0.2613\n",
            "143/239, Train_loss: 0.261311799287796 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [144/239], Dice Loss: 0.8220, Cross Loss: 0.1482\n",
            "144/239, Train_loss: 0.14823755621910095 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [145/239], Dice Loss: 0.7852, Cross Loss: 0.1482\n",
            "145/239, Train_loss: 0.14823167026042938 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [146/239], Dice Loss: 0.8110, Cross Loss: 0.1482\n",
            "146/239, Train_loss: 0.14823178946971893 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [147/239], Dice Loss: 0.8969, Cross Loss: 0.1482\n",
            "147/239, Train_loss: 0.14823219180107117 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [148/239], Dice Loss: 0.9265, Cross Loss: 0.1482\n",
            "148/239, Train_loss: 0.14823195338249207 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [149/239], Dice Loss: 0.6531, Cross Loss: 0.2613\n",
            "149/239, Train_loss: 0.261282742023468 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [150/239], Dice Loss: 0.9922, Cross Loss: 0.1482\n",
            "150/239, Train_loss: 0.14823082089424133 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [151/239], Dice Loss: 0.6956, Cross Loss: 0.2613\n",
            "151/239, Train_loss: 0.26127925515174866 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [152/239], Dice Loss: 0.9805, Cross Loss: 0.1482\n",
            "152/239, Train_loss: 0.14823389053344727 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [153/239], Dice Loss: 0.9077, Cross Loss: 0.1482\n",
            "153/239, Train_loss: 0.14823052287101746 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [154/239], Dice Loss: 0.7279, Cross Loss: 0.2613\n",
            "154/239, Train_loss: 0.2612765431404114 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [155/239], Dice Loss: 0.8400, Cross Loss: 0.1483\n",
            "155/239, Train_loss: 0.14825305342674255 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [156/239], Dice Loss: 0.8322, Cross Loss: 0.2613\n",
            "156/239, Train_loss: 0.2612752318382263 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [157/239], Dice Loss: 0.8828, Cross Loss: 0.2613\n",
            "157/239, Train_loss: 0.2612822353839874 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [158/239], Dice Loss: 0.7775, Cross Loss: 0.2613\n",
            "158/239, Train_loss: 0.26127737760543823 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [159/239], Dice Loss: 0.9223, Cross Loss: 0.1482\n",
            "159/239, Train_loss: 0.14823314547538757 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [160/239], Dice Loss: 0.8039, Cross Loss: 0.2613\n",
            "160/239, Train_loss: 0.2612728476524353 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [161/239], Dice Loss: 0.9054, Cross Loss: 0.1482\n",
            "161/239, Train_loss: 0.14823517203330994 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [162/239], Dice Loss: 0.9608, Cross Loss: 0.2613\n",
            "162/239, Train_loss: 0.26127249002456665 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [163/239], Dice Loss: 0.9934, Cross Loss: 0.1482\n",
            "163/239, Train_loss: 0.14824137091636658 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [164/239], Dice Loss: 0.9672, Cross Loss: 0.1482\n",
            "164/239, Train_loss: 0.14824199676513672 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [165/239], Dice Loss: 0.7572, Cross Loss: 0.2613\n",
            "165/239, Train_loss: 0.261269748210907 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [166/239], Dice Loss: 0.9896, Cross Loss: 0.2613\n",
            "166/239, Train_loss: 0.2612634301185608 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [167/239], Dice Loss: 0.9181, Cross Loss: 0.2613\n",
            "167/239, Train_loss: 0.261267751455307 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [168/239], Dice Loss: 0.7964, Cross Loss: 0.2613\n",
            "168/239, Train_loss: 0.2612648010253906 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [169/239], Dice Loss: 0.8883, Cross Loss: 0.2613\n",
            "169/239, Train_loss: 0.26129150390625 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [170/239], Dice Loss: 0.9698, Cross Loss: 0.2612\n",
            "170/239, Train_loss: 0.26124680042266846 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [171/239], Dice Loss: 0.8999, Cross Loss: 0.2612\n",
            "171/239, Train_loss: 0.2612420320510864 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [172/239], Dice Loss: 0.8902, Cross Loss: 0.1483\n",
            "172/239, Train_loss: 0.14825350046157837 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [173/239], Dice Loss: 0.8783, Cross Loss: 0.2613\n",
            "173/239, Train_loss: 0.26128363609313965 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [174/239], Dice Loss: 0.7579, Cross Loss: 0.2612\n",
            "174/239, Train_loss: 0.26122722029685974 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [175/239], Dice Loss: 0.9216, Cross Loss: 0.2612\n",
            "175/239, Train_loss: 0.26121580600738525 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [176/239], Dice Loss: 0.8473, Cross Loss: 0.1483\n",
            "176/239, Train_loss: 0.1482636034488678 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [177/239], Dice Loss: 0.7354, Cross Loss: 0.1483\n",
            "177/239, Train_loss: 0.14826659858226776 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [178/239], Dice Loss: 0.8992, Cross Loss: 0.2612\n",
            "178/239, Train_loss: 0.26119816303253174 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [179/239], Dice Loss: 0.9846, Cross Loss: 0.1483\n",
            "179/239, Train_loss: 0.1482740342617035 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [180/239], Dice Loss: 0.7995, Cross Loss: 0.1483\n",
            "180/239, Train_loss: 0.14827625453472137 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [181/239], Dice Loss: 0.9375, Cross Loss: 0.2613\n",
            "181/239, Train_loss: 0.26126164197921753 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [182/239], Dice Loss: 0.8041, Cross Loss: 0.1483\n",
            "182/239, Train_loss: 0.14827440679073334 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [183/239], Dice Loss: 0.9635, Cross Loss: 0.2612\n",
            "183/239, Train_loss: 0.26119595766067505 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [184/239], Dice Loss: 0.9270, Cross Loss: 0.1483\n",
            "184/239, Train_loss: 0.14828018844127655 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [185/239], Dice Loss: 0.8833, Cross Loss: 0.1483\n",
            "185/239, Train_loss: 0.1482817530632019 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [186/239], Dice Loss: 0.7513, Cross Loss: 0.1483\n",
            "186/239, Train_loss: 0.14828339219093323 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [187/239], Dice Loss: 0.9654, Cross Loss: 0.1483\n",
            "187/239, Train_loss: 0.14828358590602875 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [188/239], Dice Loss: 0.7309, Cross Loss: 0.2612\n",
            "188/239, Train_loss: 0.2611663341522217 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [189/239], Dice Loss: 0.8055, Cross Loss: 0.1483\n",
            "189/239, Train_loss: 0.14829033613204956 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [190/239], Dice Loss: 0.9700, Cross Loss: 0.1483\n",
            "190/239, Train_loss: 0.14828640222549438 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [191/239], Dice Loss: 0.8160, Cross Loss: 0.2612\n",
            "191/239, Train_loss: 0.26117050647735596 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [192/239], Dice Loss: 0.9340, Cross Loss: 0.1483\n",
            "192/239, Train_loss: 0.14828075468540192 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [193/239], Dice Loss: 0.9854, Cross Loss: 0.2612\n",
            "193/239, Train_loss: 0.2611848711967468 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [194/239], Dice Loss: 0.9923, Cross Loss: 0.1483\n",
            "194/239, Train_loss: 0.1482844352722168 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [195/239], Dice Loss: 0.9918, Cross Loss: 0.1483\n",
            "195/239, Train_loss: 0.1482848972082138 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [196/239], Dice Loss: 0.9983, Cross Loss: 0.2612\n",
            "196/239, Train_loss: 0.2611689567565918 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [197/239], Dice Loss: 0.9987, Cross Loss: 0.1483\n",
            "197/239, Train_loss: 0.14828969538211823 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [198/239], Dice Loss: 0.9985, Cross Loss: 0.1483\n",
            "198/239, Train_loss: 0.1482902467250824 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [199/239], Dice Loss: 0.8177, Cross Loss: 0.1483\n",
            "199/239, Train_loss: 0.14828717708587646 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [200/239], Dice Loss: 0.7896, Cross Loss: 0.1483\n",
            "200/239, Train_loss: 0.14828623831272125 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [201/239], Dice Loss: 0.8954, Cross Loss: 0.2612\n",
            "201/239, Train_loss: 0.26117509603500366 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [202/239], Dice Loss: 0.7516, Cross Loss: 0.2612\n",
            "202/239, Train_loss: 0.2611628770828247 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [203/239], Dice Loss: 0.8722, Cross Loss: 0.1483\n",
            "203/239, Train_loss: 0.14828775823116302 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [204/239], Dice Loss: 0.9840, Cross Loss: 0.1483\n",
            "204/239, Train_loss: 0.14828839898109436 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [205/239], Dice Loss: 0.8379, Cross Loss: 0.2612\n",
            "205/239, Train_loss: 0.2611733675003052 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [206/239], Dice Loss: 0.9243, Cross Loss: 0.1483\n",
            "206/239, Train_loss: 0.14828144013881683 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [207/239], Dice Loss: 0.8146, Cross Loss: 0.1483\n",
            "207/239, Train_loss: 0.14828632771968842 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [208/239], Dice Loss: 0.8828, Cross Loss: 0.1483\n",
            "208/239, Train_loss: 0.1482858657836914 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [209/239], Dice Loss: 0.8916, Cross Loss: 0.2612\n",
            "209/239, Train_loss: 0.261154443025589 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [210/239], Dice Loss: 0.9992, Cross Loss: 0.1483\n",
            "210/239, Train_loss: 0.1482880562543869 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [211/239], Dice Loss: 0.8850, Cross Loss: 0.1483\n",
            "211/239, Train_loss: 0.14828841388225555 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [212/239], Dice Loss: 0.8814, Cross Loss: 0.1483\n",
            "212/239, Train_loss: 0.14828768372535706 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [213/239], Dice Loss: 0.6979, Cross Loss: 0.2612\n",
            "213/239, Train_loss: 0.26117295026779175 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [214/239], Dice Loss: 0.7397, Cross Loss: 0.2612\n",
            "214/239, Train_loss: 0.2611718475818634 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [215/239], Dice Loss: 0.7425, Cross Loss: 0.1483\n",
            "215/239, Train_loss: 0.14828576147556305 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [216/239], Dice Loss: 0.8666, Cross Loss: 0.1483\n",
            "216/239, Train_loss: 0.1482861042022705 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [217/239], Dice Loss: 0.6941, Cross Loss: 0.2611\n",
            "217/239, Train_loss: 0.26114481687545776 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [218/239], Dice Loss: 0.6811, Cross Loss: 0.2612\n",
            "218/239, Train_loss: 0.26116856932640076 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [219/239], Dice Loss: 0.8568, Cross Loss: 0.1483\n",
            "219/239, Train_loss: 0.1482895314693451 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [220/239], Dice Loss: 0.9836, Cross Loss: 0.1483\n",
            "220/239, Train_loss: 0.14829012751579285 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [221/239], Dice Loss: 0.9942, Cross Loss: 0.1483\n",
            "221/239, Train_loss: 0.14832419157028198 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [222/239], Dice Loss: 0.9992, Cross Loss: 0.1483\n",
            "222/239, Train_loss: 0.14829008281230927 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [223/239], Dice Loss: 0.9920, Cross Loss: 0.1484\n",
            "223/239, Train_loss: 0.14836350083351135 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [224/239], Dice Loss: 0.6128, Cross Loss: 0.1484\n",
            "224/239, Train_loss: 0.14836221933364868 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [225/239], Dice Loss: 0.9842, Cross Loss: 0.1483\n",
            "225/239, Train_loss: 0.14831407368183136 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [226/239], Dice Loss: 0.9993, Cross Loss: 0.1483\n",
            "226/239, Train_loss: 0.1483154296875 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [227/239], Dice Loss: 0.9999, Cross Loss: 0.1483\n",
            "227/239, Train_loss: 0.14830881357192993 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [228/239], Dice Loss: 0.9982, Cross Loss: 0.1483\n",
            "228/239, Train_loss: 0.14829912781715393 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [229/239], Dice Loss: 0.9984, Cross Loss: 0.1483\n",
            "229/239, Train_loss: 0.14831708371639252 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [230/239], Dice Loss: 0.9957, Cross Loss: 0.1483\n",
            "230/239, Train_loss: 0.14830680191516876 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [231/239], Dice Loss: 0.9509, Cross Loss: 0.1483\n",
            "231/239, Train_loss: 0.1482948511838913 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [232/239], Dice Loss: 0.9824, Cross Loss: 0.2612\n",
            "232/239, Train_loss: 0.26120835542678833 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [233/239], Dice Loss: 0.7613, Cross Loss: 0.1483\n",
            "233/239, Train_loss: 0.14826302230358124 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [234/239], Dice Loss: 0.9992, Cross Loss: 0.1483\n",
            "234/239, Train_loss: 0.14826110005378723 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [235/239], Dice Loss: 0.6054, Cross Loss: 0.2613\n",
            "235/239, Train_loss: 0.26126718521118164 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [236/239], Dice Loss: 0.9958, Cross Loss: 0.2612\n",
            "236/239, Train_loss: 0.2612173557281494 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [237/239], Dice Loss: 0.9999, Cross Loss: 0.1483\n",
            "237/239, Train_loss: 0.14825914800167084 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [238/239], Dice Loss: 0.9243, Cross Loss: 0.1483\n",
            "238/239, Train_loss: 0.14826273918151855 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [2/100], Batch [239/239], Dice Loss: 0.9983, Cross Loss: 0.2612\n",
            "239/239, Train_loss: 0.26122361421585083 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1860\n",
            "Epoch_metric: tensor(0.1860, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.1860\n",
            "test_dice_epoch: tensor(0.1860, device='cuda:0')\n",
            "current epoch: 2 current mean dice: tensor(0.1860, device='cuda:0')\n",
            "best mean dice: tensor(0.1865, device='cuda:0') at epoch: 1\n",
            "----------\n",
            "epoch 3/100\n",
            "Epoch [3/100], Batch [1/239], Dice Loss: 0.9701, Cross Loss: 0.2613\n",
            "1/239, Train_loss: 0.2612703740596771 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [2/239], Dice Loss: 0.7543, Cross Loss: 0.2612\n",
            "2/239, Train_loss: 0.26122117042541504 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [3/239], Dice Loss: 0.9677, Cross Loss: 0.2612\n",
            "3/239, Train_loss: 0.26121947169303894 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [4/239], Dice Loss: 0.9312, Cross Loss: 0.2612\n",
            "4/239, Train_loss: 0.26121336221694946 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [5/239], Dice Loss: 0.9170, Cross Loss: 0.1483\n",
            "5/239, Train_loss: 0.14825986325740814 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [6/239], Dice Loss: 0.9689, Cross Loss: 0.1483\n",
            "6/239, Train_loss: 0.14826148748397827 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [7/239], Dice Loss: 0.7443, Cross Loss: 0.2612\n",
            "7/239, Train_loss: 0.26120680570602417 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [8/239], Dice Loss: 0.9470, Cross Loss: 0.2612\n",
            "8/239, Train_loss: 0.26119673252105713 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [9/239], Dice Loss: 0.9767, Cross Loss: 0.2612\n",
            "9/239, Train_loss: 0.26124581694602966 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [10/239], Dice Loss: 0.8231, Cross Loss: 0.1483\n",
            "10/239, Train_loss: 0.14827480912208557 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [11/239], Dice Loss: 0.7757, Cross Loss: 0.1483\n",
            "11/239, Train_loss: 0.14827841520309448 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [12/239], Dice Loss: 0.8638, Cross Loss: 0.1483\n",
            "12/239, Train_loss: 0.14827968180179596 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [13/239], Dice Loss: 0.9798, Cross Loss: 0.1483\n",
            "13/239, Train_loss: 0.14827877283096313 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [14/239], Dice Loss: 0.7612, Cross Loss: 0.1483\n",
            "14/239, Train_loss: 0.14828309416770935 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [15/239], Dice Loss: 0.7661, Cross Loss: 0.1483\n",
            "15/239, Train_loss: 0.14828327298164368 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [16/239], Dice Loss: 0.8912, Cross Loss: 0.1483\n",
            "16/239, Train_loss: 0.1482815444469452 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [17/239], Dice Loss: 0.7950, Cross Loss: 0.1483\n",
            "17/239, Train_loss: 0.14828012883663177 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [18/239], Dice Loss: 0.9522, Cross Loss: 0.1483\n",
            "18/239, Train_loss: 0.14827333390712738 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [19/239], Dice Loss: 0.7474, Cross Loss: 0.2612\n",
            "19/239, Train_loss: 0.2611836791038513 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [20/239], Dice Loss: 0.6150, Cross Loss: 0.2612\n",
            "20/239, Train_loss: 0.26124608516693115 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [21/239], Dice Loss: 0.9540, Cross Loss: 0.2612\n",
            "21/239, Train_loss: 0.2611922025680542 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [22/239], Dice Loss: 0.9410, Cross Loss: 0.1483\n",
            "22/239, Train_loss: 0.14827649295330048 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [23/239], Dice Loss: 0.7819, Cross Loss: 0.2612\n",
            "23/239, Train_loss: 0.26119181513786316 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [24/239], Dice Loss: 0.6523, Cross Loss: 0.2612\n",
            "24/239, Train_loss: 0.26118069887161255 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [25/239], Dice Loss: 0.9553, Cross Loss: 0.1483\n",
            "25/239, Train_loss: 0.14827945828437805 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [26/239], Dice Loss: 0.9244, Cross Loss: 0.1483\n",
            "26/239, Train_loss: 0.14828504621982574 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [27/239], Dice Loss: 0.9736, Cross Loss: 0.1483\n",
            "27/239, Train_loss: 0.14828604459762573 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [28/239], Dice Loss: 0.8005, Cross Loss: 0.1483\n",
            "28/239, Train_loss: 0.14828239381313324 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [29/239], Dice Loss: 0.8998, Cross Loss: 0.1483\n",
            "29/239, Train_loss: 0.1482865810394287 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [30/239], Dice Loss: 0.9797, Cross Loss: 0.1483\n",
            "30/239, Train_loss: 0.1482853889465332 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [31/239], Dice Loss: 0.7949, Cross Loss: 0.1483\n",
            "31/239, Train_loss: 0.14828237891197205 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [32/239], Dice Loss: 0.9421, Cross Loss: 0.1483\n",
            "32/239, Train_loss: 0.1482773721218109 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [33/239], Dice Loss: 0.5411, Cross Loss: 0.2613\n",
            "33/239, Train_loss: 0.2612544298171997 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [34/239], Dice Loss: 0.9864, Cross Loss: 0.1483\n",
            "34/239, Train_loss: 0.14827284216880798 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [35/239], Dice Loss: 0.9021, Cross Loss: 0.2612\n",
            "35/239, Train_loss: 0.26118916273117065 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [36/239], Dice Loss: 0.8969, Cross Loss: 0.1483\n",
            "36/239, Train_loss: 0.1482749879360199 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [37/239], Dice Loss: 0.9218, Cross Loss: 0.1483\n",
            "37/239, Train_loss: 0.1482723504304886 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [38/239], Dice Loss: 0.7874, Cross Loss: 0.1483\n",
            "38/239, Train_loss: 0.1482764035463333 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [39/239], Dice Loss: 0.7795, Cross Loss: 0.2612\n",
            "39/239, Train_loss: 0.26119333505630493 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [40/239], Dice Loss: 0.8124, Cross Loss: 0.2612\n",
            "40/239, Train_loss: 0.26119330525398254 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [41/239], Dice Loss: 0.9891, Cross Loss: 0.1483\n",
            "41/239, Train_loss: 0.14827144145965576 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [42/239], Dice Loss: 0.5793, Cross Loss: 0.1483\n",
            "42/239, Train_loss: 0.14827200770378113 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [43/239], Dice Loss: 0.9452, Cross Loss: 0.1483\n",
            "43/239, Train_loss: 0.14827190339565277 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [44/239], Dice Loss: 0.6628, Cross Loss: 0.1483\n",
            "44/239, Train_loss: 0.14827030897140503 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [45/239], Dice Loss: 0.7629, Cross Loss: 0.2612\n",
            "45/239, Train_loss: 0.2612000107765198 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [46/239], Dice Loss: 0.9728, Cross Loss: 0.1483\n",
            "46/239, Train_loss: 0.14827001094818115 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [47/239], Dice Loss: 0.7900, Cross Loss: 0.1483\n",
            "47/239, Train_loss: 0.14826762676239014 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [48/239], Dice Loss: 0.8745, Cross Loss: 0.2612\n",
            "48/239, Train_loss: 0.2611919939517975 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [49/239], Dice Loss: 0.9012, Cross Loss: 0.1483\n",
            "49/239, Train_loss: 0.14827430248260498 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [50/239], Dice Loss: 0.8005, Cross Loss: 0.1483\n",
            "50/239, Train_loss: 0.14826995134353638 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [51/239], Dice Loss: 0.6076, Cross Loss: 0.1483\n",
            "51/239, Train_loss: 0.14826995134353638 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [52/239], Dice Loss: 0.8027, Cross Loss: 0.1483\n",
            "52/239, Train_loss: 0.14826953411102295 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [53/239], Dice Loss: 0.7976, Cross Loss: 0.1483\n",
            "53/239, Train_loss: 0.14826154708862305 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [54/239], Dice Loss: 0.7803, Cross Loss: 0.2612\n",
            "54/239, Train_loss: 0.2611762285232544 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [55/239], Dice Loss: 0.6085, Cross Loss: 0.1483\n",
            "55/239, Train_loss: 0.1482817530632019 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [56/239], Dice Loss: 0.9474, Cross Loss: 0.1483\n",
            "56/239, Train_loss: 0.14827996492385864 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [57/239], Dice Loss: 0.7272, Cross Loss: 0.2612\n",
            "57/239, Train_loss: 0.2612200975418091 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [58/239], Dice Loss: 0.9925, Cross Loss: 0.1483\n",
            "58/239, Train_loss: 0.1482582688331604 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [59/239], Dice Loss: 0.9711, Cross Loss: 0.1483\n",
            "59/239, Train_loss: 0.14825740456581116 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [60/239], Dice Loss: 0.6812, Cross Loss: 0.1483\n",
            "60/239, Train_loss: 0.14825987815856934 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [61/239], Dice Loss: 0.9835, Cross Loss: 0.1483\n",
            "61/239, Train_loss: 0.14825351536273956 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [62/239], Dice Loss: 0.6966, Cross Loss: 0.2613\n",
            "62/239, Train_loss: 0.2612566649913788 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [63/239], Dice Loss: 0.8736, Cross Loss: 0.1483\n",
            "63/239, Train_loss: 0.1482643336057663 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [64/239], Dice Loss: 0.7155, Cross Loss: 0.1483\n",
            "64/239, Train_loss: 0.1482548713684082 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [65/239], Dice Loss: 0.6725, Cross Loss: 0.1483\n",
            "65/239, Train_loss: 0.14825591444969177 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [66/239], Dice Loss: 0.6345, Cross Loss: 0.1483\n",
            "66/239, Train_loss: 0.14825654029846191 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [67/239], Dice Loss: 0.9821, Cross Loss: 0.1483\n",
            "67/239, Train_loss: 0.14825014770030975 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [68/239], Dice Loss: 0.7886, Cross Loss: 0.1483\n",
            "68/239, Train_loss: 0.14825046062469482 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [69/239], Dice Loss: 0.9891, Cross Loss: 0.2612\n",
            "69/239, Train_loss: 0.2612435221672058 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [70/239], Dice Loss: 0.9852, Cross Loss: 0.2612\n",
            "70/239, Train_loss: 0.26124200224876404 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [71/239], Dice Loss: 0.5667, Cross Loss: 0.1482\n",
            "71/239, Train_loss: 0.14824822545051575 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [72/239], Dice Loss: 0.9740, Cross Loss: 0.2613\n",
            "72/239, Train_loss: 0.2612858712673187 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [73/239], Dice Loss: 0.9823, Cross Loss: 0.1482\n",
            "73/239, Train_loss: 0.14824925363063812 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [74/239], Dice Loss: 0.8594, Cross Loss: 0.1482\n",
            "74/239, Train_loss: 0.1482487916946411 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [75/239], Dice Loss: 0.9752, Cross Loss: 0.1483\n",
            "75/239, Train_loss: 0.14825020730495453 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [76/239], Dice Loss: 0.8735, Cross Loss: 0.1482\n",
            "76/239, Train_loss: 0.14824606478214264 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [77/239], Dice Loss: 0.9943, Cross Loss: 0.1482\n",
            "77/239, Train_loss: 0.14824208617210388 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [78/239], Dice Loss: 0.9515, Cross Loss: 0.1482\n",
            "78/239, Train_loss: 0.14824333786964417 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [79/239], Dice Loss: 0.9867, Cross Loss: 0.1482\n",
            "79/239, Train_loss: 0.1482435166835785 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [80/239], Dice Loss: 0.8237, Cross Loss: 0.1482\n",
            "80/239, Train_loss: 0.14823931455612183 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [81/239], Dice Loss: 0.9937, Cross Loss: 0.1482\n",
            "81/239, Train_loss: 0.1482428014278412 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [82/239], Dice Loss: 0.5008, Cross Loss: 0.1482\n",
            "82/239, Train_loss: 0.1482386589050293 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [83/239], Dice Loss: 0.9301, Cross Loss: 0.1482\n",
            "83/239, Train_loss: 0.14823739230632782 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [84/239], Dice Loss: 0.9436, Cross Loss: 0.1482\n",
            "84/239, Train_loss: 0.14823594689369202 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [85/239], Dice Loss: 0.8032, Cross Loss: 0.2613\n",
            "85/239, Train_loss: 0.26127150654792786 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [86/239], Dice Loss: 0.7942, Cross Loss: 0.1482\n",
            "86/239, Train_loss: 0.14823377132415771 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [87/239], Dice Loss: 0.9278, Cross Loss: 0.1482\n",
            "87/239, Train_loss: 0.14823293685913086 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [88/239], Dice Loss: 0.9482, Cross Loss: 0.1482\n",
            "88/239, Train_loss: 0.1482306867837906 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [89/239], Dice Loss: 0.9756, Cross Loss: 0.1482\n",
            "89/239, Train_loss: 0.14822974801063538 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [90/239], Dice Loss: 0.8947, Cross Loss: 0.1482\n",
            "90/239, Train_loss: 0.1482287496328354 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [91/239], Dice Loss: 0.7345, Cross Loss: 0.2613\n",
            "91/239, Train_loss: 0.2612815499305725 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [92/239], Dice Loss: 0.9320, Cross Loss: 0.1482\n",
            "92/239, Train_loss: 0.14822593331336975 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [93/239], Dice Loss: 0.9055, Cross Loss: 0.1482\n",
            "93/239, Train_loss: 0.14822524785995483 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [94/239], Dice Loss: 0.8806, Cross Loss: 0.2613\n",
            "94/239, Train_loss: 0.26131361722946167 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [95/239], Dice Loss: 0.8628, Cross Loss: 0.1482\n",
            "95/239, Train_loss: 0.1482270509004593 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [96/239], Dice Loss: 0.9595, Cross Loss: 0.1482\n",
            "96/239, Train_loss: 0.14822575449943542 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [97/239], Dice Loss: 0.8881, Cross Loss: 0.1482\n",
            "97/239, Train_loss: 0.14822494983673096 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [98/239], Dice Loss: 0.8442, Cross Loss: 0.1482\n",
            "98/239, Train_loss: 0.14822128415107727 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [99/239], Dice Loss: 0.8975, Cross Loss: 0.2613\n",
            "99/239, Train_loss: 0.2612926959991455 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [100/239], Dice Loss: 0.9678, Cross Loss: 0.2613\n",
            "100/239, Train_loss: 0.26132526993751526 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [101/239], Dice Loss: 0.8168, Cross Loss: 0.2613\n",
            "101/239, Train_loss: 0.26131346821784973 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [102/239], Dice Loss: 0.8676, Cross Loss: 0.1482\n",
            "102/239, Train_loss: 0.14822262525558472 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [103/239], Dice Loss: 0.8634, Cross Loss: 0.1482\n",
            "103/239, Train_loss: 0.14822323620319366 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [104/239], Dice Loss: 0.8867, Cross Loss: 0.2613\n",
            "104/239, Train_loss: 0.2612904906272888 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [105/239], Dice Loss: 0.8094, Cross Loss: 0.1482\n",
            "105/239, Train_loss: 0.14822757244110107 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [106/239], Dice Loss: 0.9815, Cross Loss: 0.1482\n",
            "106/239, Train_loss: 0.14822779595851898 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [107/239], Dice Loss: 0.7758, Cross Loss: 0.1482\n",
            "107/239, Train_loss: 0.14822368323802948 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [108/239], Dice Loss: 0.8331, Cross Loss: 0.2613\n",
            "108/239, Train_loss: 0.2612980008125305 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [109/239], Dice Loss: 0.8774, Cross Loss: 0.1482\n",
            "109/239, Train_loss: 0.14822272956371307 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [110/239], Dice Loss: 0.9409, Cross Loss: 0.1482\n",
            "110/239, Train_loss: 0.14822262525558472 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [111/239], Dice Loss: 0.9529, Cross Loss: 0.1482\n",
            "111/239, Train_loss: 0.14822247624397278 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [112/239], Dice Loss: 0.7772, Cross Loss: 0.1482\n",
            "112/239, Train_loss: 0.1482221931219101 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [113/239], Dice Loss: 0.8858, Cross Loss: 0.1482\n",
            "113/239, Train_loss: 0.14822286367416382 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [114/239], Dice Loss: 0.7837, Cross Loss: 0.1482\n",
            "114/239, Train_loss: 0.14822247624397278 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [115/239], Dice Loss: 0.9506, Cross Loss: 0.1482\n",
            "115/239, Train_loss: 0.14822277426719666 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [116/239], Dice Loss: 0.9092, Cross Loss: 0.1482\n",
            "116/239, Train_loss: 0.14822211861610413 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [117/239], Dice Loss: 0.9290, Cross Loss: 0.1482\n",
            "117/239, Train_loss: 0.14821851253509521 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [118/239], Dice Loss: 0.9259, Cross Loss: 0.1482\n",
            "118/239, Train_loss: 0.1482178270816803 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [119/239], Dice Loss: 0.9027, Cross Loss: 0.1482\n",
            "119/239, Train_loss: 0.14821864664554596 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [120/239], Dice Loss: 0.9243, Cross Loss: 0.1482\n",
            "120/239, Train_loss: 0.14821791648864746 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [121/239], Dice Loss: 0.7834, Cross Loss: 0.1482\n",
            "121/239, Train_loss: 0.14821717143058777 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [122/239], Dice Loss: 0.7245, Cross Loss: 0.2613\n",
            "122/239, Train_loss: 0.2612893283367157 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [123/239], Dice Loss: 0.9182, Cross Loss: 0.1482\n",
            "123/239, Train_loss: 0.14822152256965637 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [124/239], Dice Loss: 0.6986, Cross Loss: 0.2613\n",
            "124/239, Train_loss: 0.26129722595214844 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [125/239], Dice Loss: 0.8723, Cross Loss: 0.2613\n",
            "125/239, Train_loss: 0.26133453845977783 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [126/239], Dice Loss: 0.8868, Cross Loss: 0.1482\n",
            "126/239, Train_loss: 0.14821507036685944 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [127/239], Dice Loss: 0.8933, Cross Loss: 0.1482\n",
            "127/239, Train_loss: 0.14821356534957886 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [128/239], Dice Loss: 0.9061, Cross Loss: 0.2613\n",
            "128/239, Train_loss: 0.26130902767181396 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [129/239], Dice Loss: 0.8333, Cross Loss: 0.1482\n",
            "129/239, Train_loss: 0.14821353554725647 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [130/239], Dice Loss: 0.7169, Cross Loss: 0.2613\n",
            "130/239, Train_loss: 0.26131299138069153 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [131/239], Dice Loss: 0.8472, Cross Loss: 0.1482\n",
            "131/239, Train_loss: 0.14821556210517883 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [132/239], Dice Loss: 0.9982, Cross Loss: 0.1482\n",
            "132/239, Train_loss: 0.14821431040763855 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [133/239], Dice Loss: 0.7248, Cross Loss: 0.1482\n",
            "133/239, Train_loss: 0.1482141762971878 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [134/239], Dice Loss: 0.8352, Cross Loss: 0.2613\n",
            "134/239, Train_loss: 0.26131686568260193 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [135/239], Dice Loss: 0.6443, Cross Loss: 0.1482\n",
            "135/239, Train_loss: 0.14821425080299377 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [136/239], Dice Loss: 0.9974, Cross Loss: 0.2613\n",
            "136/239, Train_loss: 0.2613145709037781 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [137/239], Dice Loss: 0.5685, Cross Loss: 0.2613\n",
            "137/239, Train_loss: 0.26133808493614197 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [138/239], Dice Loss: 0.8826, Cross Loss: 0.1482\n",
            "138/239, Train_loss: 0.14821617305278778 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [139/239], Dice Loss: 0.8606, Cross Loss: 0.2613\n",
            "139/239, Train_loss: 0.26131361722946167 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [140/239], Dice Loss: 0.9600, Cross Loss: 0.1482\n",
            "140/239, Train_loss: 0.14821526408195496 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [141/239], Dice Loss: 0.8308, Cross Loss: 0.2613\n",
            "141/239, Train_loss: 0.2613094747066498 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [142/239], Dice Loss: 0.9039, Cross Loss: 0.1482\n",
            "142/239, Train_loss: 0.14821550250053406 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [143/239], Dice Loss: 0.9709, Cross Loss: 0.2613\n",
            "143/239, Train_loss: 0.2613333463668823 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [144/239], Dice Loss: 0.9070, Cross Loss: 0.1482\n",
            "144/239, Train_loss: 0.14821641147136688 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [145/239], Dice Loss: 0.7009, Cross Loss: 0.1482\n",
            "145/239, Train_loss: 0.14821703732013702 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [146/239], Dice Loss: 0.8468, Cross Loss: 0.1482\n",
            "146/239, Train_loss: 0.14821727573871613 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [147/239], Dice Loss: 0.9207, Cross Loss: 0.1482\n",
            "147/239, Train_loss: 0.1482165902853012 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [148/239], Dice Loss: 0.9359, Cross Loss: 0.1482\n",
            "148/239, Train_loss: 0.14821667969226837 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [149/239], Dice Loss: 0.5693, Cross Loss: 0.2613\n",
            "149/239, Train_loss: 0.26131319999694824 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [150/239], Dice Loss: 0.9906, Cross Loss: 0.1482\n",
            "150/239, Train_loss: 0.14821605384349823 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [151/239], Dice Loss: 0.7088, Cross Loss: 0.2613\n",
            "151/239, Train_loss: 0.2613092064857483 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [152/239], Dice Loss: 0.9558, Cross Loss: 0.1482\n",
            "152/239, Train_loss: 0.1482168734073639 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [153/239], Dice Loss: 0.8932, Cross Loss: 0.1482\n",
            "153/239, Train_loss: 0.14821544289588928 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [154/239], Dice Loss: 0.7095, Cross Loss: 0.2613\n",
            "154/239, Train_loss: 0.2613191306591034 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [155/239], Dice Loss: 0.8649, Cross Loss: 0.1482\n",
            "155/239, Train_loss: 0.14822602272033691 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [156/239], Dice Loss: 0.8146, Cross Loss: 0.2612\n",
            "156/239, Train_loss: 0.2612296938896179 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [157/239], Dice Loss: 0.8344, Cross Loss: 0.2613\n",
            "157/239, Train_loss: 0.2612699270248413 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [158/239], Dice Loss: 0.7179, Cross Loss: 0.2613\n",
            "158/239, Train_loss: 0.2612583041191101 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [159/239], Dice Loss: 0.9306, Cross Loss: 0.1482\n",
            "159/239, Train_loss: 0.1482412964105606 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [160/239], Dice Loss: 0.8094, Cross Loss: 0.2613\n",
            "160/239, Train_loss: 0.2612997591495514 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [161/239], Dice Loss: 0.8487, Cross Loss: 0.1482\n",
            "161/239, Train_loss: 0.14822246134281158 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [162/239], Dice Loss: 0.9254, Cross Loss: 0.2613\n",
            "162/239, Train_loss: 0.2613012194633484 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [163/239], Dice Loss: 0.9927, Cross Loss: 0.1482\n",
            "163/239, Train_loss: 0.14822348952293396 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [164/239], Dice Loss: 0.9323, Cross Loss: 0.1482\n",
            "164/239, Train_loss: 0.1482243537902832 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [165/239], Dice Loss: 0.8213, Cross Loss: 0.2613\n",
            "165/239, Train_loss: 0.2612989842891693 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [166/239], Dice Loss: 0.9884, Cross Loss: 0.2613\n",
            "166/239, Train_loss: 0.26129183173179626 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [167/239], Dice Loss: 0.8757, Cross Loss: 0.2613\n",
            "167/239, Train_loss: 0.2612870931625366 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [168/239], Dice Loss: 0.7079, Cross Loss: 0.2613\n",
            "168/239, Train_loss: 0.26128312945365906 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [169/239], Dice Loss: 0.7366, Cross Loss: 0.2613\n",
            "169/239, Train_loss: 0.26130324602127075 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [170/239], Dice Loss: 0.9403, Cross Loss: 0.2613\n",
            "170/239, Train_loss: 0.2612836956977844 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [171/239], Dice Loss: 0.8763, Cross Loss: 0.2612\n",
            "171/239, Train_loss: 0.26123151183128357 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [172/239], Dice Loss: 0.8561, Cross Loss: 0.1482\n",
            "172/239, Train_loss: 0.14824415743350983 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [173/239], Dice Loss: 0.8840, Cross Loss: 0.2613\n",
            "173/239, Train_loss: 0.2612961530685425 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [174/239], Dice Loss: 0.6624, Cross Loss: 0.2612\n",
            "174/239, Train_loss: 0.2612268924713135 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [175/239], Dice Loss: 0.9266, Cross Loss: 0.2612\n",
            "175/239, Train_loss: 0.2612040638923645 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [176/239], Dice Loss: 0.9104, Cross Loss: 0.1483\n",
            "176/239, Train_loss: 0.148271381855011 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [177/239], Dice Loss: 0.6609, Cross Loss: 0.1483\n",
            "177/239, Train_loss: 0.14827725291252136 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [178/239], Dice Loss: 0.8540, Cross Loss: 0.2612\n",
            "178/239, Train_loss: 0.2611784338951111 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [179/239], Dice Loss: 0.9300, Cross Loss: 0.1483\n",
            "179/239, Train_loss: 0.14826494455337524 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [180/239], Dice Loss: 0.7979, Cross Loss: 0.1483\n",
            "180/239, Train_loss: 0.14826880395412445 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [181/239], Dice Loss: 0.9131, Cross Loss: 0.2612\n",
            "181/239, Train_loss: 0.26122790575027466 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [182/239], Dice Loss: 0.7282, Cross Loss: 0.1483\n",
            "182/239, Train_loss: 0.14829370379447937 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [183/239], Dice Loss: 0.9345, Cross Loss: 0.2611\n",
            "183/239, Train_loss: 0.26111894845962524 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [184/239], Dice Loss: 0.9101, Cross Loss: 0.1483\n",
            "184/239, Train_loss: 0.14830739796161652 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [185/239], Dice Loss: 0.8602, Cross Loss: 0.1483\n",
            "185/239, Train_loss: 0.14831355214118958 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [186/239], Dice Loss: 0.7906, Cross Loss: 0.1483\n",
            "186/239, Train_loss: 0.14832672476768494 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [187/239], Dice Loss: 0.9646, Cross Loss: 0.1483\n",
            "187/239, Train_loss: 0.14832614362239838 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [188/239], Dice Loss: 0.7779, Cross Loss: 0.2612\n",
            "188/239, Train_loss: 0.2611531913280487 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [189/239], Dice Loss: 0.7158, Cross Loss: 0.1483\n",
            "189/239, Train_loss: 0.14830026030540466 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [190/239], Dice Loss: 0.9562, Cross Loss: 0.1483\n",
            "190/239, Train_loss: 0.14828737080097198 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [191/239], Dice Loss: 0.7766, Cross Loss: 0.2611\n",
            "191/239, Train_loss: 0.261109322309494 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [192/239], Dice Loss: 0.9327, Cross Loss: 0.1483\n",
            "192/239, Train_loss: 0.1483110785484314 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [193/239], Dice Loss: 0.9781, Cross Loss: 0.2611\n",
            "193/239, Train_loss: 0.2611258029937744 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [194/239], Dice Loss: 0.9968, Cross Loss: 0.1483\n",
            "194/239, Train_loss: 0.14831411838531494 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [195/239], Dice Loss: 0.9822, Cross Loss: 0.1483\n",
            "195/239, Train_loss: 0.14831587672233582 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [196/239], Dice Loss: 0.9978, Cross Loss: 0.2611\n",
            "196/239, Train_loss: 0.26111361384391785 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [197/239], Dice Loss: 0.9985, Cross Loss: 0.1483\n",
            "197/239, Train_loss: 0.1483253836631775 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [198/239], Dice Loss: 0.9984, Cross Loss: 0.1483\n",
            "198/239, Train_loss: 0.1483285129070282 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [199/239], Dice Loss: 0.8405, Cross Loss: 0.1483\n",
            "199/239, Train_loss: 0.14829273521900177 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [200/239], Dice Loss: 0.7799, Cross Loss: 0.1483\n",
            "200/239, Train_loss: 0.1482900083065033 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [201/239], Dice Loss: 0.8353, Cross Loss: 0.2612\n",
            "201/239, Train_loss: 0.2611757516860962 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [202/239], Dice Loss: 0.6878, Cross Loss: 0.2610\n",
            "202/239, Train_loss: 0.2610044479370117 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [203/239], Dice Loss: 0.8338, Cross Loss: 0.1484\n",
            "203/239, Train_loss: 0.14844214916229248 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [204/239], Dice Loss: 0.9930, Cross Loss: 0.1484\n",
            "204/239, Train_loss: 0.14843249320983887 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [205/239], Dice Loss: 0.7642, Cross Loss: 0.2610\n",
            "205/239, Train_loss: 0.2609942555427551 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [206/239], Dice Loss: 0.8978, Cross Loss: 0.1483\n",
            "206/239, Train_loss: 0.14827756583690643 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [207/239], Dice Loss: 0.8027, Cross Loss: 0.1483\n",
            "207/239, Train_loss: 0.14832627773284912 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [208/239], Dice Loss: 0.8297, Cross Loss: 0.1483\n",
            "208/239, Train_loss: 0.1483287811279297 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [209/239], Dice Loss: 0.8894, Cross Loss: 0.2611\n",
            "209/239, Train_loss: 0.26110514998435974 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [210/239], Dice Loss: 0.9996, Cross Loss: 0.1483\n",
            "210/239, Train_loss: 0.14830923080444336 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [211/239], Dice Loss: 0.9017, Cross Loss: 0.1483\n",
            "211/239, Train_loss: 0.14830254018306732 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [212/239], Dice Loss: 0.8841, Cross Loss: 0.1483\n",
            "212/239, Train_loss: 0.1482999324798584 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [213/239], Dice Loss: 0.6691, Cross Loss: 0.2607\n",
            "213/239, Train_loss: 0.2607191801071167 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [214/239], Dice Loss: 0.7085, Cross Loss: 0.2604\n",
            "214/239, Train_loss: 0.260398805141449 Train_dice: tensor(0.2604, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [215/239], Dice Loss: 0.6335, Cross Loss: 0.1484\n",
            "215/239, Train_loss: 0.14835450053215027 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [216/239], Dice Loss: 0.9145, Cross Loss: 0.1484\n",
            "216/239, Train_loss: 0.1484423726797104 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [217/239], Dice Loss: 0.6653, Cross Loss: 0.2611\n",
            "217/239, Train_loss: 0.26109474897384644 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [218/239], Dice Loss: 0.6558, Cross Loss: 0.2609\n",
            "218/239, Train_loss: 0.260873407125473 Train_dice: tensor(0.2609, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [219/239], Dice Loss: 0.8928, Cross Loss: 0.1485\n",
            "219/239, Train_loss: 0.1485312432050705 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [220/239], Dice Loss: 0.9885, Cross Loss: 0.1484\n",
            "220/239, Train_loss: 0.14838236570358276 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [221/239], Dice Loss: 0.9870, Cross Loss: 0.1511\n",
            "221/239, Train_loss: 0.1510554850101471 Train_dice: tensor(0.1511, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [222/239], Dice Loss: 0.9976, Cross Loss: 0.1494\n",
            "222/239, Train_loss: 0.14936888217926025 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [223/239], Dice Loss: 0.9890, Cross Loss: 0.1492\n",
            "223/239, Train_loss: 0.14915969967842102 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [224/239], Dice Loss: 0.5986, Cross Loss: 0.1489\n",
            "224/239, Train_loss: 0.14888222515583038 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [225/239], Dice Loss: 0.9651, Cross Loss: 0.1489\n",
            "225/239, Train_loss: 0.1489284187555313 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [226/239], Dice Loss: 0.9980, Cross Loss: 0.1488\n",
            "226/239, Train_loss: 0.14879751205444336 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [227/239], Dice Loss: 0.9999, Cross Loss: 0.1486\n",
            "227/239, Train_loss: 0.14858028292655945 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [228/239], Dice Loss: 0.9965, Cross Loss: 0.1485\n",
            "228/239, Train_loss: 0.14847850799560547 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [229/239], Dice Loss: 0.9968, Cross Loss: 0.1493\n",
            "229/239, Train_loss: 0.14932745695114136 Train_dice: tensor(0.1493, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [230/239], Dice Loss: 0.9934, Cross Loss: 0.1489\n",
            "230/239, Train_loss: 0.14891937375068665 Train_dice: tensor(0.1489, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [231/239], Dice Loss: 0.9916, Cross Loss: 0.1486\n",
            "231/239, Train_loss: 0.14862695336341858 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [232/239], Dice Loss: 0.9913, Cross Loss: 0.2611\n",
            "232/239, Train_loss: 0.26109206676483154 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [233/239], Dice Loss: 0.8223, Cross Loss: 0.1483\n",
            "233/239, Train_loss: 0.14827272295951843 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [234/239], Dice Loss: 0.9992, Cross Loss: 0.1483\n",
            "234/239, Train_loss: 0.14825771749019623 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [235/239], Dice Loss: 0.6359, Cross Loss: 0.2613\n",
            "235/239, Train_loss: 0.261309951543808 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [236/239], Dice Loss: 0.9953, Cross Loss: 0.2613\n",
            "236/239, Train_loss: 0.26130035519599915 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [237/239], Dice Loss: 1.0000, Cross Loss: 0.1482\n",
            "237/239, Train_loss: 0.14822418987751007 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [238/239], Dice Loss: 0.9287, Cross Loss: 0.1482\n",
            "238/239, Train_loss: 0.148229718208313 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [3/100], Batch [239/239], Dice Loss: 0.9992, Cross Loss: 0.2613\n",
            "239/239, Train_loss: 0.2612849473953247 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1861\n",
            "Epoch_metric: tensor(0.1861, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.1860\n",
            "test_dice_epoch: tensor(0.1860, device='cuda:0')\n",
            "current epoch: 3 current mean dice: tensor(0.1860, device='cuda:0')\n",
            "best mean dice: tensor(0.1865, device='cuda:0') at epoch: 1\n",
            "----------\n",
            "epoch 4/100\n",
            "Epoch [4/100], Batch [1/239], Dice Loss: 0.9707, Cross Loss: 0.2613\n",
            "1/239, Train_loss: 0.2612929344177246 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [2/239], Dice Loss: 0.8609, Cross Loss: 0.2613\n",
            "2/239, Train_loss: 0.26129090785980225 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [3/239], Dice Loss: 0.9703, Cross Loss: 0.2613\n",
            "3/239, Train_loss: 0.2612883746623993 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [4/239], Dice Loss: 0.9562, Cross Loss: 0.2613\n",
            "4/239, Train_loss: 0.2612758278846741 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [5/239], Dice Loss: 0.8753, Cross Loss: 0.1482\n",
            "5/239, Train_loss: 0.1482175588607788 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [6/239], Dice Loss: 0.9152, Cross Loss: 0.1482\n",
            "6/239, Train_loss: 0.14821942150592804 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [7/239], Dice Loss: 0.5560, Cross Loss: 0.2613\n",
            "7/239, Train_loss: 0.2613041400909424 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [8/239], Dice Loss: 0.9937, Cross Loss: 0.2613\n",
            "8/239, Train_loss: 0.2612801790237427 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [9/239], Dice Loss: 0.9937, Cross Loss: 0.2613\n",
            "9/239, Train_loss: 0.261303186416626 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [10/239], Dice Loss: 0.8505, Cross Loss: 0.1482\n",
            "10/239, Train_loss: 0.148233100771904 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [11/239], Dice Loss: 0.7896, Cross Loss: 0.1482\n",
            "11/239, Train_loss: 0.1482340395450592 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [12/239], Dice Loss: 0.9153, Cross Loss: 0.1482\n",
            "12/239, Train_loss: 0.14824511110782623 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [13/239], Dice Loss: 0.9270, Cross Loss: 0.1482\n",
            "13/239, Train_loss: 0.14823637902736664 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [14/239], Dice Loss: 0.7250, Cross Loss: 0.1483\n",
            "14/239, Train_loss: 0.14825592935085297 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [15/239], Dice Loss: 0.7696, Cross Loss: 0.1482\n",
            "15/239, Train_loss: 0.14824925363063812 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [16/239], Dice Loss: 0.9376, Cross Loss: 0.1483\n",
            "16/239, Train_loss: 0.14825090765953064 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [17/239], Dice Loss: 0.6313, Cross Loss: 0.1482\n",
            "17/239, Train_loss: 0.14824670553207397 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [18/239], Dice Loss: 0.9406, Cross Loss: 0.1482\n",
            "18/239, Train_loss: 0.14822983741760254 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [19/239], Dice Loss: 0.6652, Cross Loss: 0.2613\n",
            "19/239, Train_loss: 0.261279433965683 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [20/239], Dice Loss: 0.6228, Cross Loss: 0.2613\n",
            "20/239, Train_loss: 0.2613254189491272 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [21/239], Dice Loss: 0.8605, Cross Loss: 0.2613\n",
            "21/239, Train_loss: 0.2612881064414978 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [22/239], Dice Loss: 0.9119, Cross Loss: 0.1482\n",
            "22/239, Train_loss: 0.1482362151145935 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [23/239], Dice Loss: 0.6738, Cross Loss: 0.2613\n",
            "23/239, Train_loss: 0.26131248474121094 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [24/239], Dice Loss: 0.8444, Cross Loss: 0.2613\n",
            "24/239, Train_loss: 0.26128801703453064 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [25/239], Dice Loss: 0.9729, Cross Loss: 0.1482\n",
            "25/239, Train_loss: 0.1482241153717041 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [26/239], Dice Loss: 0.9552, Cross Loss: 0.1482\n",
            "26/239, Train_loss: 0.14822079241275787 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [27/239], Dice Loss: 0.9427, Cross Loss: 0.1482\n",
            "27/239, Train_loss: 0.14822086691856384 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [28/239], Dice Loss: 0.7315, Cross Loss: 0.1482\n",
            "28/239, Train_loss: 0.14821940660476685 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [29/239], Dice Loss: 0.8712, Cross Loss: 0.1482\n",
            "29/239, Train_loss: 0.14822697639465332 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [30/239], Dice Loss: 0.9839, Cross Loss: 0.1482\n",
            "30/239, Train_loss: 0.1482260823249817 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [31/239], Dice Loss: 0.7293, Cross Loss: 0.1482\n",
            "31/239, Train_loss: 0.14822375774383545 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [32/239], Dice Loss: 0.8946, Cross Loss: 0.1482\n",
            "32/239, Train_loss: 0.14821955561637878 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [33/239], Dice Loss: 0.4845, Cross Loss: 0.2613\n",
            "33/239, Train_loss: 0.26132017374038696 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [34/239], Dice Loss: 0.9969, Cross Loss: 0.1482\n",
            "34/239, Train_loss: 0.14821980893611908 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [35/239], Dice Loss: 0.9100, Cross Loss: 0.2613\n",
            "35/239, Train_loss: 0.26130014657974243 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [36/239], Dice Loss: 0.9092, Cross Loss: 0.1482\n",
            "36/239, Train_loss: 0.1482229232788086 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [37/239], Dice Loss: 0.9084, Cross Loss: 0.1482\n",
            "37/239, Train_loss: 0.1482158899307251 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [38/239], Dice Loss: 0.8356, Cross Loss: 0.1482\n",
            "38/239, Train_loss: 0.14821942150592804 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [39/239], Dice Loss: 0.6677, Cross Loss: 0.2613\n",
            "39/239, Train_loss: 0.26129987835884094 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [40/239], Dice Loss: 0.7255, Cross Loss: 0.2613\n",
            "40/239, Train_loss: 0.26129403710365295 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [41/239], Dice Loss: 0.9652, Cross Loss: 0.1482\n",
            "41/239, Train_loss: 0.148220032453537 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [42/239], Dice Loss: 0.5784, Cross Loss: 0.1482\n",
            "42/239, Train_loss: 0.1482229232788086 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [43/239], Dice Loss: 0.9444, Cross Loss: 0.1482\n",
            "43/239, Train_loss: 0.14822286367416382 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [44/239], Dice Loss: 0.6670, Cross Loss: 0.1482\n",
            "44/239, Train_loss: 0.14821851253509521 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [45/239], Dice Loss: 0.7472, Cross Loss: 0.2613\n",
            "45/239, Train_loss: 0.261318176984787 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [46/239], Dice Loss: 0.9708, Cross Loss: 0.1482\n",
            "46/239, Train_loss: 0.1482142060995102 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [47/239], Dice Loss: 0.7455, Cross Loss: 0.1482\n",
            "47/239, Train_loss: 0.14821842312812805 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [48/239], Dice Loss: 0.8942, Cross Loss: 0.2613\n",
            "48/239, Train_loss: 0.26129505038261414 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [49/239], Dice Loss: 0.8516, Cross Loss: 0.1482\n",
            "49/239, Train_loss: 0.14822593331336975 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [50/239], Dice Loss: 0.8453, Cross Loss: 0.1482\n",
            "50/239, Train_loss: 0.14821821451187134 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [51/239], Dice Loss: 0.6047, Cross Loss: 0.1482\n",
            "51/239, Train_loss: 0.14821794629096985 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [52/239], Dice Loss: 0.7702, Cross Loss: 0.1482\n",
            "52/239, Train_loss: 0.14821749925613403 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [53/239], Dice Loss: 0.8254, Cross Loss: 0.1482\n",
            "53/239, Train_loss: 0.14821869134902954 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [54/239], Dice Loss: 0.7495, Cross Loss: 0.2613\n",
            "54/239, Train_loss: 0.2613070607185364 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [55/239], Dice Loss: 0.5361, Cross Loss: 0.1482\n",
            "55/239, Train_loss: 0.14821961522102356 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [56/239], Dice Loss: 0.9396, Cross Loss: 0.1482\n",
            "56/239, Train_loss: 0.14821946620941162 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [57/239], Dice Loss: 0.7262, Cross Loss: 0.2613\n",
            "57/239, Train_loss: 0.26131224632263184 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [58/239], Dice Loss: 0.9641, Cross Loss: 0.1482\n",
            "58/239, Train_loss: 0.14821146428585052 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [59/239], Dice Loss: 0.9676, Cross Loss: 0.1482\n",
            "59/239, Train_loss: 0.14821277558803558 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [60/239], Dice Loss: 0.7218, Cross Loss: 0.1482\n",
            "60/239, Train_loss: 0.14821039140224457 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [61/239], Dice Loss: 0.9497, Cross Loss: 0.1482\n",
            "61/239, Train_loss: 0.14820942282676697 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [62/239], Dice Loss: 0.6837, Cross Loss: 0.2613\n",
            "62/239, Train_loss: 0.2613232731819153 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [63/239], Dice Loss: 0.9080, Cross Loss: 0.1482\n",
            "63/239, Train_loss: 0.1482231765985489 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [64/239], Dice Loss: 0.8241, Cross Loss: 0.1482\n",
            "64/239, Train_loss: 0.14821583032608032 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [65/239], Dice Loss: 0.6472, Cross Loss: 0.1482\n",
            "65/239, Train_loss: 0.14821335673332214 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [66/239], Dice Loss: 0.6423, Cross Loss: 0.1482\n",
            "66/239, Train_loss: 0.14821965992450714 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [67/239], Dice Loss: 0.9360, Cross Loss: 0.1482\n",
            "67/239, Train_loss: 0.14821003377437592 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [68/239], Dice Loss: 0.7744, Cross Loss: 0.1482\n",
            "68/239, Train_loss: 0.1482226848602295 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [69/239], Dice Loss: 0.9903, Cross Loss: 0.2613\n",
            "69/239, Train_loss: 0.26131466031074524 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [70/239], Dice Loss: 0.9612, Cross Loss: 0.2613\n",
            "70/239, Train_loss: 0.26132625341415405 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [71/239], Dice Loss: 0.5442, Cross Loss: 0.1482\n",
            "71/239, Train_loss: 0.14820975065231323 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [72/239], Dice Loss: 0.9468, Cross Loss: 0.2613\n",
            "72/239, Train_loss: 0.26133501529693604 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [73/239], Dice Loss: 0.9622, Cross Loss: 0.1482\n",
            "73/239, Train_loss: 0.1482125073671341 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [74/239], Dice Loss: 0.8784, Cross Loss: 0.1482\n",
            "74/239, Train_loss: 0.1482124775648117 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [75/239], Dice Loss: 0.9481, Cross Loss: 0.1482\n",
            "75/239, Train_loss: 0.14820757508277893 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [76/239], Dice Loss: 0.8478, Cross Loss: 0.1482\n",
            "76/239, Train_loss: 0.14821109175682068 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [77/239], Dice Loss: 0.9896, Cross Loss: 0.1482\n",
            "77/239, Train_loss: 0.14820635318756104 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [78/239], Dice Loss: 0.9460, Cross Loss: 0.1482\n",
            "78/239, Train_loss: 0.14820997416973114 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [79/239], Dice Loss: 0.9783, Cross Loss: 0.1482\n",
            "79/239, Train_loss: 0.1482204645872116 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [80/239], Dice Loss: 0.7733, Cross Loss: 0.1482\n",
            "80/239, Train_loss: 0.14820557832717896 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [81/239], Dice Loss: 0.9908, Cross Loss: 0.1482\n",
            "81/239, Train_loss: 0.14820735156536102 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [82/239], Dice Loss: 0.4378, Cross Loss: 0.1482\n",
            "82/239, Train_loss: 0.14821195602416992 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [83/239], Dice Loss: 0.9227, Cross Loss: 0.1482\n",
            "83/239, Train_loss: 0.14821118116378784 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [84/239], Dice Loss: 0.9355, Cross Loss: 0.1482\n",
            "84/239, Train_loss: 0.14820654690265656 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [85/239], Dice Loss: 0.7857, Cross Loss: 0.2613\n",
            "85/239, Train_loss: 0.2613295614719391 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [86/239], Dice Loss: 0.7937, Cross Loss: 0.1482\n",
            "86/239, Train_loss: 0.14820793271064758 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [87/239], Dice Loss: 0.9216, Cross Loss: 0.1482\n",
            "87/239, Train_loss: 0.148207426071167 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [88/239], Dice Loss: 0.9385, Cross Loss: 0.1482\n",
            "88/239, Train_loss: 0.14820483326911926 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [89/239], Dice Loss: 0.9806, Cross Loss: 0.1482\n",
            "89/239, Train_loss: 0.14820435643196106 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [90/239], Dice Loss: 0.8722, Cross Loss: 0.1482\n",
            "90/239, Train_loss: 0.14820387959480286 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [91/239], Dice Loss: 0.7084, Cross Loss: 0.2613\n",
            "91/239, Train_loss: 0.2613229751586914 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [92/239], Dice Loss: 0.9109, Cross Loss: 0.1482\n",
            "92/239, Train_loss: 0.14820155501365662 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [93/239], Dice Loss: 0.8667, Cross Loss: 0.1482\n",
            "93/239, Train_loss: 0.1482013761997223 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [94/239], Dice Loss: 0.8462, Cross Loss: 0.2613\n",
            "94/239, Train_loss: 0.2613460123538971 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [95/239], Dice Loss: 0.8651, Cross Loss: 0.1482\n",
            "95/239, Train_loss: 0.1482025384902954 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [96/239], Dice Loss: 0.9683, Cross Loss: 0.1482\n",
            "96/239, Train_loss: 0.14820247888565063 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [97/239], Dice Loss: 0.8930, Cross Loss: 0.1482\n",
            "97/239, Train_loss: 0.14820383489131927 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [98/239], Dice Loss: 0.8456, Cross Loss: 0.1482\n",
            "98/239, Train_loss: 0.14820191264152527 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [99/239], Dice Loss: 0.8955, Cross Loss: 0.2613\n",
            "99/239, Train_loss: 0.2613329589366913 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [100/239], Dice Loss: 0.9694, Cross Loss: 0.2614\n",
            "100/239, Train_loss: 0.2613540589809418 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [101/239], Dice Loss: 0.7504, Cross Loss: 0.2614\n",
            "101/239, Train_loss: 0.2613523304462433 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [102/239], Dice Loss: 0.8288, Cross Loss: 0.1482\n",
            "102/239, Train_loss: 0.14820092916488647 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [103/239], Dice Loss: 0.9374, Cross Loss: 0.1482\n",
            "103/239, Train_loss: 0.14820444583892822 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [104/239], Dice Loss: 0.8296, Cross Loss: 0.2613\n",
            "104/239, Train_loss: 0.2613410949707031 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [105/239], Dice Loss: 0.8061, Cross Loss: 0.1482\n",
            "105/239, Train_loss: 0.14821070432662964 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [106/239], Dice Loss: 0.9555, Cross Loss: 0.1482\n",
            "106/239, Train_loss: 0.14821089804172516 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [107/239], Dice Loss: 0.7837, Cross Loss: 0.1482\n",
            "107/239, Train_loss: 0.14820508658885956 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [108/239], Dice Loss: 0.8146, Cross Loss: 0.2613\n",
            "108/239, Train_loss: 0.26133406162261963 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [109/239], Dice Loss: 0.8197, Cross Loss: 0.1482\n",
            "109/239, Train_loss: 0.14820343255996704 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [110/239], Dice Loss: 0.9323, Cross Loss: 0.1482\n",
            "110/239, Train_loss: 0.14820361137390137 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [111/239], Dice Loss: 0.9298, Cross Loss: 0.1482\n",
            "111/239, Train_loss: 0.14820381999015808 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [112/239], Dice Loss: 0.8297, Cross Loss: 0.1482\n",
            "112/239, Train_loss: 0.1482035517692566 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [113/239], Dice Loss: 0.8338, Cross Loss: 0.1482\n",
            "113/239, Train_loss: 0.1482006311416626 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [114/239], Dice Loss: 0.7519, Cross Loss: 0.1482\n",
            "114/239, Train_loss: 0.14820055663585663 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [115/239], Dice Loss: 0.9385, Cross Loss: 0.1482\n",
            "115/239, Train_loss: 0.1482030600309372 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [116/239], Dice Loss: 0.9063, Cross Loss: 0.1482\n",
            "116/239, Train_loss: 0.14820268750190735 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [117/239], Dice Loss: 0.9083, Cross Loss: 0.1482\n",
            "117/239, Train_loss: 0.14819937944412231 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [118/239], Dice Loss: 0.9035, Cross Loss: 0.1482\n",
            "118/239, Train_loss: 0.14819912612438202 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [119/239], Dice Loss: 0.9175, Cross Loss: 0.1482\n",
            "119/239, Train_loss: 0.14819982647895813 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [120/239], Dice Loss: 0.9245, Cross Loss: 0.1482\n",
            "120/239, Train_loss: 0.14819873869419098 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [121/239], Dice Loss: 0.7921, Cross Loss: 0.1482\n",
            "121/239, Train_loss: 0.1481996327638626 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [122/239], Dice Loss: 0.7035, Cross Loss: 0.2613\n",
            "122/239, Train_loss: 0.2613477110862732 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [123/239], Dice Loss: 0.9123, Cross Loss: 0.1482\n",
            "123/239, Train_loss: 0.1482047587633133 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [124/239], Dice Loss: 0.5864, Cross Loss: 0.2614\n",
            "124/239, Train_loss: 0.26135170459747314 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [125/239], Dice Loss: 0.8338, Cross Loss: 0.2614\n",
            "125/239, Train_loss: 0.26135802268981934 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [126/239], Dice Loss: 0.9160, Cross Loss: 0.1482\n",
            "126/239, Train_loss: 0.14819951355457306 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [127/239], Dice Loss: 0.9393, Cross Loss: 0.1482\n",
            "127/239, Train_loss: 0.14819779992103577 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [128/239], Dice Loss: 0.8306, Cross Loss: 0.2613\n",
            "128/239, Train_loss: 0.26134270429611206 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [129/239], Dice Loss: 0.7577, Cross Loss: 0.1482\n",
            "129/239, Train_loss: 0.14819739758968353 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [130/239], Dice Loss: 0.6207, Cross Loss: 0.2613\n",
            "130/239, Train_loss: 0.2613461911678314 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [131/239], Dice Loss: 0.8558, Cross Loss: 0.1482\n",
            "131/239, Train_loss: 0.14820027351379395 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [132/239], Dice Loss: 0.9987, Cross Loss: 0.1482\n",
            "132/239, Train_loss: 0.1481986939907074 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [133/239], Dice Loss: 0.7162, Cross Loss: 0.1482\n",
            "133/239, Train_loss: 0.1481989026069641 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [134/239], Dice Loss: 0.8680, Cross Loss: 0.2613\n",
            "134/239, Train_loss: 0.2613475024700165 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [135/239], Dice Loss: 0.6058, Cross Loss: 0.1482\n",
            "135/239, Train_loss: 0.14819882810115814 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [136/239], Dice Loss: 0.9974, Cross Loss: 0.2613\n",
            "136/239, Train_loss: 0.26134559512138367 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [137/239], Dice Loss: 0.5304, Cross Loss: 0.2614\n",
            "137/239, Train_loss: 0.26135799288749695 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [138/239], Dice Loss: 0.8620, Cross Loss: 0.1482\n",
            "138/239, Train_loss: 0.14820128679275513 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [139/239], Dice Loss: 0.8249, Cross Loss: 0.2613\n",
            "139/239, Train_loss: 0.2613428235054016 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [140/239], Dice Loss: 0.9915, Cross Loss: 0.1482\n",
            "140/239, Train_loss: 0.14820225536823273 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [141/239], Dice Loss: 0.8810, Cross Loss: 0.2613\n",
            "141/239, Train_loss: 0.2613447308540344 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [142/239], Dice Loss: 0.8824, Cross Loss: 0.1482\n",
            "142/239, Train_loss: 0.1482025682926178 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [143/239], Dice Loss: 0.9906, Cross Loss: 0.2614\n",
            "143/239, Train_loss: 0.2613533139228821 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [144/239], Dice Loss: 0.8880, Cross Loss: 0.1482\n",
            "144/239, Train_loss: 0.1481996476650238 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [145/239], Dice Loss: 0.6391, Cross Loss: 0.1482\n",
            "145/239, Train_loss: 0.14820131659507751 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [146/239], Dice Loss: 0.8931, Cross Loss: 0.1482\n",
            "146/239, Train_loss: 0.14820125699043274 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [147/239], Dice Loss: 0.9272, Cross Loss: 0.1482\n",
            "147/239, Train_loss: 0.1481994390487671 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [148/239], Dice Loss: 0.9390, Cross Loss: 0.1482\n",
            "148/239, Train_loss: 0.1481991708278656 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [149/239], Dice Loss: 0.5230, Cross Loss: 0.2613\n",
            "149/239, Train_loss: 0.2613489031791687 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [150/239], Dice Loss: 0.9916, Cross Loss: 0.1482\n",
            "150/239, Train_loss: 0.14819814264774323 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [151/239], Dice Loss: 0.6349, Cross Loss: 0.2613\n",
            "151/239, Train_loss: 0.2613472640514374 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [152/239], Dice Loss: 0.9430, Cross Loss: 0.1482\n",
            "152/239, Train_loss: 0.14820009469985962 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [153/239], Dice Loss: 0.8842, Cross Loss: 0.1482\n",
            "153/239, Train_loss: 0.14820033311843872 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [154/239], Dice Loss: 0.6801, Cross Loss: 0.2614\n",
            "154/239, Train_loss: 0.26135560870170593 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [155/239], Dice Loss: 0.8433, Cross Loss: 0.1482\n",
            "155/239, Train_loss: 0.1482010930776596 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [156/239], Dice Loss: 0.7949, Cross Loss: 0.2613\n",
            "156/239, Train_loss: 0.26133573055267334 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [157/239], Dice Loss: 0.8229, Cross Loss: 0.2613\n",
            "157/239, Train_loss: 0.2613287568092346 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [158/239], Dice Loss: 0.7540, Cross Loss: 0.2613\n",
            "158/239, Train_loss: 0.26133963465690613 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [159/239], Dice Loss: 0.9566, Cross Loss: 0.1482\n",
            "159/239, Train_loss: 0.1482040286064148 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [160/239], Dice Loss: 0.7618, Cross Loss: 0.2613\n",
            "160/239, Train_loss: 0.26133859157562256 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [161/239], Dice Loss: 0.8982, Cross Loss: 0.1482\n",
            "161/239, Train_loss: 0.14820453524589539 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [162/239], Dice Loss: 0.9392, Cross Loss: 0.2613\n",
            "162/239, Train_loss: 0.2613493800163269 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [163/239], Dice Loss: 0.9812, Cross Loss: 0.1482\n",
            "163/239, Train_loss: 0.14820362627506256 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [164/239], Dice Loss: 0.9196, Cross Loss: 0.1482\n",
            "164/239, Train_loss: 0.14820386469364166 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [165/239], Dice Loss: 0.7824, Cross Loss: 0.2613\n",
            "165/239, Train_loss: 0.26134416460990906 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [166/239], Dice Loss: 0.9918, Cross Loss: 0.2613\n",
            "166/239, Train_loss: 0.261345237493515 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [167/239], Dice Loss: 0.9050, Cross Loss: 0.2613\n",
            "167/239, Train_loss: 0.2613369822502136 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [168/239], Dice Loss: 0.7438, Cross Loss: 0.2613\n",
            "168/239, Train_loss: 0.26133596897125244 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [169/239], Dice Loss: 0.8576, Cross Loss: 0.2613\n",
            "169/239, Train_loss: 0.2613467872142792 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [170/239], Dice Loss: 0.9527, Cross Loss: 0.2613\n",
            "170/239, Train_loss: 0.2613373100757599 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [171/239], Dice Loss: 0.8712, Cross Loss: 0.2613\n",
            "171/239, Train_loss: 0.2613208293914795 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [172/239], Dice Loss: 0.7605, Cross Loss: 0.1482\n",
            "172/239, Train_loss: 0.14820727705955505 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [173/239], Dice Loss: 0.8376, Cross Loss: 0.2613\n",
            "173/239, Train_loss: 0.26134854555130005 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [174/239], Dice Loss: 0.6194, Cross Loss: 0.2613\n",
            "174/239, Train_loss: 0.26132482290267944 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [175/239], Dice Loss: 0.9634, Cross Loss: 0.2613\n",
            "175/239, Train_loss: 0.26131927967071533 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [176/239], Dice Loss: 0.8369, Cross Loss: 0.1482\n",
            "176/239, Train_loss: 0.14821502566337585 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [177/239], Dice Loss: 0.6645, Cross Loss: 0.1482\n",
            "177/239, Train_loss: 0.14821656048297882 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [178/239], Dice Loss: 0.8068, Cross Loss: 0.2613\n",
            "178/239, Train_loss: 0.2613128125667572 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [179/239], Dice Loss: 0.8789, Cross Loss: 0.1482\n",
            "179/239, Train_loss: 0.1482134312391281 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [180/239], Dice Loss: 0.8281, Cross Loss: 0.1482\n",
            "180/239, Train_loss: 0.14821460843086243 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [181/239], Dice Loss: 0.9243, Cross Loss: 0.2613\n",
            "181/239, Train_loss: 0.26132163405418396 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [182/239], Dice Loss: 0.7254, Cross Loss: 0.1482\n",
            "182/239, Train_loss: 0.1482183188199997 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [183/239], Dice Loss: 0.9363, Cross Loss: 0.2613\n",
            "183/239, Train_loss: 0.26126182079315186 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [184/239], Dice Loss: 0.8890, Cross Loss: 0.1482\n",
            "184/239, Train_loss: 0.14822110533714294 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [185/239], Dice Loss: 0.9132, Cross Loss: 0.1482\n",
            "185/239, Train_loss: 0.14822198450565338 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [186/239], Dice Loss: 0.8062, Cross Loss: 0.1482\n",
            "186/239, Train_loss: 0.14823056757450104 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [187/239], Dice Loss: 0.9800, Cross Loss: 0.1482\n",
            "187/239, Train_loss: 0.1482311338186264 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [188/239], Dice Loss: 0.8572, Cross Loss: 0.2613\n",
            "188/239, Train_loss: 0.26129791140556335 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [189/239], Dice Loss: 0.5932, Cross Loss: 0.1482\n",
            "189/239, Train_loss: 0.14822322130203247 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [190/239], Dice Loss: 0.9740, Cross Loss: 0.1482\n",
            "190/239, Train_loss: 0.14822061359882355 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [191/239], Dice Loss: 0.7260, Cross Loss: 0.2613\n",
            "191/239, Train_loss: 0.2612772583961487 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [192/239], Dice Loss: 0.9055, Cross Loss: 0.1482\n",
            "192/239, Train_loss: 0.14823488891124725 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [193/239], Dice Loss: 0.9549, Cross Loss: 0.2613\n",
            "193/239, Train_loss: 0.26126980781555176 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [194/239], Dice Loss: 0.9901, Cross Loss: 0.1482\n",
            "194/239, Train_loss: 0.14823105931282043 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [195/239], Dice Loss: 0.9968, Cross Loss: 0.1482\n",
            "195/239, Train_loss: 0.14823201298713684 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [196/239], Dice Loss: 0.9973, Cross Loss: 0.2613\n",
            "196/239, Train_loss: 0.26128286123275757 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [197/239], Dice Loss: 0.9981, Cross Loss: 0.1482\n",
            "197/239, Train_loss: 0.14823147654533386 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [198/239], Dice Loss: 0.9979, Cross Loss: 0.1482\n",
            "198/239, Train_loss: 0.14823254942893982 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [199/239], Dice Loss: 0.8358, Cross Loss: 0.1482\n",
            "199/239, Train_loss: 0.14822134375572205 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [200/239], Dice Loss: 0.6247, Cross Loss: 0.1482\n",
            "200/239, Train_loss: 0.14822092652320862 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [201/239], Dice Loss: 0.8393, Cross Loss: 0.2613\n",
            "201/239, Train_loss: 0.26130250096321106 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [202/239], Dice Loss: 0.6866, Cross Loss: 0.2613\n",
            "202/239, Train_loss: 0.26125627756118774 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [203/239], Dice Loss: 0.7775, Cross Loss: 0.1482\n",
            "203/239, Train_loss: 0.14823591709136963 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [204/239], Dice Loss: 0.9900, Cross Loss: 0.1482\n",
            "204/239, Train_loss: 0.14823698997497559 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [205/239], Dice Loss: 0.7772, Cross Loss: 0.2613\n",
            "205/239, Train_loss: 0.2612651586532593 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [206/239], Dice Loss: 0.8488, Cross Loss: 0.1482\n",
            "206/239, Train_loss: 0.14821186661720276 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [207/239], Dice Loss: 0.8529, Cross Loss: 0.1482\n",
            "207/239, Train_loss: 0.1482430100440979 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [208/239], Dice Loss: 0.8572, Cross Loss: 0.1482\n",
            "208/239, Train_loss: 0.14824290573596954 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [209/239], Dice Loss: 0.8360, Cross Loss: 0.2613\n",
            "209/239, Train_loss: 0.2612804174423218 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [210/239], Dice Loss: 0.9997, Cross Loss: 0.1482\n",
            "210/239, Train_loss: 0.14822925627231598 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [211/239], Dice Loss: 0.8785, Cross Loss: 0.1482\n",
            "211/239, Train_loss: 0.1482318490743637 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [212/239], Dice Loss: 0.8668, Cross Loss: 0.1482\n",
            "212/239, Train_loss: 0.14823010563850403 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [213/239], Dice Loss: 0.6414, Cross Loss: 0.2613\n",
            "213/239, Train_loss: 0.26127493381500244 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [214/239], Dice Loss: 0.6730, Cross Loss: 0.2613\n",
            "214/239, Train_loss: 0.26128140091896057 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [215/239], Dice Loss: 0.6211, Cross Loss: 0.1482\n",
            "215/239, Train_loss: 0.14821799099445343 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [216/239], Dice Loss: 0.9057, Cross Loss: 0.1482\n",
            "216/239, Train_loss: 0.14821603894233704 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [217/239], Dice Loss: 0.6166, Cross Loss: 0.2613\n",
            "217/239, Train_loss: 0.2613140642642975 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [218/239], Dice Loss: 0.6331, Cross Loss: 0.2613\n",
            "218/239, Train_loss: 0.26130175590515137 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [219/239], Dice Loss: 0.8728, Cross Loss: 0.1482\n",
            "219/239, Train_loss: 0.14822085201740265 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [220/239], Dice Loss: 0.9899, Cross Loss: 0.1482\n",
            "220/239, Train_loss: 0.14821287989616394 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [221/239], Dice Loss: 0.9871, Cross Loss: 0.1482\n",
            "221/239, Train_loss: 0.14821678400039673 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [222/239], Dice Loss: 0.9969, Cross Loss: 0.1482\n",
            "222/239, Train_loss: 0.14821335673332214 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [223/239], Dice Loss: 0.9861, Cross Loss: 0.1482\n",
            "223/239, Train_loss: 0.14821800589561462 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [224/239], Dice Loss: 0.6054, Cross Loss: 0.1482\n",
            "224/239, Train_loss: 0.14821791648864746 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [225/239], Dice Loss: 0.9554, Cross Loss: 0.1482\n",
            "225/239, Train_loss: 0.14821913838386536 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [226/239], Dice Loss: 0.9979, Cross Loss: 0.1482\n",
            "226/239, Train_loss: 0.14822375774383545 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [227/239], Dice Loss: 1.0000, Cross Loss: 0.1482\n",
            "227/239, Train_loss: 0.1482236534357071 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [228/239], Dice Loss: 0.9992, Cross Loss: 0.1482\n",
            "228/239, Train_loss: 0.14821426570415497 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [229/239], Dice Loss: 0.9980, Cross Loss: 0.1482\n",
            "229/239, Train_loss: 0.14822301268577576 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [230/239], Dice Loss: 0.9944, Cross Loss: 0.1482\n",
            "230/239, Train_loss: 0.14822354912757874 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [231/239], Dice Loss: 0.9858, Cross Loss: 0.1482\n",
            "231/239, Train_loss: 0.1482233703136444 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [232/239], Dice Loss: 0.9740, Cross Loss: 0.2613\n",
            "232/239, Train_loss: 0.2612992227077484 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [233/239], Dice Loss: 0.7634, Cross Loss: 0.1482\n",
            "233/239, Train_loss: 0.1482132077217102 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [234/239], Dice Loss: 0.9993, Cross Loss: 0.1482\n",
            "234/239, Train_loss: 0.14821204543113708 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [235/239], Dice Loss: 0.5587, Cross Loss: 0.2614\n",
            "235/239, Train_loss: 0.2613503634929657 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [236/239], Dice Loss: 0.9964, Cross Loss: 0.2613\n",
            "236/239, Train_loss: 0.2613252103328705 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [237/239], Dice Loss: 1.0000, Cross Loss: 0.1482\n",
            "237/239, Train_loss: 0.1482136845588684 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [238/239], Dice Loss: 0.9456, Cross Loss: 0.1482\n",
            "238/239, Train_loss: 0.14821982383728027 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [4/100], Batch [239/239], Dice Loss: 0.9988, Cross Loss: 0.2613\n",
            "239/239, Train_loss: 0.26128488779067993 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1861\n",
            "Epoch_metric: tensor(0.1861, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "test_loss_epoch: 0.1860\n",
            "test_dice_epoch: tensor(0.1860, device='cuda:0')\n",
            "current epoch: 4 current mean dice: tensor(0.1860, device='cuda:0')\n",
            "best mean dice: tensor(0.1865, device='cuda:0') at epoch: 1\n",
            "----------\n",
            "epoch 5/100\n",
            "Epoch [5/100], Batch [1/239], Dice Loss: 0.9773, Cross Loss: 0.2613\n",
            "1/239, Train_loss: 0.2612804174423218 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [2/239], Dice Loss: 0.7488, Cross Loss: 0.2613\n",
            "2/239, Train_loss: 0.26127538084983826 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [3/239], Dice Loss: 0.9723, Cross Loss: 0.2613\n",
            "3/239, Train_loss: 0.2612953186035156 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [4/239], Dice Loss: 0.9584, Cross Loss: 0.2613\n",
            "4/239, Train_loss: 0.2612762451171875 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [5/239], Dice Loss: 0.8957, Cross Loss: 0.1482\n",
            "5/239, Train_loss: 0.14820784330368042 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [6/239], Dice Loss: 0.9243, Cross Loss: 0.1482\n",
            "6/239, Train_loss: 0.1482090801000595 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [7/239], Dice Loss: 0.6527, Cross Loss: 0.2613\n",
            "7/239, Train_loss: 0.2613134980201721 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [8/239], Dice Loss: 0.9973, Cross Loss: 0.2613\n",
            "8/239, Train_loss: 0.2612546682357788 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [9/239], Dice Loss: 0.9975, Cross Loss: 0.2613\n",
            "9/239, Train_loss: 0.26128214597702026 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [10/239], Dice Loss: 0.8734, Cross Loss: 0.1482\n",
            "10/239, Train_loss: 0.14824146032333374 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [11/239], Dice Loss: 0.7735, Cross Loss: 0.1482\n",
            "11/239, Train_loss: 0.14822843670845032 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [12/239], Dice Loss: 0.9294, Cross Loss: 0.1483\n",
            "12/239, Train_loss: 0.1482705920934677 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [13/239], Dice Loss: 0.9051, Cross Loss: 0.1483\n",
            "13/239, Train_loss: 0.14825743436813354 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [14/239], Dice Loss: 0.7038, Cross Loss: 0.1483\n",
            "14/239, Train_loss: 0.14827680587768555 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [15/239], Dice Loss: 0.7221, Cross Loss: 0.1483\n",
            "15/239, Train_loss: 0.14828257262706757 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [16/239], Dice Loss: 0.9433, Cross Loss: 0.1483\n",
            "16/239, Train_loss: 0.1483067274093628 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [17/239], Dice Loss: 0.5886, Cross Loss: 0.1483\n",
            "17/239, Train_loss: 0.14829827845096588 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [18/239], Dice Loss: 0.9447, Cross Loss: 0.1482\n",
            "18/239, Train_loss: 0.14824378490447998 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [19/239], Dice Loss: 0.6469, Cross Loss: 0.2612\n",
            "19/239, Train_loss: 0.2612321078777313 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [20/239], Dice Loss: 0.6077, Cross Loss: 0.2613\n",
            "20/239, Train_loss: 0.26131680607795715 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [21/239], Dice Loss: 0.8960, Cross Loss: 0.2612\n",
            "21/239, Train_loss: 0.26124823093414307 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [22/239], Dice Loss: 0.9596, Cross Loss: 0.1483\n",
            "22/239, Train_loss: 0.14826685190200806 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [23/239], Dice Loss: 0.6764, Cross Loss: 0.2613\n",
            "23/239, Train_loss: 0.261316180229187 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [24/239], Dice Loss: 0.8191, Cross Loss: 0.2612\n",
            "24/239, Train_loss: 0.26122623682022095 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [25/239], Dice Loss: 0.9670, Cross Loss: 0.1482\n",
            "25/239, Train_loss: 0.14823326468467712 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [26/239], Dice Loss: 0.9530, Cross Loss: 0.1482\n",
            "26/239, Train_loss: 0.148221954703331 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [27/239], Dice Loss: 0.9649, Cross Loss: 0.1482\n",
            "27/239, Train_loss: 0.1482224315404892 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [28/239], Dice Loss: 0.7459, Cross Loss: 0.1482\n",
            "28/239, Train_loss: 0.14822715520858765 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [29/239], Dice Loss: 0.9102, Cross Loss: 0.1482\n",
            "29/239, Train_loss: 0.14824777841567993 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [30/239], Dice Loss: 0.9808, Cross Loss: 0.1482\n",
            "30/239, Train_loss: 0.14824488759040833 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [31/239], Dice Loss: 0.7595, Cross Loss: 0.1482\n",
            "31/239, Train_loss: 0.14823809266090393 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [32/239], Dice Loss: 0.9353, Cross Loss: 0.1482\n",
            "32/239, Train_loss: 0.14822274446487427 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [33/239], Dice Loss: 0.4252, Cross Loss: 0.2613\n",
            "33/239, Train_loss: 0.2612778842449188 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [34/239], Dice Loss: 0.9965, Cross Loss: 0.1482\n",
            "34/239, Train_loss: 0.14822310209274292 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [35/239], Dice Loss: 0.9194, Cross Loss: 0.2613\n",
            "35/239, Train_loss: 0.261272132396698 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [36/239], Dice Loss: 0.9210, Cross Loss: 0.1482\n",
            "36/239, Train_loss: 0.14823609590530396 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [37/239], Dice Loss: 0.9178, Cross Loss: 0.1482\n",
            "37/239, Train_loss: 0.14821481704711914 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [38/239], Dice Loss: 0.8314, Cross Loss: 0.1482\n",
            "38/239, Train_loss: 0.14822113513946533 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [39/239], Dice Loss: 0.6760, Cross Loss: 0.2613\n",
            "39/239, Train_loss: 0.2612740993499756 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [40/239], Dice Loss: 0.7612, Cross Loss: 0.2613\n",
            "40/239, Train_loss: 0.26126593351364136 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [41/239], Dice Loss: 0.9550, Cross Loss: 0.1482\n",
            "41/239, Train_loss: 0.1482233703136444 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [42/239], Dice Loss: 0.5982, Cross Loss: 0.1482\n",
            "42/239, Train_loss: 0.1482333391904831 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [43/239], Dice Loss: 0.9346, Cross Loss: 0.1482\n",
            "43/239, Train_loss: 0.14823263883590698 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [44/239], Dice Loss: 0.6757, Cross Loss: 0.1482\n",
            "44/239, Train_loss: 0.14821523427963257 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [45/239], Dice Loss: 0.7173, Cross Loss: 0.2613\n",
            "45/239, Train_loss: 0.26132839918136597 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [46/239], Dice Loss: 0.9672, Cross Loss: 0.1482\n",
            "46/239, Train_loss: 0.14820913970470428 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [47/239], Dice Loss: 0.7690, Cross Loss: 0.1482\n",
            "47/239, Train_loss: 0.148216113448143 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [48/239], Dice Loss: 0.8156, Cross Loss: 0.2613\n",
            "48/239, Train_loss: 0.2612649202346802 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [49/239], Dice Loss: 0.8689, Cross Loss: 0.1482\n",
            "49/239, Train_loss: 0.14824168384075165 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [50/239], Dice Loss: 0.8034, Cross Loss: 0.1482\n",
            "50/239, Train_loss: 0.14821529388427734 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [51/239], Dice Loss: 0.6122, Cross Loss: 0.1482\n",
            "51/239, Train_loss: 0.14821463823318481 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [52/239], Dice Loss: 0.7953, Cross Loss: 0.1482\n",
            "52/239, Train_loss: 0.14821377396583557 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [53/239], Dice Loss: 0.7821, Cross Loss: 0.1482\n",
            "53/239, Train_loss: 0.14822006225585938 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [54/239], Dice Loss: 0.7884, Cross Loss: 0.2613\n",
            "54/239, Train_loss: 0.2613111734390259 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [55/239], Dice Loss: 0.5689, Cross Loss: 0.1482\n",
            "55/239, Train_loss: 0.1482173204421997 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [56/239], Dice Loss: 0.9522, Cross Loss: 0.1482\n",
            "56/239, Train_loss: 0.14821690320968628 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [57/239], Dice Loss: 0.7337, Cross Loss: 0.2613\n",
            "57/239, Train_loss: 0.26131731271743774 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [58/239], Dice Loss: 0.9652, Cross Loss: 0.1482\n",
            "58/239, Train_loss: 0.14820627868175507 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [59/239], Dice Loss: 0.9535, Cross Loss: 0.1482\n",
            "59/239, Train_loss: 0.14820539951324463 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [60/239], Dice Loss: 0.7528, Cross Loss: 0.1482\n",
            "60/239, Train_loss: 0.14820684492588043 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [61/239], Dice Loss: 0.9335, Cross Loss: 0.1482\n",
            "61/239, Train_loss: 0.14820566773414612 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [62/239], Dice Loss: 0.6620, Cross Loss: 0.2613\n",
            "62/239, Train_loss: 0.2612994909286499 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [63/239], Dice Loss: 0.8844, Cross Loss: 0.1482\n",
            "63/239, Train_loss: 0.14824092388153076 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [64/239], Dice Loss: 0.8172, Cross Loss: 0.1482\n",
            "64/239, Train_loss: 0.1482141613960266 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [65/239], Dice Loss: 0.6441, Cross Loss: 0.1482\n",
            "65/239, Train_loss: 0.1482054889202118 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [66/239], Dice Loss: 0.6144, Cross Loss: 0.1482\n",
            "66/239, Train_loss: 0.14821551740169525 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [67/239], Dice Loss: 0.9715, Cross Loss: 0.1482\n",
            "67/239, Train_loss: 0.14820393919944763 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [68/239], Dice Loss: 0.7899, Cross Loss: 0.1483\n",
            "68/239, Train_loss: 0.14825031161308289 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [69/239], Dice Loss: 0.9883, Cross Loss: 0.2613\n",
            "69/239, Train_loss: 0.26131677627563477 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [70/239], Dice Loss: 0.9735, Cross Loss: 0.2613\n",
            "70/239, Train_loss: 0.26132380962371826 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [71/239], Dice Loss: 0.5271, Cross Loss: 0.1482\n",
            "71/239, Train_loss: 0.14821162819862366 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [72/239], Dice Loss: 0.9649, Cross Loss: 0.2613\n",
            "72/239, Train_loss: 0.2613294720649719 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [73/239], Dice Loss: 0.9477, Cross Loss: 0.1482\n",
            "73/239, Train_loss: 0.1482202708721161 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [74/239], Dice Loss: 0.8885, Cross Loss: 0.1482\n",
            "74/239, Train_loss: 0.14822031557559967 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [75/239], Dice Loss: 0.9712, Cross Loss: 0.1482\n",
            "75/239, Train_loss: 0.14820696413516998 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [76/239], Dice Loss: 0.8374, Cross Loss: 0.1482\n",
            "76/239, Train_loss: 0.14821356534957886 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [77/239], Dice Loss: 0.9954, Cross Loss: 0.1482\n",
            "77/239, Train_loss: 0.1482158899307251 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [78/239], Dice Loss: 0.9608, Cross Loss: 0.1482\n",
            "78/239, Train_loss: 0.14820979535579681 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [79/239], Dice Loss: 0.9881, Cross Loss: 0.1482\n",
            "79/239, Train_loss: 0.14821437001228333 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [80/239], Dice Loss: 0.7789, Cross Loss: 0.1482\n",
            "80/239, Train_loss: 0.14820167422294617 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [81/239], Dice Loss: 0.9888, Cross Loss: 0.1482\n",
            "81/239, Train_loss: 0.14820310473442078 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [82/239], Dice Loss: 0.4165, Cross Loss: 0.1482\n",
            "82/239, Train_loss: 0.1482010930776596 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [83/239], Dice Loss: 0.9294, Cross Loss: 0.1482\n",
            "83/239, Train_loss: 0.14820078015327454 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [84/239], Dice Loss: 0.9300, Cross Loss: 0.1482\n",
            "84/239, Train_loss: 0.14820298552513123 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [85/239], Dice Loss: 0.7708, Cross Loss: 0.2613\n",
            "85/239, Train_loss: 0.26134347915649414 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [86/239], Dice Loss: 0.7535, Cross Loss: 0.1482\n",
            "86/239, Train_loss: 0.14820127189159393 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [87/239], Dice Loss: 0.9161, Cross Loss: 0.1482\n",
            "87/239, Train_loss: 0.14820101857185364 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [88/239], Dice Loss: 0.9306, Cross Loss: 0.1482\n",
            "88/239, Train_loss: 0.14819972217082977 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [89/239], Dice Loss: 0.9810, Cross Loss: 0.1482\n",
            "89/239, Train_loss: 0.1481994092464447 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [90/239], Dice Loss: 0.9252, Cross Loss: 0.1482\n",
            "90/239, Train_loss: 0.14819905161857605 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [91/239], Dice Loss: 0.6612, Cross Loss: 0.2613\n",
            "91/239, Train_loss: 0.26133057475090027 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [92/239], Dice Loss: 0.9449, Cross Loss: 0.1482\n",
            "92/239, Train_loss: 0.1482064425945282 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [93/239], Dice Loss: 0.9186, Cross Loss: 0.1482\n",
            "93/239, Train_loss: 0.14820601046085358 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [94/239], Dice Loss: 0.8583, Cross Loss: 0.2613\n",
            "94/239, Train_loss: 0.2613479495048523 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [95/239], Dice Loss: 0.8763, Cross Loss: 0.1482\n",
            "95/239, Train_loss: 0.14819841086864471 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [96/239], Dice Loss: 0.9471, Cross Loss: 0.1482\n",
            "96/239, Train_loss: 0.14819841086864471 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [97/239], Dice Loss: 0.8700, Cross Loss: 0.1482\n",
            "97/239, Train_loss: 0.14820140600204468 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [98/239], Dice Loss: 0.8584, Cross Loss: 0.1482\n",
            "98/239, Train_loss: 0.1482025682926178 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [99/239], Dice Loss: 0.8819, Cross Loss: 0.2613\n",
            "99/239, Train_loss: 0.26133257150650024 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [100/239], Dice Loss: 0.9592, Cross Loss: 0.2614\n",
            "100/239, Train_loss: 0.2613563537597656 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [101/239], Dice Loss: 0.6612, Cross Loss: 0.2613\n",
            "101/239, Train_loss: 0.2613494396209717 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [102/239], Dice Loss: 0.8058, Cross Loss: 0.1482\n",
            "102/239, Train_loss: 0.14819779992103577 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [103/239], Dice Loss: 0.8665, Cross Loss: 0.1482\n",
            "103/239, Train_loss: 0.14820744097232819 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [104/239], Dice Loss: 0.8692, Cross Loss: 0.2613\n",
            "104/239, Train_loss: 0.26134711503982544 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [105/239], Dice Loss: 0.8511, Cross Loss: 0.1482\n",
            "105/239, Train_loss: 0.1482202261686325 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [106/239], Dice Loss: 0.9862, Cross Loss: 0.1482\n",
            "106/239, Train_loss: 0.1482209414243698 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [107/239], Dice Loss: 0.7788, Cross Loss: 0.1482\n",
            "107/239, Train_loss: 0.14820769429206848 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [108/239], Dice Loss: 0.8076, Cross Loss: 0.2613\n",
            "108/239, Train_loss: 0.2613290250301361 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [109/239], Dice Loss: 0.7416, Cross Loss: 0.1482\n",
            "109/239, Train_loss: 0.14820167422294617 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [110/239], Dice Loss: 0.9735, Cross Loss: 0.1482\n",
            "110/239, Train_loss: 0.14820167422294617 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [111/239], Dice Loss: 0.9479, Cross Loss: 0.1482\n",
            "111/239, Train_loss: 0.1482016146183014 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [112/239], Dice Loss: 0.8606, Cross Loss: 0.1482\n",
            "112/239, Train_loss: 0.1482016146183014 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [113/239], Dice Loss: 0.8378, Cross Loss: 0.1482\n",
            "113/239, Train_loss: 0.14819839596748352 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [114/239], Dice Loss: 0.7639, Cross Loss: 0.1482\n",
            "114/239, Train_loss: 0.14819824695587158 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [115/239], Dice Loss: 0.9207, Cross Loss: 0.1482\n",
            "115/239, Train_loss: 0.14820607006549835 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [116/239], Dice Loss: 0.8965, Cross Loss: 0.1482\n",
            "116/239, Train_loss: 0.14820577204227448 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [117/239], Dice Loss: 0.9207, Cross Loss: 0.1482\n",
            "117/239, Train_loss: 0.14819678664207458 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [118/239], Dice Loss: 0.8862, Cross Loss: 0.1482\n",
            "118/239, Train_loss: 0.14819656312465668 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [119/239], Dice Loss: 0.9025, Cross Loss: 0.1482\n",
            "119/239, Train_loss: 0.14819806814193726 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [120/239], Dice Loss: 0.9227, Cross Loss: 0.1482\n",
            "120/239, Train_loss: 0.14819632470607758 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [121/239], Dice Loss: 0.7644, Cross Loss: 0.1482\n",
            "121/239, Train_loss: 0.14819687604904175 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [122/239], Dice Loss: 0.6788, Cross Loss: 0.2613\n",
            "122/239, Train_loss: 0.26132723689079285 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [123/239], Dice Loss: 0.9060, Cross Loss: 0.1482\n",
            "123/239, Train_loss: 0.1482100784778595 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [124/239], Dice Loss: 0.5717, Cross Loss: 0.2613\n",
            "124/239, Train_loss: 0.26133307814598083 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [125/239], Dice Loss: 0.8266, Cross Loss: 0.2614\n",
            "125/239, Train_loss: 0.2613620162010193 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [126/239], Dice Loss: 0.9084, Cross Loss: 0.1482\n",
            "126/239, Train_loss: 0.14819709956645966 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [127/239], Dice Loss: 0.9295, Cross Loss: 0.1482\n",
            "127/239, Train_loss: 0.14819519221782684 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [128/239], Dice Loss: 0.8029, Cross Loss: 0.2613\n",
            "128/239, Train_loss: 0.26130399107933044 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [129/239], Dice Loss: 0.8068, Cross Loss: 0.1482\n",
            "129/239, Train_loss: 0.14819470047950745 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [130/239], Dice Loss: 0.6718, Cross Loss: 0.2613\n",
            "130/239, Train_loss: 0.26133739948272705 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [131/239], Dice Loss: 0.8796, Cross Loss: 0.1482\n",
            "131/239, Train_loss: 0.14820519089698792 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [132/239], Dice Loss: 0.9989, Cross Loss: 0.1482\n",
            "132/239, Train_loss: 0.14819684624671936 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [133/239], Dice Loss: 0.7641, Cross Loss: 0.1482\n",
            "133/239, Train_loss: 0.14819711446762085 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [134/239], Dice Loss: 0.8192, Cross Loss: 0.2614\n",
            "134/239, Train_loss: 0.26135069131851196 Train_dice: tensor(0.2614, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [135/239], Dice Loss: 0.6021, Cross Loss: 0.1482\n",
            "135/239, Train_loss: 0.14819757640361786 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [136/239], Dice Loss: 0.9989, Cross Loss: 0.2613\n",
            "136/239, Train_loss: 0.2613295912742615 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [137/239], Dice Loss: 0.5204, Cross Loss: 0.2613\n",
            "137/239, Train_loss: 0.2613433599472046 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [138/239], Dice Loss: 0.8674, Cross Loss: 0.1482\n",
            "138/239, Train_loss: 0.14821353554725647 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [139/239], Dice Loss: 0.5834, Cross Loss: 0.2613\n",
            "139/239, Train_loss: 0.2613222599029541 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [140/239], Dice Loss: 0.9953, Cross Loss: 0.1482\n",
            "140/239, Train_loss: 0.1482134312391281 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [141/239], Dice Loss: 0.8069, Cross Loss: 0.2613\n",
            "141/239, Train_loss: 0.26130184531211853 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [142/239], Dice Loss: 0.9003, Cross Loss: 0.1482\n",
            "142/239, Train_loss: 0.14821016788482666 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [143/239], Dice Loss: 0.9885, Cross Loss: 0.2613\n",
            "143/239, Train_loss: 0.2613399922847748 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [144/239], Dice Loss: 0.8878, Cross Loss: 0.1482\n",
            "144/239, Train_loss: 0.1482039988040924 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [145/239], Dice Loss: 0.6637, Cross Loss: 0.1482\n",
            "145/239, Train_loss: 0.14821234345436096 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [146/239], Dice Loss: 0.9103, Cross Loss: 0.1482\n",
            "146/239, Train_loss: 0.14821411669254303 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [147/239], Dice Loss: 0.9507, Cross Loss: 0.1482\n",
            "147/239, Train_loss: 0.1482059210538864 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [148/239], Dice Loss: 0.9530, Cross Loss: 0.1482\n",
            "148/239, Train_loss: 0.14820563793182373 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [149/239], Dice Loss: 0.5307, Cross Loss: 0.2613\n",
            "149/239, Train_loss: 0.2613459825515747 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [150/239], Dice Loss: 0.9938, Cross Loss: 0.1482\n",
            "150/239, Train_loss: 0.14820000529289246 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [151/239], Dice Loss: 0.5651, Cross Loss: 0.2613\n",
            "151/239, Train_loss: 0.26131802797317505 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [152/239], Dice Loss: 0.9074, Cross Loss: 0.1482\n",
            "152/239, Train_loss: 0.148213192820549 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [153/239], Dice Loss: 0.8481, Cross Loss: 0.1482\n",
            "153/239, Train_loss: 0.1482156366109848 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [154/239], Dice Loss: 0.6936, Cross Loss: 0.2613\n",
            "154/239, Train_loss: 0.2613174617290497 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [155/239], Dice Loss: 0.8178, Cross Loss: 0.1482\n",
            "155/239, Train_loss: 0.14822803437709808 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [156/239], Dice Loss: 0.8100, Cross Loss: 0.2612\n",
            "156/239, Train_loss: 0.26123499870300293 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [157/239], Dice Loss: 0.7968, Cross Loss: 0.2612\n",
            "157/239, Train_loss: 0.26122626662254333 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [158/239], Dice Loss: 0.7118, Cross Loss: 0.2612\n",
            "158/239, Train_loss: 0.2612334191799164 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [159/239], Dice Loss: 0.9656, Cross Loss: 0.1483\n",
            "159/239, Train_loss: 0.14826558530330658 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [160/239], Dice Loss: 0.7443, Cross Loss: 0.2612\n",
            "160/239, Train_loss: 0.26123952865600586 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [161/239], Dice Loss: 0.8729, Cross Loss: 0.1483\n",
            "161/239, Train_loss: 0.14825966954231262 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [162/239], Dice Loss: 0.9087, Cross Loss: 0.2613\n",
            "162/239, Train_loss: 0.2613285183906555 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [163/239], Dice Loss: 0.9855, Cross Loss: 0.1483\n",
            "163/239, Train_loss: 0.1482665240764618 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [164/239], Dice Loss: 0.8809, Cross Loss: 0.1483\n",
            "164/239, Train_loss: 0.14827074110507965 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [165/239], Dice Loss: 0.8636, Cross Loss: 0.2613\n",
            "165/239, Train_loss: 0.2612767815589905 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [166/239], Dice Loss: 0.9910, Cross Loss: 0.2613\n",
            "166/239, Train_loss: 0.2612619400024414 Train_dice: tensor(0.2613, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [167/239], Dice Loss: 0.9592, Cross Loss: 0.2612\n",
            "167/239, Train_loss: 0.2611965537071228 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [168/239], Dice Loss: 0.7904, Cross Loss: 0.2612\n",
            "168/239, Train_loss: 0.261163592338562 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [169/239], Dice Loss: 0.8215, Cross Loss: 0.2612\n",
            "169/239, Train_loss: 0.2611563801765442 Train_dice: tensor(0.2612, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [170/239], Dice Loss: 0.9179, Cross Loss: 0.2611\n",
            "170/239, Train_loss: 0.2611115574836731 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [171/239], Dice Loss: 0.9026, Cross Loss: 0.2610\n",
            "171/239, Train_loss: 0.2609963119029999 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [172/239], Dice Loss: 0.7183, Cross Loss: 0.1484\n",
            "172/239, Train_loss: 0.1484082043170929 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [173/239], Dice Loss: 0.8912, Cross Loss: 0.2610\n",
            "173/239, Train_loss: 0.2610390782356262 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [174/239], Dice Loss: 0.5871, Cross Loss: 0.2605\n",
            "174/239, Train_loss: 0.2605397701263428 Train_dice: tensor(0.2605, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [175/239], Dice Loss: 0.9624, Cross Loss: 0.2602\n",
            "175/239, Train_loss: 0.2601562440395355 Train_dice: tensor(0.2602, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [176/239], Dice Loss: 0.8090, Cross Loss: 0.1490\n",
            "176/239, Train_loss: 0.14903473854064941 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [177/239], Dice Loss: 0.6889, Cross Loss: 0.1492\n",
            "177/239, Train_loss: 0.14919504523277283 Train_dice: tensor(0.1492, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [178/239], Dice Loss: 0.8182, Cross Loss: 0.2592\n",
            "178/239, Train_loss: 0.25922882556915283 Train_dice: tensor(0.2592, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [179/239], Dice Loss: 0.8451, Cross Loss: 0.1494\n",
            "179/239, Train_loss: 0.14938369393348694 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [180/239], Dice Loss: 0.8155, Cross Loss: 0.1495\n",
            "180/239, Train_loss: 0.14948910474777222 Train_dice: tensor(0.1495, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [181/239], Dice Loss: 0.8869, Cross Loss: 0.2595\n",
            "181/239, Train_loss: 0.25949928164482117 Train_dice: tensor(0.2595, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [182/239], Dice Loss: 0.6579, Cross Loss: 0.1497\n",
            "182/239, Train_loss: 0.14970506727695465 Train_dice: tensor(0.1497, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [183/239], Dice Loss: 0.9555, Cross Loss: 0.2590\n",
            "183/239, Train_loss: 0.25903064012527466 Train_dice: tensor(0.2590, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [184/239], Dice Loss: 0.8780, Cross Loss: 0.1498\n",
            "184/239, Train_loss: 0.14983373880386353 Train_dice: tensor(0.1498, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [185/239], Dice Loss: 0.8755, Cross Loss: 0.1498\n",
            "185/239, Train_loss: 0.1498188078403473 Train_dice: tensor(0.1498, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [186/239], Dice Loss: 0.7551, Cross Loss: 0.1498\n",
            "186/239, Train_loss: 0.1497504562139511 Train_dice: tensor(0.1498, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [187/239], Dice Loss: 0.9800, Cross Loss: 0.1494\n",
            "187/239, Train_loss: 0.1494479924440384 Train_dice: tensor(0.1494, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [188/239], Dice Loss: 0.8564, Cross Loss: 0.2597\n",
            "188/239, Train_loss: 0.25968554615974426 Train_dice: tensor(0.2597, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [189/239], Dice Loss: 0.5903, Cross Loss: 0.1490\n",
            "189/239, Train_loss: 0.14897343516349792 Train_dice: tensor(0.1490, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [190/239], Dice Loss: 0.9738, Cross Loss: 0.1488\n",
            "190/239, Train_loss: 0.14884406328201294 Train_dice: tensor(0.1488, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [191/239], Dice Loss: 0.7012, Cross Loss: 0.2600\n",
            "191/239, Train_loss: 0.26004838943481445 Train_dice: tensor(0.2600, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [192/239], Dice Loss: 0.9143, Cross Loss: 0.1487\n",
            "192/239, Train_loss: 0.14874446392059326 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [193/239], Dice Loss: 0.9608, Cross Loss: 0.2604\n",
            "193/239, Train_loss: 0.2603679597377777 Train_dice: tensor(0.2604, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [194/239], Dice Loss: 0.9935, Cross Loss: 0.1487\n",
            "194/239, Train_loss: 0.14865925908088684 Train_dice: tensor(0.1487, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [195/239], Dice Loss: 0.9995, Cross Loss: 0.1486\n",
            "195/239, Train_loss: 0.14863653481006622 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [196/239], Dice Loss: 0.9986, Cross Loss: 0.2606\n",
            "196/239, Train_loss: 0.2605554461479187 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [197/239], Dice Loss: 0.9988, Cross Loss: 0.1486\n",
            "197/239, Train_loss: 0.1485878825187683 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [198/239], Dice Loss: 0.9984, Cross Loss: 0.1486\n",
            "198/239, Train_loss: 0.14857405424118042 Train_dice: tensor(0.1486, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [199/239], Dice Loss: 0.7924, Cross Loss: 0.1485\n",
            "199/239, Train_loss: 0.14853611588478088 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [200/239], Dice Loss: 0.6425, Cross Loss: 0.1485\n",
            "200/239, Train_loss: 0.14850914478302002 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [201/239], Dice Loss: 0.7960, Cross Loss: 0.2608\n",
            "201/239, Train_loss: 0.26076027750968933 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [202/239], Dice Loss: 0.5989, Cross Loss: 0.2606\n",
            "202/239, Train_loss: 0.26056820154190063 Train_dice: tensor(0.2606, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [203/239], Dice Loss: 0.8152, Cross Loss: 0.1485\n",
            "203/239, Train_loss: 0.14851690828800201 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [204/239], Dice Loss: 0.9908, Cross Loss: 0.1485\n",
            "204/239, Train_loss: 0.1485157608985901 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [205/239], Dice Loss: 0.7116, Cross Loss: 0.2607\n",
            "205/239, Train_loss: 0.26071634888648987 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [206/239], Dice Loss: 0.8311, Cross Loss: 0.1485\n",
            "206/239, Train_loss: 0.1484622061252594 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [207/239], Dice Loss: 0.8845, Cross Loss: 0.1485\n",
            "207/239, Train_loss: 0.14851632714271545 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [208/239], Dice Loss: 0.8743, Cross Loss: 0.1485\n",
            "208/239, Train_loss: 0.14851278066635132 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [209/239], Dice Loss: 0.8590, Cross Loss: 0.2607\n",
            "209/239, Train_loss: 0.26065921783447266 Train_dice: tensor(0.2607, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [210/239], Dice Loss: 0.9991, Cross Loss: 0.1485\n",
            "210/239, Train_loss: 0.14852756261825562 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [211/239], Dice Loss: 0.8428, Cross Loss: 0.1485\n",
            "211/239, Train_loss: 0.14846216142177582 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [212/239], Dice Loss: 0.8570, Cross Loss: 0.1485\n",
            "212/239, Train_loss: 0.14845184981822968 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [213/239], Dice Loss: 0.6195, Cross Loss: 0.2608\n",
            "213/239, Train_loss: 0.26077595353126526 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [214/239], Dice Loss: 0.6275, Cross Loss: 0.2608\n",
            "214/239, Train_loss: 0.26078033447265625 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [215/239], Dice Loss: 0.5958, Cross Loss: 0.1484\n",
            "215/239, Train_loss: 0.1484335958957672 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [216/239], Dice Loss: 0.9114, Cross Loss: 0.1484\n",
            "216/239, Train_loss: 0.1484350860118866 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [217/239], Dice Loss: 0.5769, Cross Loss: 0.2608\n",
            "217/239, Train_loss: 0.260841429233551 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [218/239], Dice Loss: 0.6314, Cross Loss: 0.2608\n",
            "218/239, Train_loss: 0.26080140471458435 Train_dice: tensor(0.2608, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [219/239], Dice Loss: 0.8784, Cross Loss: 0.1485\n",
            "219/239, Train_loss: 0.1484641283750534 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [220/239], Dice Loss: 0.9852, Cross Loss: 0.1485\n",
            "220/239, Train_loss: 0.1484634429216385 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [221/239], Dice Loss: 0.9734, Cross Loss: 0.1485\n",
            "221/239, Train_loss: 0.14845652878284454 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [222/239], Dice Loss: 0.9951, Cross Loss: 0.1485\n",
            "222/239, Train_loss: 0.1484660804271698 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [223/239], Dice Loss: 0.9879, Cross Loss: 0.1485\n",
            "223/239, Train_loss: 0.14847305417060852 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [224/239], Dice Loss: 0.5971, Cross Loss: 0.1485\n",
            "224/239, Train_loss: 0.14846190810203552 Train_dice: tensor(0.1485, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [225/239], Dice Loss: 0.9254, Cross Loss: 0.1484\n",
            "225/239, Train_loss: 0.1484294831752777 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [226/239], Dice Loss: 0.9897, Cross Loss: 0.1484\n",
            "226/239, Train_loss: 0.14843079447746277 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [227/239], Dice Loss: 0.9999, Cross Loss: 0.1484\n",
            "227/239, Train_loss: 0.14841191470623016 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [228/239], Dice Loss: 0.9997, Cross Loss: 0.1484\n",
            "228/239, Train_loss: 0.14836445450782776 Train_dice: tensor(0.1484, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [229/239], Dice Loss: 0.9991, Cross Loss: 0.1482\n",
            "229/239, Train_loss: 0.14823441207408905 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [230/239], Dice Loss: 0.9229, Cross Loss: 0.1482\n",
            "230/239, Train_loss: 0.14822730422019958 Train_dice: tensor(0.1482, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [231/239], Dice Loss: 0.8563, Cross Loss: 0.1483\n",
            "231/239, Train_loss: 0.14825475215911865 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [232/239], Dice Loss: 0.9528, Cross Loss: 0.2610\n",
            "232/239, Train_loss: 0.26101019978523254 Train_dice: tensor(0.2610, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [233/239], Dice Loss: 0.7758, Cross Loss: 0.1483\n",
            "233/239, Train_loss: 0.14831504225730896 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [234/239], Dice Loss: 0.9991, Cross Loss: 0.1483\n",
            "234/239, Train_loss: 0.14830604195594788 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [235/239], Dice Loss: 0.5617, Cross Loss: 0.2611\n",
            "235/239, Train_loss: 0.26114022731781006 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [236/239], Dice Loss: 0.9973, Cross Loss: 0.2611\n",
            "236/239, Train_loss: 0.26109880208969116 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [237/239], Dice Loss: 1.0000, Cross Loss: 0.1483\n",
            "237/239, Train_loss: 0.14831221103668213 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [238/239], Dice Loss: 0.9310, Cross Loss: 0.1483\n",
            "238/239, Train_loss: 0.14834779500961304 Train_dice: tensor(0.1483, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "Epoch [5/100], Batch [239/239], Dice Loss: 0.9991, Cross Loss: 0.2611\n",
            "239/239, Train_loss: 0.26106008887290955 Train_dice: tensor(0.2611, device='cuda:0', grad_fn=<AliasBackward0>)\n",
            "--------------------\n",
            "Epoch_loss: 0.1861\n",
            "Epoch_metric: tensor(0.1861, device='cuda:0', grad_fn=<AliasBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "def updateLogs(path, data):\n",
        "    f = open(path,'a')\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "\n",
        "model_dir = './results/lesion/Focal_dice_Loss/'\n",
        "\n",
        "\n",
        "start_from = 1\n",
        "test_interval =1\n",
        "num_epochs = 100\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "\n",
        "save_loss_train = []\n",
        "save_loss_test = []\n",
        "save_metric_train = []\n",
        "save_metric_test = []\n",
        "if (start_from != 1):\n",
        "  save_loss_train, save_metric_train, save_loss_test, save_metric_test= [x.tolist() for x in load_metrices(load_from)]\n",
        "  if(len(save_metric_test)):\n",
        "        best_metric = max(save_metric_test)\n",
        "  best_metric_epoch = -2\n",
        "train_loader, test_loader = data_in\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    train_epoch_loss = 0\n",
        "    train_step = 0\n",
        "    epoch_metric_train = 0\n",
        "\n",
        "    for batch_id, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        train_step += 1\n",
        "        \n",
        "        volume = batch_data[\"image\"]\n",
        "        label = batch_data[\"label\"]\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          volume, label = (volume.to(device), label.to(device))\n",
        "\n",
        "        \n",
        "        # Zero the gradients\n",
        "        optimizer_out.zero_grad()\n",
        "        optimizer_bb.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        mask_output, bb_output = model(volume)\n",
        "        \n",
        "        #Compute loss for segmentation mask\n",
        "        # mask_loss = loss_function(label, mask_output)\n",
        "        \n",
        "       \n",
        "        bb_target = one_hot_tensor[batch_id]\n",
        "        \n",
        "       \n",
        "        bb_target = bb_target.view(1, -1).to(device)\n",
        "        \n",
        "        #print(bb_target.cpu().detach().numpy())\n",
        "        #print(\"True \",encoder.inverse_transform(bb_target))\n",
        "        #print(bb_output.cpu().detach().numpy())\n",
        "        #bb_output.to(device)\n",
        "        #bb = bb_output.detach().numpy()\n",
        "        #print(\"Predicted \", encoder.inverse_transform(bb))\n",
        "       \n",
        "        dsc = dsc_loss( mask_output, label)\n",
        "        cse_loss = cross_entropy_loss(bb_output, bb_target.float())\n",
        "        # Total loss as a combination of mask loss and bounding box classifier loss\n",
        "        #train_loss = criterion( mask_output, label, bb_output, bb_target)\n",
        "        \n",
        "        # Backward pass\n",
        "        dsc.backward( retain_graph=True)\n",
        "        cse_loss.backward()\n",
        "        \n",
        "        # Update weights\n",
        "        optimizer_out.step()\n",
        "        optimizer_bb.step()\n",
        "        \n",
        "        # Print training progress\n",
        "        print('Epoch [{}/{}], Batch [{}/{}], Dice Loss: {:.4f}, Cross Loss: {:.4f}'\n",
        "              .format(epoch+1, num_epochs, batch_id+1, len(train_loader), dsc.item(), cse_loss.item()))\n",
        "        updateLogs(os.path.join(model_dir, \"logs.txt\"), f'Epoch [{epoch+1}/{num_epochs}], Batch [{ batch_id+1}/{ len(train_loader)}],Dice Loss: {dsc.item():.4f}, Cross Loss:  {cse_loss.item():.4f}\\n')\n",
        "        updateLogs(os.path.join(model_dir, \"logs.txt\"), f'True {bb_target.cpu().detach().numpy()}\\n Predicted {bb_output.cpu().detach().numpy()} \\n')\n",
        "\n",
        "\n",
        "        train_epoch_loss += cse_loss.item()\n",
        "        print(\n",
        "                f\"{train_step}/{len(train_loader) // train_loader.batch_size}, \"\n",
        "                f\"Train_loss: {cse_loss.item()}\", end=\" \")\n",
        "\n",
        "        train_metric = cross_entropy_loss(bb_output, bb_target.float())\n",
        "        epoch_metric_train += train_metric\n",
        "        print(f'Train_dice: {train_metric}')\n",
        "\n",
        "    print('-'*20)\n",
        "        \n",
        "    train_epoch_loss /= train_step\n",
        "    print(f'Epoch_loss: {train_epoch_loss:.4f}')\n",
        "    save_loss_train.append(train_epoch_loss)\n",
        "    np.save(os.path.join(model_dir, 'loss_train.npy'), save_loss_train)\n",
        "        \n",
        "    epoch_metric_train /= train_step\n",
        "    print(f'Epoch_metric: {epoch_metric_train}')\n",
        "\n",
        "    save_metric_train.append(epoch_metric_train.cpu().detach().numpy())\n",
        "    np.save(os.path.join(model_dir, 'metric_train.npy'), save_metric_train)\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(\n",
        "            model_dir, \"current_metric_model.pth\"))\n",
        "        \n",
        "    updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"{'-'*20}{epoch+1} \\nEpoch_loss: {train_epoch_loss:.4f}\\nEpoch_metric: {epoch_metric_train}\\n\")\n",
        "\n",
        "    if (epoch + 1) % test_interval == 0:\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          test_epoch_loss = 0\n",
        "          test_metric = 0\n",
        "          epoch_metric_test = 0\n",
        "          test_step = 0\n",
        "          total_loss = 0\n",
        "          for batch_id, batch_data in enumerate(test_loader):\n",
        "              test_step += 1\n",
        "\n",
        "              volume = batch_data[\"image\"]\n",
        "              label = batch_data[\"label\"]\n",
        "              if torch.cuda.is_available():\n",
        "                  volume, label = (volume.to(device), label.to(device))\n",
        "              \n",
        "              # Forward pass\n",
        "              mask_output, bb_output = model(volume)\n",
        "              \n",
        "              bb_target = one_hot_tensor[batch_id]\n",
        "        \n",
        "       \n",
        "              bb_target = bb_target.view(1, -1).to(device)\n",
        "              \n",
        "              # Compute loss for bounding box classifier\n",
        "\n",
        "              test_loss = cross_entropy_loss( bb_output, bb_target.float())\n",
        "              \n",
        "              test_epoch_loss += test_loss.item()\n",
        "              test_metric = cross_entropy_loss( bb_output, bb_target.float())\n",
        "              \n",
        "              epoch_metric_test += test_metric\n",
        "          \n",
        "          # Calculate the average loss across all batches in the test_loader\n",
        "          test_epoch_loss /= test_step\n",
        "          print(f'test_loss_epoch: {test_epoch_loss:.4f}')\n",
        "          save_loss_test.append(test_epoch_loss)\n",
        "          np.save(os.path.join(model_dir, 'loss_test.npy'), save_loss_test)\n",
        "\n",
        "          epoch_metric_test /= test_step\n",
        "          print(f'test_dice_epoch: {epoch_metric_test}')\n",
        "          save_metric_test.append(epoch_metric_test.cpu().detach().numpy())\n",
        "          np.save(os.path.join(model_dir, 'metric_test.npy'), save_metric_test)\n",
        "\n",
        "\n",
        "          if epoch_metric_test > best_metric:\n",
        "              best_metric = epoch_metric_test\n",
        "              best_metric_epoch = epoch + 1\n",
        "              torch.save(model.state_dict(), os.path.join(\n",
        "              model_dir, \"best_metric_model.pth\"))\n",
        "                \n",
        "          print(\n",
        "                    f\"current epoch: {epoch + 1} current mean dice: {epoch_metric_test}\"\n",
        "                    f\"\\nbest mean dice: {best_metric} \"\n",
        "                     \n",
        "                    f\"at epoch: {best_metric_epoch}\"\n",
        "                )\n",
        "          \n",
        "          \n",
        "          updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"{'-'*30}\\ncurrent epoch: {epoch + 1} \\n\"\n",
        "                    f'test_dice_epoch: {epoch_metric_test}\\n'\n",
        "                    f'test_loss_epoch: {test_epoch_loss:.4f}\\n'\n",
        "                    f\"best mean dice: {best_metric} \"\n",
        "                    f\"at epoch: {best_metric_epoch}\\n\")\n",
        "          \n",
        "        updateLogs(os.path.join(model_dir, \"logs.txt\"), f\"train completed, best_metric: {best_metric}\"\n",
        "        f\"at epoch: {best_metric_epoch}\\n\")\n",
        "\n",
        "        update_history([start_from, num_epochs, best_metric, best_metric_epoch],model_dir=model_dir)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl61WEOJ4ZQ9"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "def load_metrices(path):\n",
        "  metrices_dir = path\n",
        "  train_loss = train_metric = test_loss = test_metric =np.array([])\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'loss_train.npy')):\n",
        "    train_loss = np.load(os.path.join(metrices_dir, 'loss_train.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'metric_train.npy')):\n",
        "    train_metric = np.load(os.path.join(metrices_dir, 'metric_train.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'loss_test.npy')):\n",
        "    test_loss = np.load(os.path.join(metrices_dir, 'loss_test.npy'))\n",
        "  if os.path.isfile(os.path.join(metrices_dir, 'metric_test.npy')):\n",
        "    test_metric = np.load(os.path.join(metrices_dir, 'metric_test.npy'))\n",
        "  return train_loss, train_metric, test_loss, test_metric\n",
        "\n",
        "def dice_metric(predicted, target):\n",
        "    '''\n",
        "    In this function we take `predicted` and `target` (label) to calculate the dice coeficient then we use it \n",
        "    to calculate a metric value for the training and the validation.\n",
        "    '''\n",
        "    dice_value = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
        "    value = 1 - dice_value(predicted, target).item()\n",
        "    return value\n",
        "\n",
        "\n",
        "def get_time():\n",
        "  utc_time= datetime.datetime.now(pytz.utc)\n",
        "  local_time = utc_time.astimezone(pytz.timezone('Asia/Colombo'))\n",
        "  return local_time.strftime(\"%Y:%m:%d %H:%M:%S\")\n",
        "\n",
        "def update_history(data,model_dir):\n",
        "  history_file_path = model_dir + \"history.csv\"\n",
        "  if not os.path.exists(history_file_path):\n",
        "    with open(history_file_path,'a') as fd:\n",
        "        fd.write(\",\".join([\"Start\", \"End\", \"Best Matrix\", \"Best M. At\"]))\n",
        "  with open(history_file_path,'a') as fd:\n",
        "      str_data=[str(x) for x in (data + [get_time()])]\n",
        "      fd.write(\"\\n\" + \",\".join(str_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAA2fmxbywsV"
      },
      "source": [
        "## Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmqiWAAdywsW",
        "outputId": "4cfc4d58-7483-4b40-b42e-c8a66adaa19f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(384, 384, 16) (384, 384, 16)\n"
          ]
        }
      ],
      "source": [
        "# first image, label from orignal image\n",
        "imagea =nib.load(data[0][\"image\"]).get_fdata()\n",
        "labela =nib.load(data[0][\"label\"]).get_fdata()\n",
        "print(imagea.shape,labela.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGnnLlngywsW",
        "outputId": "a8f47b40-f866-4daa-f1ec-8eaede514f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 128, 128, 16]) torch.Size([1, 1, 128, 128, 16])\n"
          ]
        }
      ],
      "source": [
        "# first image, label from preprossed image\n",
        "image1= first(train_loader)[\"image\"]\n",
        "label1 = first(train_loader)[\"label\"]\n",
        "print(image1.shape,label1.shape)\n",
        "image11 =image1.get_array()\n",
        "label11 = label1.get_array()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8kWaIwrywsW"
      },
      "outputs": [],
      "source": [
        "# Define a function to visualize the data\n",
        "def explore_3dimage2(layer):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    channel = 2\n",
        "    # plt.imshow(image11[0,0,:,:,layer], cmap='gray');\n",
        "    # plt.title('Explore Layers of Prostate MRI', fontsize=20)\n",
        "    # plt.axis('off')\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
        "\n",
        "    ax[0].imshow(image1[0,0,:,:,layer], cmap='gray')\n",
        "    ax[0].set_title(f\"Image\", fontsize=15)\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(label1[0,0,:,:,layer])\n",
        "    ax[1].axis('off')\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "qmJVtNMWywsW",
        "outputId": "2ad740b3-b743-4984-84df-415a9b78c895"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d4ca001c8124085a6d046e89e56720c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=7, description='layer', max=15), Output()), _dom_classes=('widget-intera…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<function __main__.explore_3dimage2(layer)>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Explore the preprocessed image, label\n",
        "interact(explore_3dimage2, layer=(0, 15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEDLevmu6TRU"
      },
      "source": [
        "## 3 attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDxbp9pcLjX3",
        "outputId": "7362c2ad-e474-4341-c77c-70d22ef9f098"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 6])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MulticlassImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MulticlassImageClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(4, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(32)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(64)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm3d(128)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8 * 2, 512)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.relu4 = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = x.view(-1, 128 * 8 * 8 * 2)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the model, loss function and optimizer\n",
        "model = MulticlassImageClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "model(torch.randn(1,4,128,128,16)).shape\n",
        "\n",
        "# # Train the model\n",
        "# for epoch in range(num_epochs):\n",
        "#     for i, (inputs, labels) in enumerate(train_loader):\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         if i % 100 == 0:\n",
        "#             print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# # Test the model\n",
        "# with torch.no_grad():\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for inputs, labels in test_loader:\n",
        "#         outputs = model(inputs)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#     print(f\"Test Accuracy: {(100 * correct / total):.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0oVgMzR_ywr_",
        "Pasts0BAz-F1",
        "diKStuuskvkS",
        "aqf2izgHRetB",
        "MD3y2qam_2Ig",
        "txse4NidVs-S",
        "QAA2fmxbywsV"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "packages",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
